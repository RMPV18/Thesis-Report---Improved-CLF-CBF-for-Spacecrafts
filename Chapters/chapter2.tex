%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%


\chapter{Methodology}
\label{cha:methodology}

\glsresetall %Para em vez de mostrar o acrónimo voltar a mostrar o nome completo para relembrar



%Aqui falar do modelo generico e de algumas coisas que serviram de base para os proximos topicos e que não valem a pena uma secção, alem de introduzir o que ai vem

\section{\glsxtrshort{CLF-CBF}}
\label{sec:clf_cbf}

Considering a nonlinear control-affine system:

\begin{equation}
    \dot{\xVec} = f(\xVec) + g(\xVec)\uVec
    \label{eq:Nonlinear-First-Order-System} 
\end{equation}

with stet \(\xVec \in \mathbb{R}^n\), control input \(\uVec \in \mathbb{R}^m\), and locally Lipschitz continuos\footnote{Given two metric spaces (\(X, d_X\)) and (\(Y, d_Y\)), where \(d_X\) and \(d_Y\)  represent the \(X\) and \(Y\) metrics respectively, a function \(f:X \to Y\) is Lipschitz continuos if there exist a constant \(L \geq 0\) such that \(\forall (x_1,x_2) \in X\), \hspace{0.4em} \(d_Y(f(x_1),f(x_2))\leq L \hspace{0.2em} d_X(x_1,x_2)\)} functions \(f: \mathbb{R}^n \to \mathbb{R}^n\) and \(g: \mathbb{R}^n \to \mathbb{R}^{n \times m}\). Given a locally Lipschitz continuos \(k: \mathbb{R}^n \to \mathbb{R}^m\), it results in the closed-loop system:

\begin{equation}
    \dot{\xVec} = f(\xVec) + g(\xVec)k(\xVec)
    \label{eq:Nonlinear-First-Order-Closed-Loop-System} 
\end{equation}

For any initial condition \(\xVec_0 \in \mathbb{R}^n\), there is a maximal time interval \(\mathcal{I}(\xVec_0) = [0; t_{max}(\xVec_0)[ \) and unique continuosly differentiable solution \(\varphi: \mathcal{I}(\xVec_0) \to \mathbb{R}^n \) such that:

\begin{equation}
    \begin{array}{l}
        \dot{\varphi}(t) = f(\varphi(t))+ g(\varphi(t))k(\varphi(t)) \quad \forall t \in \mathcal{I}(\xVec_0)  \\
        \varphi(0) = \xVec_0
    \end{array}
 \label{eq:Nonlinear-First-Order-Closed-Loop-System-UniqueSol}
\end{equation}


\subsection{First-Order Formulation}
\label{sub:formulation}

\subsubsection{\glsxtrfull{CLF}}
\label{subsub:control_lyapunov_function}

Having the objective of global assymptotically stability of the system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System}, is desired for the system converge to an equilibrium point \( \bar{\mathbf{x}} \). A solution comes by designing a positive definite function \( V: \mathbb{R}^n \rightarrow \mathbb{R}_{\geq 0} \) which converges to zero while the system converges to the respective equilibrium point, and for that, designing a control law that acts in order to achieve specifically the convergence of the function. Consequentially is pretended to the function \( V\mathbf(\mathbf{x}) \) to be continuos and differentiable:

\[\dot{V} (\mathbf{x}, \mathbf{u}) = \frac{\partial V}{\partial x}\dot{x} = L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \]

where \(L_fV\) and \(L_GV\) are the Lie derivatives of \(V\) and \(\mathbf{u}\) the control input vector resposible to manipulate the function, more precisely, its convergence rate.\par

That beeing said, a continuosly differentiable, positive definite function \( V: \mathbb{R}^n \rightarrow \mathbb{R}_{\geq 0}  \) wich \( \lim_{\mathbf{x} \to \bar{\mathbf{x}}}{V(\mathbf{x})} = 0 \) is a \glsxtrshort{CLF} if there exists a \( \gamma \in \mathcal{K}^e_{\infty}  \)\footnote{If a continuos functions \(\alpha:[-b,a] \to \mathbb{R}\) with \((a,b) > 0\), strictly increasing and \(\alpha(0) = 0\) then \(\alpha \in \mathcal{K}^e\) and if it adds \(\lim_{r \to \infty} \alpha(r) = \infty\) then \(\alpha \in \mathcal{K}^e_{\infty}\)}  such that for all \( \mathbf{x} \in \mathbb{R}^n \setminus \{\bar{\mathbf{x}}\} \):

\begin{equation}
 \inf_{\mathbf{u} \in \mathbb{R}^m} [L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u}] \leq -\gamma(V(\mathbf{x}))
 \label{eq:CLF}
\end{equation}

From \glsxtrshort{CLF} \(V\) and an adequate tuning of \( \gamma \) parameter results a control law followed by the controller \(K_{CLF}\):

\begin{equation}
 K_{CLF}(\mathbf{x}) = \{ \mathbf{u} \in \mathbb{R}^m: L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \leq -\gamma(V(\mathbf{x})) \}
 \label{eq:K-CLF}
\end{equation}

The stets generated by \(K_{CLF}(\mathbf{x}) \hspace{0.5em} \forall x \in \mathbb{R}^n \) refer to the possible control input values necessary to the \glsxtrshort{CLF} \(V(\mathbf{x})\) continuosly converge to zero while the closed-loop system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System} to the equilibrium point, but more slowly as is close to reach the objective. \\


\subsubsection{\glsxtrfull{CBF}}
\label{subsub:control_barrier_function}

Now having the objective of garantee safety to system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System}, is aimed to keep the state inside a safe set \(\mathcal{C}\) (foward invariant\footnote{A set \(\mathcal{S}\) is said foward invariant if the initial system state \( x_0 \in \mathcal{S}\) and so \(x(t) \in \mathcal{S} \hspace{0.25em} \forall t \geq 0\) }) defined accordingly the problem needs. The notion of safety, i.e., the safe set \(\mathcal{C}\) is defined as the 0-superlevel\footnote{For function \(f: \mathbb{R}^n \to \mathbb{R}\), its superlevel set is given by \(\mathcal{S}(f) = \{(x_1,...,x_n): f(x_1,...,x_n) \geq c\}\), where having \(c = 0\) corresponds to the 0-superlevel set of \(f\)} set of a continuosly differentiable function \( h: \mathbb{R}^n \rightarrow \mathbb{R}  \):

\begin{equation}
    \begin{array}{l}
        \qquad   \mathcal{C} = \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} )\geq 0\} \\
        \qquad \hspace{-0.5em}  \partial\mathcal{C} = \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} ) = 0\} \\
        int(\mathcal{C}) = \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} ) > 0\}
    \end{array}
 \label{eq:safe-set}
\end{equation}


In order to mantain \(h( \mathbf{x} )\) positive, there is a need to have a control law and but first to be able to interface with the function. Since \(h( \mathbf{x} )\) is differentiable:  

\[\dot{h}(\mathbf{x}, \mathbf{u}) = \frac{\partial h}{\partial x}\dot{x} = L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \]

where \(L_fh\) and \(L_Gh\) are the Lie derivatives of \(h\) and \(\mathbf{u}\) the control input vector resposible to manipulate the function, more precisely, its time derivative.\par

So, a differentiable function \( h: \mathbb{R}^n \rightarrow \mathbb{R}  \) is a \glsxtrshort{CBF} if there exists a \( \alpha \in \mathcal{K}^e_{\infty}  \) such that for all \( \mathbf{x} \in \mathbb{R}^n \setminus \{\bar{\mathbf{x}}\} \):

\begin{equation}
 \sup_{\mathbf{u} \in \mathbb{R}^m} [L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u}] \geq -\alpha(h( \mathbf{x} ))
 \label{eq:CBF}
\end{equation}

Similar to the \glsxtrshort{CLF}, from \glsxtrshort{CBF} \(h\) and an adequate tuning of \( \alpha \) parameter results a control law encapsulated by the controller \(K_{CBF}\):

\begin{equation}
 K_{CBF}(\mathbf{x}) = \{ \mathbf{u} \in \mathbb{R}^m: L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \geq -\alpha(h( \mathbf{x} )) \}
 \label{eq:K-CBF}
\end{equation}

\newpage %Só para ajeitar

The controller \(K_{CLF}(\mathbf{x}) \hspace{0.5em} \forall x \in \mathbb{R}^n \) renders control input sets that keep the closed-loop system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System} safe with respect to the set \(\mathcal{C}\). The controller by defenition is able to evolve the closed-loop system more freely when \(\dot{h}( \mathbf{x} ) > 0\) and more cautiously as \(h( \mathbf{x} ) \) shortens and \(\dot{h}( \mathbf{x} ) < 0\), if \(h( \mathbf{x} ) < 0\) it means is in the unsafe region, so \(\dot{h}( \mathbf{x}) > 0\) implies converging continuosly to the safe set \(\mathcal{C}\). \\ 




\subsubsection{Quadratic Program Formulation}
\label{subsub:quadratic_program_formulation}

Now having the objective of not only achieve safety but also assymptotically stability through the usage of \glsxtrshort{CLF} and \glsxtrshort{CBF} it renders a set of control inputs able to achieve the respective purposes.  

Using the conditions imposed by \glsxtrshort{CLF} and \glsxtrshort{CBF} as constraints, is presented an optimization problem  via \glsxtrfull{QP}~\footnote{\glsxtrlong{QP} consists of solving a mathematical optimization problem with a quadratic objective function}, given a locally Lipschitz continuos controller \(k: \mathbb{R}^n \rightarrow \mathbb{R}^m \):

\begin{equation}
    \begin{array}{l}
        k(\xVec) = \underset{\uVec \in \mathbb{R}^m }{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec) \vspace{1em} \\  
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \leq -\gamma(V(\mathbf{x})) \vspace{0.25em} \\ 
        \quad \quad \hspace{1em} L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \geq -\alpha(h(\mathbf{x}))
    \end{array}
 \label{eq:CLF-CBF-QP}
\end{equation}

The sets of control inputs restricted to the optimization problem are given by \(K_{CLF-CBF}(\xVec) = K_{CLF}(\xVec) \cap  K_{CBF}(\xVec)\) hopefully non-empty which can happen if the conditions are in conflict. \par

In case of conditions' conflict, in order to the closed loop system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System} controller have a solution from the \glsxtrshort{QP}, since safety is critical, using a slack variable \(\delta\)  allows to relax the stability condition imposed by the \glsxtrshort{CLF} and highly contribute to obtaining a solution while keeping system safe:

\begin{equation}
    \begin{array}{l}
        k(\xVec) = \underset{\uVec \in \mathbb{R}^m, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \leq -\gamma(V(\mathbf{x})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em} L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \geq -\alpha(h(\mathbf{x}))
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed}
\end{equation}

Having \(\delta\) as an optimization variable not only allows to balance between the input values exceed usage and the level of relaxation but also to not have a fixed \(\delta\) value which would result for all states not so faster convergence and to the neighborhood of the equilibrium point instead of the actual target point. \\

%tikz daquele esquema
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item If it's not pretended to enforce stability or safety, just remove from the optimization problem the respective constraint responsible to it
\end{itemize}
\end{tcolorbox}

\newpage %Só para ajeitar


\subsection{Backstepping (Second-Order Formulation)}
\label{sub:backstepping}

\subsubsection{Second-Order Systems}
\label{subsub:higher_order_systems}

Usually control systems are governed by higher-order systems, such as some second-order cyber-physical systems controlling the position by applying some force to manipulate the speed that is felt in the position just instants after.  Higher order systems' outputs variation is felt by an multiple integrative effect, by an integration of each state affecting the next state dynamic until reach the so wanted output. Considering a non-linear system in strict-feedback form:

\begin{subequations}
    
    \label{eq:HH-NL-System}
    \begin{align}
        \dot{\xVec} = f_0(\xVec) + g_0(\xVec) \mathbf{\xi} 
        \label{eq:HH-NL-System-1stOrder}\\
        \dot{ \mathbf{\xi} } =  f_1(\xVec, \mathbf{\xi}) + g_1(\xVec, \mathbf{\xi}) \uVec
        \label{eq:HH-NL-System-2ndOrder}
    \end{align}
\end{subequations}

with \(\mathbf{x} \in \mathbb{R}^n \), \(\mathbf{\xi} \in \mathbb{R}^p \), and \(\mathbf{u} \in \mathbb{R}^m \), and functions \(f_0: \mathbb{R}^n \to \mathbb{R}^n \), \(g_0: \mathbb{R}^n \to \mathbb{R}^{n \times p} \), \(f_1: \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^p \) and \(g_1: \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^{ p \times m} \) locally Lipschitz continuos on their respective domains. \par
Given a locally Lipschitz continuos feedback controller \(\mathbf{k}: \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^m  \) the closed-loop system is:

\begin{equation}
    \begin{array}{l}
        \dot{\xVec} = f_0(\xVec) + g_0(\xVec) \mathbf{\xi}\\ 
        \dot{ \mathbf{\xi} } =  f_1(\xVec, \mathbf{\xi}) + g_1(\xVec, \mathbf{\xi}) \mathbf{k}(\xVec, \mathbf{\xi})
    \end{array}
 \label{eq:HH-NL-CL-System}
\end{equation}

for any initial states \((\xVec_0, \mathbf{\xi}_0) \in \mathbb{R}^n \times \mathbb{R}^p\) exists a maximum time interval \(I(\xVec_0, \mathbf{\xi}_0) \subseteq \mathbb{R}_{\geq 0}  \) and a unique solution \( \varphi = (\varphi_{\mathbf{x}}, \varphi_{\mathbf{\xi}}) \) satisfying \ref{eq:Nonlinear-First-Order-Closed-Loop-System-UniqueSol} \(\forall t \in I(\xVec_0, \mathbf{\xi}_0) \). \\


\subsubsection{Purposal Solution Obtaining}
\label{subsub:quadratic_program_formulation}

Since having a second order system~\ref{eq:HH-NL-System} the access to \(\mathbf{\xi}\) is not direct, we must "backstepp" in order to be able to control \(\uVec\), affect \(\mathbf{\xi}\) and consequentially control the wanted dynamic \(\dot{\xVec}\). The strategy passes by obtaining the solution if it was a first order system~\ref{eq:Nonlinear-First-Order-System} and if \(\mathbf{\xi}\) could be directly controlled (i.e. as part of the first order system control input), so given a differentiable and locally Lipschitz continuos \(k_0(\xVec):\mathbb{R}^n \to \mathbb{R}^p\) (that encompasses the "controllable \(\mathbf{\xi}\)" ), the closed-loop system:

\begin{equation}
    \dot{\xVec} = f_0(\xVec) + g_0(\xVec) k_0(\xVec)\\ 
 \label{eq:NL-CL-System-0Backstep}
\end{equation}

That beeing said, the first order solution could be obtained via \glsxtrshort{QP}, so supposing a \glsxtrshort{CLF} \(V_0\),  \glsxtrshort{CBF} \(h_0\),  a differentiable and locally Lipschitz continuos described just now \(k_0(\xVec)\) and \((\gamma_0, \alpha_0) \in \mathcal{K}^e_\infty \times \mathcal{K}^e_\infty\):

\begin{equation}
    \begin{array}{l}
        k_0(\xVec) = \underset{\uVec \in \mathbb{R}^p, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R_0 \uVec + p_0\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV_0(\mathbf{x}) + L_GV_0(\mathbf{x})\mathbf{u} \leq -\gamma_0(V_0(\mathbf{x})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em} L_fh_0(\mathbf{x}) + L_Gh_0(\mathbf{x})\mathbf{u} \geq -\alpha_0(h_0(\mathbf{x}))
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed-0Backstep}
\end{equation}


The solution is equivalent to a value of \(\mathbf{\xi}\) closest to an optimal value taken by it that would impose safety and contribute to stability not only to the first-order system~\ref{eq:NL-CL-System-0Backstep} but also to the actual second-order system. Although like it was said is not possible to have direct access to \(\mathbf{\xi}\), the backstepping technique is essentially the second-order trying to converge to the first order solution. Supposing there is \( V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}): \mathbb{R}^n \times \mathbb{R}^p \rightarrow \mathbb{R}_{\geq 0} \), the new \glsxtrshort{CLF} \(V(\xVec, \mathbf{\xi})\) and \glsxtrshort{CBF} \(h(\xVec, \mathbf{\xi})\) are:
\begin{subequations}
    \label{eq:Backstepp_CF}
    \begin{align}
        V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) = \frac{1}{2}|| \mathbf{\xi} -  k_0(\xVec) ||^2 
        \label{eq:Backstepp_prejudice}\\
        V(\xVec, \mathbf{\xi}) = V_0(\xVec) + V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) 
        \label{eq:Backstepp_V}\\
        h(\xVec, \mathbf{\xi}) = h_0(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi})
        \label{eq:Backstepp_h}
    \end{align}
\end{subequations}


Finally, via \Glsxtrshort{QP}, given a differentiable and locally Lipschitz continuos \(k(\xVec, \mathbf{\xi}):\mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^m\):

\begin{equation}
    \begin{array}{l}
        k(\xVec, \mathbf{\xi}) = \underset{\uVec \in \mathbb{R}^p, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\xVec, \mathbf{\xi}) + L_GV(\xVec, \mathbf{\xi})\mathbf{u} \leq -\gamma(V_0(\mathbf{x})) -\gamma'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em}                 L_fh(\xVec, \mathbf{\xi}) + L_Gh(\xVec, \mathbf{\xi})\mathbf{u} \geq -\alpha(h_0(\mathbf{x})) + \alpha'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi}))
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed-1Backstep}
\end{equation}

It's expected that  \( \gamma << \gamma'\)  contributing to a faster convergence of \(\xi\) and more similar behaviour of the system~\ref{eq:HH-NL-CL-System} relative to the first-order system~\ref{eq:NL-CL-System-0Backstep} and mitigate the chances of the system~\ref{eq:HH-NL-CL-System} not be safe in relation to \(\mathcal{C}_0\). \\

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item If it's not pretended to enforce stability or safety, just remove from both optimization problem (\ref{eq:CLF-CBF-QP-relaxed-0Backstep} and \ref{eq:CLF-CBF-QP-relaxed-1Backstep}) the respective constraint responsible to it
    \item If it's a higher-order (than second) system, then, assuming it's a third-order system, the solution obtained in \ref{eq:CLF-CBF-QP-relaxed-1Backstep} is the "new \(k_0\)" and the third-order solution tries to converge to the second-order one (while this one is converging to the first-order solution)
\end{itemize}
\end{tcolorbox} 



\subsection{Closed Form}
\label{sub:closed_form}

The objective of this thesis is to purpose a set of low computational effort techniques suitable to spacecrafts. Aiming a faster computation of the optimization problems solutions, any of the previous \glsxtrshort{QP}s (\ref{eq:CLF-CBF-QP}, \ref{eq:CLF-CBF-QP-relaxed}, \ref{eq:CLF-CBF-QP-relaxed-0Backstep}, \ref{eq:CLF-CBF-QP-relaxed-1Backstep}) have a closed-form\footnote{An closed-form expression is expressed by a limit number and well defined operations (elementary functions)} solution which allows to calculate directly the respective solution instead of depending of a numerical solver. Given the \glsxtrshort{QP} optimization problem and a controller \(k(\xVec, \mathcolor{gray}{\mathbf{\xi}}): \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p} \to \mathbb{R}^m\) \textcolor{gray}{(if it is a second-order system)}:

\begin{equation}
    \begin{array}{l}
        k(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = \underset{\uVec \in \mathbb{R}^m, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} \underbrace{L_fV(\xVec \mathcolor{gray}{, \mathbf{\xi}}) + \gamma(\mathcolor{orange}{V_0(\mathbf{x})}) \mathcolor{gray}{+ \gamma'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi}))}}_{F_V(\mathbf{x} \mathcolor{gray}{, \mathbf{\xi}})} + L_GV(\xVec \mathcolor{gray}{, \mathbf{\xi}})\mathbf{u} - \delta \leq  0 \vspace{0.25em}\\
        \quad \quad \hspace{1em}                 \underbrace{L_fh(\xVec \mathcolor{gray}{, \mathbf{\xi}}) + \alpha(\mathcolor{orange}{h_0(\mathbf{x})}) \mathcolor{gray}{- \alpha'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi}))}}_{F_h(\mathbf{x} \mathcolor{gray}{, \mathbf{\xi}})} + L_Gh(\xVec \mathcolor{gray}{, \mathbf{\xi}})\mathbf{u} \geq 0 
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed-generic}
\end{equation}

With a \glsxtrshort{CLF} \(V\), \glsxtrshort{CBF} \(h\), \(\mathbf{x}\in\mathbb{R}^n\), \textcolor{orange}{the first-order system \glsxtrshort{CLF}} \(\mathcolor{orange}{V_0} ( = V\) like in \ref{eq:CLF-CBF-QP}, \ref{eq:CLF-CBF-QP-relaxed} and \ref{eq:CLF-CBF-QP-relaxed-0Backstep}) \textcolor{gray}{, and if it is a second-order system with} \(\mathcolor{gray}{\mathbf{\xi}\in\mathbb{R}^p}\) \textcolor{gray}{(equivalent to the \glsxtrshort{QP} seen while doing Backstepping~\ref{eq:CLF-CBF-QP-relaxed-1Backstep})}. \\


Using \cite{matias2025hybrid} as a reference, assuming equal control input wheights relative to each other (\(R = \mathbf{I}_{m \times m}\)) and just one \glsxtrshort{CLF} and \glsxtrshort{CBF} each, according to the \glsxtrfull{KKT} conditions\footnote{\href{https://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions}{KKT Conditions Wikipedia (Click here to be Redirected)} \label{foot: KKT_Conditions}} (to see the full deduction \ref{app:CL_QP_CLF-CBF}):

\begin{equation}
    \mathbf{k}(\xVec \mathcolor{gray}{, \mathbf{\xi}}) =
    \begin{cases}
        \mathbf{k}_1(\xVec \mathcolor{gray}{, \mathbf{\xi}}), \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_1 \\
        \mathbf{k}_2(\xVec \mathcolor{gray}{, \mathbf{\xi}}), \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_2 \\
        \mathbf{k}_3(\xVec \mathcolor{gray}{, \mathbf{\xi}}), \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_3 \\
        \mathbf{0}_{m \times 1 } \quad  \hspace{0.25em}, \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_4
    \end{cases}
    \label{eq:closed-form_controller}
\end{equation}


Where each controller \(\mathbf{k}_i\), for \( i = 1, ..., 4 \), corresponds to the closed-form equation depending which constraints (\glsxtrshort{CLF} or \glsxtrshort{CBF}) are active what can be inferred by \(\mathcal{S}_i\), for \( i = 1, ..., 4 \). Via \glsxtrshort{KKT} conditions the respectives controllers and domains are obtained due to the \glsxtrshort{KKT} multipliers.\\

Auxiliary Variables:

\begin{align*}
    & L \sOrderArgB = L_GV \sOrderArgB \hspace{0.2em} L_Gh\sOrderArgB^{\top} \\
    & \Delta = L \sOrderArgB^2 (p^{-1} + ||L_Gh\sOrderArgB||^2)  \hspace{0.2em} || L_Gh\sOrderArgB||^2
\end{align*}

\glsxtrshort{KKT} multipliers:

\begin{align}
    & \lambda_1(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = \Delta^{-1} (L \sOrderArgB \hspace{0.2em} F_h \sOrderArgB \hspace{0.2em} - ||L_Gh\sOrderArgB||^2  \hspace{0.2em}F_V \sOrderArgB)
    \notag\\
    & \lambda_2(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = \Delta^{-1} (L \sOrderArgB F_V \sOrderArgB \hspace{0.2em} - (p^{-1} + ||L_Gh\sOrderArgB||^2)  \hspace{0.2em}F_h \sOrderArgB) 
    \label{eq:KKT-Multipliers_formulas}
\end{align}

Controller \(\mathbf{k}_i\) closed-form equation:

\begin{equation}
    \begin{array}{l}
    \mathbf{k}_1(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = -(p^{-1} + ||L_GV \sOrderArgB||^2)^{-1} \hspace{0.2em} F_V\sOrderArgB \hspace{0.2em} L_GV\sOrderArgB^{\top} \vspace{0.5em} \\ 
    \mathbf{k}_2(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = -||L_Gh \sOrderArgB||^{-2} \hspace{0.2em} F_h \sOrderArgB \hspace{0.2em} L_Gh \sOrderArgB^{\top} \vspace{0.5em} \\ 
    \mathbf{k}_3(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = -\lambda_1\sOrderArgB L_GV \sOrderArgB^{\top} + \lambda_2\sOrderArgB L_Gh \sOrderArgB^{\top} \vspace{1.5em}
    \label{eq:closed-form_controller_formulas}
    \end{array} 
\end{equation}



Constraint Activation Subdomains \(\mathcal{S}_i\) ( based on the dual feasibilitie condition~\footref{foot: KKT_Conditions}):

\begin{align}
    \intertext{\hspace{4em} If CLF is active: }
    \mathcal{S}_1 \sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 \geq 0 \cap \lambda_2 < 0 \}
    \notag \\
    \intertext{\hspace{4em} If CBF is active:}
    \mathcal{S}_2 \sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 < 0 \cap \lambda_2 \geq 0 \}
    \notag \\
    \label{eq:Constraints_Activation_Subdomains}
    \intertext{\hspace{4em} If CLF and CBF is active:}
    \mathcal{S}_3 \sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 \geq 0 \cap \lambda_2 \geq 0 \}
    \notag\\
    \intertext{\hspace{4em} If Any is active:}
    \mathcal{S}_4\sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 < 0 \cap \lambda_2 < 0 \}
    \notag
\end{align}


\subsection{Unifying \glsxtrshort{CBF}}
\label{sub:unifying_CBF}

The previous controllers synthesized control inputs based on just one \glsxtrshort{CBF}, i.e., those controllers garantee safety w.r.t. a single safe set \(\mathcal{C}\), but given diffrent scenarios, it could exist more complex safety specifications and the need to impose more than one safety contraints simultaneously such as \glsxtrshort{CBF} \(h_i(\xVec)\), for \(i \in  \mathcal{I}=\{1,...N\}\):

\begin{align}
    & \mathcal{C}_i = \{ \xVec \in \mathbb{R}_n: h_i(\xVec) \geq 0\}
    \label{eq:Multiple_Safe_Sets} \\ 
    \notag\\
    & \mathcal{C} = \bigcap_{i \in \mathcal{I}} \mathcal{C}_i 
    \label{eq:Safe_Set_of_multiple}
\end{align}

But like it was said before regarding having more than one constraint could leave to conflict problems, empty safe sets (\(\mathcal{C} = \emptyset\)) and an inexistent set of acceptable control inputs (\(\mathbf{k}\sOrderArgB = \emptyset\)). Using \cite{molnar2023composing} as guidance, in order to avoid conflicts, merges all \glsxtrshort{CBF} into a single one that is able to capture all those safety notions and allow for even more complex safety specifications and to the resultant constraint fit the already obtained \glsxtrshort{CLF-CBF} closed-form solution~\ref{eq:closed-form_controller}. \\


\subsubsection{First-Order System}
\label{subsub:uniCBF_firstOrder_system}

The more complex safety specifications are defined via boolean logical operations between 0-superlevel sets of the diffrent \glsxtrshort{CBF}. Those operations are tipically seen in constrained optimization problems, like in \glsxtrshort{QP} of the form:


\begin{equation}
    \begin{array}{l}
        k(\xVec) = \underset{\uVec \in \mathbb{R}^p, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\xVec) + L_GV(\xVec)\mathbf{u} \leq -\gamma(V(\mathbf{x})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em}                 L_fh_i(\xVec) + L_Gh_i(\xVec)\mathbf{u} \geq -\alpha_i(h_i(\mathbf{x}))  \quad \forall i \in \mathcal{I}    
    \end{array}
 \label{eq:CLF-MultipleCBF-QP-relaxed-generic}
\end{equation}

Where the resultant safe set~\ref{eq:Safe_Set_of_multiple} of the system refers to the intersection of all safe sets~\ref{eq:Multiple_Safe_Sets}. More than be propitious to be not solvable, the formulation itself of the optimization problem~\ref{eq:CLF-MultipleCBF-QP-relaxed-generic} is limitated by no other logical operation than AND. \par

Among the the logical operations that could be applied to \glsxtrshort{CBF}s, there are:

\begin{description}

    \item[Identity ( Class-\(\mathcal{K}^e\)):] 
    \begin{subequations}
        \begin{align}
            \mathcal{C}_i &= \{(\xVec, \alpha_i) \in \mathbb{R}^n \times \mathcal{K}^e: \alpha_i(h_i(\xVec) \geq 0)\} = 
            \label{eq:Indentity_set} \\
                          &= \{\xVec \in \mathbb{R}^n: h_i(\xVec) \geq 0\}
            \label{eq:Gen_Safe_set} 
        \end{align}
        
    \end{subequations}

    \item[Complement Set (Negation):] 
    
    \begin{equation}
        \bar{\mathcal{C}_i} = \{\xVec \in \mathbb{R}^n: -h_i(\xVec) \geq 0\} 
        \label{eq:Negation_set} 
    \end{equation}

    \item[Intersection of Sets (AND):] 

        \begin{equation}
            \begin{array}{rl}      
            \underset{i \in \mathcal{I}}{\bigcap} \mathcal{C}_i &= \{\xVec \in \mathbb{R}^n: h_i(\xVec) \geq 0 \hspace{0.5em} \forall i \in \mathcal{I}\}  \\
                                                      &= \{\xVec \in \mathbb{R}^n: \underset{i \in \mathcal{I}}{\min}(h_i(\xVec)) \geq 0 \} 
            \end{array}
            \label{eq:Intersection_set}
        \end{equation}

    \item[Union of Sets (OR):] 
    
        \begin{equation}
            \begin{array}{rl}
                \underset{i \in \mathcal{I}}{\bigcup} \mathcal{C}_i &= \{\xVec \in \mathbb{R}^n: \exists i \in \mathcal{I} \hspace{0.5em} st. \hspace{0.5em} h_i(\xVec) \geq 0 \}  \\
                                                          &= \{\xVec \in \mathbb{R}^n: \underset{i \in \mathcal{I}}{\max}(h_i(\xVec)) \geq 0 \} 
            \end{array}
            \label{eq:Union_set}
        \end{equation}
\end{description}

The logical operations identity, complement, intersection and union are represented by respectively, class-\(\mathcal{K}^e\) functions, negation, min and max operations. Although the class-\(\mathcal{K}^e\) (\(\alpha_i(h_i(\xVec))\)) and negation (\(-h_i(\xVec)\)) functions which are continuosly differentiable and valid as \glsxtrshort{CBF}, contrary to the AND and OR functions, \(\min_{ i \in \mathcal{I}} (h_i(\xVec))\) and \(\max_{ i \in \mathcal{I}} (h_i(\xVec))\) that aren't continuosly differentiable and therefore not \glsxtrshort{CBF} candidates.  The \(\max\) and \(\min\) functions capacitate combining safe sets defined by multiple \glsxtrshort{CBF} (accordingly to the operation) and to obtain a new safety specification as is expected to see while merging multiple \glsxtrshort{CBF} into a single one. So, in order to viablize the AND and OR operation and achieve the merge of \glsxtrshort{CBF}s, the \(min\) and \(max\) functions can be replaced via smooth-aproximation, keeping their properties with just a relative small degradation of accuracy, by continuosly differentiable \textbf{log-sum-exp functions}:


\begin{description}

    \item[Intersection of Sets \(\setminus\)AND \(\setminus\) Minimum]

    \begin{align}
        h(\xVec) &= \frac{-1}{k}ln(\sum_{i \in \mathcal{I}} e^{-k \hspace{0.2em} h_i(\xVec)}) 
        \label{eq:Min_smoothFunc} \\
                 &\approx \underset{i \in \mathcal{I}}{\min} (h_i(\xVec))
        \notag  
    \end{align}

    Beeing \(k \in \mathbb{R}_{>0}\) as smoothing parameter (if \(k<0\) then is equivalent to the smooth-aproximation of \(\max_{i \in \mathcal{I}}(h_i(\xVec))\) ). 

    \begin{subequations}
        \begin{align}
            &\dot{h}(\xVec , \uVec) = \underbrace{\sum_{i \in \mathcal{I}} [\lambda_i \hspace{0.2em} L_fh_i(\xVec)]}_{Lfh(\xVec)} + (\underbrace{\sum_{i \in \mathcal{I}} [\lambda_i \hspace{0.2em} L_Gh_i(\xVec)]}_{LGh(\xVec)})\uVec 
            \label{eq:Min_smooth_derivative} \\ \notag \\
            &\lambda_i(\xVec) = e^{-k(h_i(\xVec)-h(\xVec))}
            \label{eq:Min_lambda_smoothFunc}
        \end{align}
    \end{subequations}

    With \(\sum_{i \in \mathcal{I}} \lambda_i(\xVec) = 1\) \par
    Encountering an under-aproximation of \(h\) relative to the \(\min\) function, given the safe set \(\mathcal{C}\) defined based on \ref{eq:safe-set} it verifies:

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\min} (h_i(\xVec)) \geq h(\xVec) \quad \forall \xVec \in \mathbb{R}^n 
            \label{eq:Intersection_CBF_Comparation} \\
            &\mathcal{C} \subseteq \bigcap_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Intersection_SafeSet_Comparation}
        \end{align}
    \end{subequations}

    But, \(\hspace{0.4em} \lim_{k \to \infty} h(\xVec) = {\min}_{i \in \mathcal{I}} (h_i(\xVec)) \hspace{0.4em}\) and \( \hspace{0.4em} \lim_{k \to \infty} \mathcal{C} = \bigcap_{i \in \mathcal{I}} \mathcal{C}_i\) \par

    On par with that, the new \glsxtrshort{CBF} \(h(\xVec)\) \ref{eq:Min_smoothFunc} could restrain too much the closed-loop system in comparison to theoretical \glsxtrshort{CBF} \( \lim_{k \to \infty} h(\xVec)\). In order to relax the safe set \(\mathcal{C}\) and potentially contain \( \bigcap_{i \in \mathcal{I}} \mathcal{C}_i\): 
    
    \begin{equation}
        h(\xVec) = \frac{-1}{k}ln(\sum_{i \in \mathcal{I}} e^{-k \hspace{0.2em} h_i(\xVec)}) + \frac{b}{k}
        \label{eq:Min_smoothFunc_tighted} \\  
    \end{equation}



    Beeing \(b\) a parameter tuned according with the problem needs, knowing that if \(b = ln (N)\):

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\min} (h_i(\xVec)) \leq h(\xVec) % \leq \underset{i \in\mathcal{I}}{\max} + \frac{b}{k} \quad \forall \xVec \in \mathbb{R}^n
            \label{eq:Intersection_CBF_Comparation_tighted} \\
            &\mathcal{C} \supseteq \bigcap_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Intersection_SafeSet_Comparation_tighted}
        \end{align}
    \end{subequations}

    Making the closed-loop system less constrained but not as safe.

    \item[Union of Sets \(\setminus\) OR \(\setminus\) Maximum]

    \begin{align}
        h(\xVec) &= \frac{1}{k}ln(\sum_{i \in \mathcal{I}} e^{k \hspace{0.2em} h_i(\xVec)}) 
        \label{eq:Max_smoothFunc} \\
                 &\approx \underset{i \in \mathcal{I}}{\max} (h_i(\xVec))
        \notag  
    \end{align}

    Beeing \(k \in \mathbb{R}_{>0}\) as smoothing parameter. The derivative of \(h(\xVec)\) is equal to \ref{eq:Min_smooth_derivative} but:

    \begin{equation}
        \lambda_i(\xVec) = e^{k(h_i(\xVec)-h(\xVec))}
        \label{eq:Max_lambda_smoothFunc}
    \end{equation}

    With also \(\sum_{i \in \mathcal{I}} \lambda_i(\xVec) = 1\). \par
    Due to an over-aproximation of \(h\) relative to the \(\max\) function, given the safe set \(\mathcal{C}\) defined based on \ref{eq:safe-set} it verifies:

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\max} (h_i(\xVec)) \leq h(\xVec) \quad \forall \xVec \in \mathbb{R}^n 
            \label{eq:Union_CBF_Comparation} \\
            &\mathcal{C} \supseteq \bigcup_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Union_SafeSet_Comparation}
        \end{align}
    \end{subequations}

    But, \(\hspace{0.4em} \lim_{k \to \infty} h(\xVec) = {\max}_{i \in \mathcal{I}} (h_i(\xVec)) \hspace{0.4em}\) and \( \hspace{0.4em} \lim_{k \to \infty} \mathcal{C} = \bigcup_{i \in \mathcal{I}} \mathcal{C}_i\) \par

    Now showing a different point of view, safety is a critical factor, so in order to tight the safe set \(\mathcal{C}\) and potentially be contained within \( \bigcup_{i \in \mathcal{I}} \mathcal{C}_i\): 
    
    \begin{equation}
        h(\xVec) = \frac{1}{k}ln(\sum_{i \in \mathcal{I}} e^{k \hspace{0.2em} h_i(\xVec)}) - \frac{b}{k}
        \label{eq:Max_smoothFunc_tighted} \\  
    \end{equation}

    Beeing \(b\) a tunable parameter. Taking \(b = ln (N)\):

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\max} (h_i(\xVec)) \geq h(\xVec) % \leq \underset{i \in\mathcal{I}}{\max} + \frac{b}{k} \quad \forall \xVec \in \mathbb{R}^n
            \label{eq:Union_CBF_Comparation_tighted} \\
            &\mathcal{C} \subseteq \bigcup_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Union_SafeSet_Comparation_tighted}
        \end{align}
    \end{subequations}

    Therefore the higher \(b\), the safer the system~\ref{eq:NL-CL-System-0Backstep} but possibly too constrained by the new \glsxtrshort{CBF} \(h(\xVec)\) \ref{eq:Max_smoothFunc_tighted}. 
\end{description}



\subsubsection{Second-Order System}
\label{subsub:uniCBF_secondOrder_system}

Following the explanations given in the previous subsubsection~\ref{subsub:uniCBF_firstOrder_system}, now applying for a second-order system~\ref{eq:HH-NL-System}, more precisely, for a Backstepping~\ref{sub:backstepping} situation. \\

Given a \glsxtrshort{CLF} \(V_0(\xVec): \) and multiple \glsxtrshort{CBF} \(h0_i(\xVec) \quad \forall i \in \mathcal{I}\) combined into a \glsxtrshort{CBF} \(h_0(\xVec)\) via \ref{eq:Min_smoothFunc_tighted} or \ref{eq:Max_smoothFunc_tighted} (according it is an AND or OR operation), through the \glsxtrshort{QP}~\ref{eq:CLF-CBF-QP-relaxed-0Backstep} (or its closed-form solution~\ref{eq:closed-form_controller}) is obtained a differentiable and locally Lipschitz continuos theoretical controller \(k_0(\xVec): \mathbb{R}^n \to \mathbb{R}\). \\

In order to the second-order system~\ref{eq:HH-NL-CL-System} dynamic converge to \(k_0(\xVec)\) imposed dynamic, given multiple \glsxtrshort{CBF} \(h_i(\xVec) \quad \forall i \in \mathcal{I} \):

\begin{subequations}
    \label{eq:Backstepp_CF_multipleCBF}
    \begin{align}
        &V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) = \frac{1}{2}|| \mathbf{\xi} -  k_0(\xVec) ||^2 
        \label{eq:Backstepp_prejudice_multipleCBF}\\
        &V(\xVec, \mathbf{\xi}) = V_0(\xVec) + V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) 
        \label{eq:Backstepp_V_multipleCBF}\\
        &h_i(\xVec, \mathbf{\xi}) = h0_i(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) \quad \forall i \in \mathcal{I}
        \label{eq:Backstepp_h_multipleCBF}
    \end{align}
\end{subequations}

The resultant \glsxtrshort{CBF} \(h_i(\xVec, \mathbf{\xi}) \quad \forall i \in \mathcal{I}\), one more time can be merged via \ref{eq:Min_smoothFunc_tighted} or \ref{eq:Max_smoothFunc_tighted} (according it is an AND or OR operation) into a single \glsxtrshort{CBF} \(h(\xVec, \mathbf{\xi})\). However there is another way in this case to have the new single \glsxtrshort{CBF} \(h(\xVec, \mathbf{\xi})\), so, considering the second-order system~\ref{eq:HH-NL-System} and \(lo \in \{\max, \min\}\):

\begin{align}
        \underset{ i \in \mathcal{I}}{lo}(h0_i(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi})) &= \underset{ i \in \mathcal{I}}{lo}(h0_i(\xVec)) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) \approx 
                                                                                                \notag\\
                                                                                               &\approx h_0(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) = 
                                                                                               \notag\\
                                                                                               &=h(\xVec, \mathbf{\xi})
                                                                                               \label{eq:Backstepp_h_multipleCBF2}
\end{align}

\begin{equation}
        \dot{h}(\xVec, \mathbf{\xi}) = \underbrace{L_fh_0(\xVec) + L_Gh_0(\xVec)\mathbf{\xi} - L_fV_{\mathbf{\xi}}(\xVec, \mathbf{\xi})}_{L_fh(\xVec, \mathbf{\xi})} + (\underbrace{- L_GV_{\mathbf{\xi}}(\xVec, \mathbf{\xi})}_{L_Gh(\xVec, \mathbf{\xi})}) \uVec
        \label{eq:Backstepp_h_multipleCBF_derivatives2}
\end{equation}


Having \glsxtrshort{CBF} \(h\), the second-order solution, a differentiable and locally Lipschitz continuos controller \(k(\xVec, \mathbf{\xi}): \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}\) is obtained using the \glsxtrshort{QP}~\ref{eq:CLF-CBF-QP-relaxed-1Backstep} (or its closed-form solution~\ref{eq:closed-form_controller}). \\

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item There are other possible variations of a second-order system that invalidates \ref{eq:Backstepp_h_multipleCBF_derivatives2} like the unicycle model that will be shown next chapter.
\end{itemize}
\end{tcolorbox} 



\newpage %Só para ajeitar




\section{Single Parameter Optmization}
\label{sec:Single_Parameter_Optmization}


%The optimization problems such as \ref{eq:CLF-CBF-QP}, \ref{eq:CLF-CBF-QP-relaxed}, \ref{eq:CLF-CBF-QP-relaxed-0Backstep}, \ref{eq:CLF-CBF-QP-relaxed-1Backstep}, can be processed by making use of solvers able to optimize multiple variables using diffrent methods like the ones presented here, or if it is feasible using the closed-form solution of the optimization problem like \ref{sub:closed_form}. However, the methods shown will just be compreended to one optimization variable, owing to the specifications is pretended to achieve with the  proposed decision algorithms~\ref{sec:Proposed_Propagation_Algorithms}. \par 
The optimization of the respective parameter is based on minimizing a cost function \(g:\mathbb{R} \to \mathbb{R}\), which in the context of this work expected to be quasi-convex~\footnote{ A quasi-convex function \(f:\mathcal{S} \to \mathbb{R}\), beeing \(\mathcal{S}\) a convex set, verifies \(\forall x,y \in \mathcal{S}\) and \(\lambda \in [0;1]\), \(f(\lambda x + (1-\lambda)y) \leq max\{f(x), f(y)\}\)   } along the optimization variable \(x \in \mathbb{R}\), and to do that there are algorithms such as the highly used gradient descent~\ref{subsec:Gradient_Descent}  and binary search~\ref{subsec:Binary_Search} method.



\subsection{Gradient of the Cost Function - Discretization}
\label{subsec:Gradient_Cost_Function_Discretization}

In both algorithms is make use of \(g(x)\) gradient in order to essentially indicate the direction the optimization variable \(x\) must take in order to minimize the cost. More than that, if \(g(x)\) is a quasi-convex function to quantifie the proximity to a local optimal point. \par
If the \(\nabla g(x)\) extact equation is not possible to obtain then:

\begin{equation}
    \nabla g(x) = \lim_{dev \to 0} \frac{g(x + dev) - g(x)}{dev}
    \label{eq:Gradient_formula}
\end{equation}

Is approximated by:

\begin{equation}
    \nabla g(x, dev) = \frac{g(x + dev) - g(x)}{dev}
    \label{eq:Gradient_Discretization}
\end{equation}

With \(dev \in \mathbb{R}\) sufficiently close to \(0\). \par

If \(g(x)\) has relevant noise could compromise the value obtained of \(\nabla g(x)\), it could be needed increase the number of samples (\(nS \in \mathbb{R}\)) in the neighborhood of \(x\) separated by \(devN\) between them, so digitally:
\SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$nS$, $devN$, $dev$, $g$, $x$}
    \KwResult{$mean(grad_i)$}
    $i\gets -1$\;
    \While{$i < nS -1$}{
        $devN_i \gets  \frac{1}{2} \times (i + (i \hspace{0.2em} mod \hspace{0.2em} 2) ) \times devN $ \;
        $grad_i \gets \nabla g(x + (-1)^{i}  \times devN_i, (-1)^i \times dev)$ \Comment*[r]{Equation \ref{eq:Gradient_Discretization}}
        $i\gets i + 1$\;    
        }
    \caption{Discrete Gradient Function (DGF)} \label{alg:Discrete_Gradient_Function}
  \end{algorithm}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item It is possible to digitally obtain \(\nabla g : \mathbb{R}^n \to \mathbb{R}^n\) by otaining each row \(r\) separately with \(dev_r \in \{ dev_1, ..., dev_n \} \). Beeing \(dev_r \) a column vector, just equal to the divisor \(dev\) in the respective row element (\(dev = ||dev_r||\)).
\end{itemize}
\end{tcolorbox} 


%\newpage %Só para ajeitar

\subsection{Gradient Descent}
\label{subsec:Gradient_Descent} 

This method is characterized by an iterative update of the optimization variable \(x\), based on the current variable value and function \(g(x)\) gradient responsible to indicate the evolution direction of \(x\) in order to minimize the cost:

\begin{equation}
    x_{k+1} = x_k + \underbrace{(- a \nabla g(x_k))}_{step_{k}}
    \label{eq:Gradient_Descent_Normal}
\end{equation}

where \(a\) is an arbitrary constant for the step intensity adjustment . Assuming \(g(x)\) is a quasi-convex function, the closure to local minimum the smaller the gradient (or step) module. \par

However, this method has another variations including adding "\underline{momentum}" where is had in account the previous iteration step applied:

\begin{equation}
    x_{k+1} = x_k + b \underbrace{(x_k - x_{k-1})}_{step_{k-1}} - a \nabla g(x_k)
    \label{eq_Gradient_Descent_Momentum}
\end{equation}

where \(b\) is an argument of this algorithm like \(a\) responsible to tune the previous step intensity. \par
According to \cite{hao2025analysis} there are also another two methods. Refering to one of them, this expands on the idea of momentum called \underline{Nesterov's Accelerated Gradient}, using a predictive step based on the previous one: 

\begin{equation}
    \begin{array}{rl}
      y_k &=  x_k + b (x_k - x_{k-1})  \\
      x_{k+1} &= y_k + (\underbrace{- a \nabla g(y_k)}_{predictive  \hspace{0.2em}step})
    \end{array}
    \label{eq_Gradient_Descent_Nesterov}
\end{equation}

where \(a\) now tunes the predictive step. \par

Finally the \underline{Triple-Momentum} method which presents an output equation and the parameters applied on the previous step are distinct in all usages of it:

\begin{equation}
    \begin{array}{rl}
      z_k &= x_k + d (x_k - x_{k-1}) \\
      y_k &=  x_k + c (x_k - x_{k-1})  \\
      x_{k+1} &= x_k + b(x_k - x_{k-1}) - a \nabla g(y_k)
    \end{array}
    \label{eq_Gradient_Descent_Nesterov}
\end{equation}

where \(a, b, c, d\) are the respective steps intensity tuning parameter. \\




\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item The method supports multiple optimization variables.
\end{itemize}
\end{tcolorbox} 







%\newpage %Só para ajeitar


\subsection{Binary Search}
\label{subsec:Binary_Search}

This method is composed by two phases, beeing the first one resposible to obtain two extreme points \((xI,xS) \in \mathbb{R} \times \mathbb{R}\) such that the Bolzano theorem~\footnote{Bolzano's theorem says that given the continuos function \(f:[a;b] \subset \mathbb{R} \to \mathbb{R}\) and \(f(a)f(b) < 0\), then \(\exists x \in  ]a; b[\) such that \(f(x) = 0\)} is verified for the continuos gradient funtion (\(  x^*= \{ x \in ]xI; xS[ \hspace{0.1em}  : \hspace{0.25em} \exists x \hspace{0.25em}\nabla g(x) = 0 \} \)). In order to achieve that, is used an variation of the gradient descent method~\ref{eq:Gradient_Descent_Normal}, with a starting point \(x_0\):

\begin{equation}
    x_{k+1} = x_k + \underbrace{(- a \nabla g(x_0))}_{step}
    \label{eq:Binary_Search_Phase1_gradDesc}
\end{equation}

Where the \(step\) is always equal. That beeing said, the phase 1 algorithm: 

\SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$xI$, $a$}
    \KwResult{$(xI,xS)$}
    $x_0\gets xI$\;
    $xS\gets xI$\;
    \While{$\nabla g(xI) \times \nabla g(xS) > 0$}{
        $xS \gets xI - a \nabla g(x_0)$    \Comment*[r]{Equation \ref{eq:Binary_Search_Phase1_gradDesc}}
        \If{$\nabla g(xI) \times \nabla g(xS) > 0$}{
            $xI\gets xS$\;
        }
    }
    \caption{Binary Search Phase 1} \label{alg:Binary_Search_Phase_1}
  \end{algorithm}


The second phase consists in iteratively taking the extreme points and do a weighted mean obtaining a new delimiter that replace one of the previous points while preserving the application of the Bolzano theorem for next iterations, narrowing the interval and approximating the points to a local minimum.   

\SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$xI$, $xS$, $tI$, $w$}
    \KwResult{$xMid$}
    \While{$i < tI$ \textbf{AND} Conditions$^*$}{   
        $xMid \gets w \times xI + (1-w) \times xS $ \Comment*[r]{Weighted Mean}
        \eIf{$\nabla g(xMid) > 0$}{
            $xS\gets xMid$\;
        }{
            $xI\gets xMid$\;
        }
    }
    \caption{Binary Search Phase 2} \label{alg:Binary_Search_Phase_2}
  \end{algorithm}



\newpage %Só para ajeitar


\section{Constraint Contour Point}
\label{sec:Constraint_Contour_Point}

Having the objective of global assympyotically stability of a system, the system converges a desired equilibrium point \(\bar{\xVec}_F\) but kept outside a bounded unsafe set \(\mathcal{B} \subset \mathbb{R}^n\). Depending on certain decision algorithms, such as \ref{eq:CLF-CBF-QP-relaxed} which uses \glsxtrshort{CLF-CBF}, by pursuing another equilibrium point \(\bar{\xVec}_I\) first, the path taken by the system's states \(\xVec \in \mathbb{R}^n\) until \(\bar{\xVec}_F\) could be oriented in order to be a better fit for problem demands, specially to deal more efficiently with the implications of an unsafe set \(\mathcal{B}\).\\

\subsection{Constraint Fit}
\label{subsec:Constraint_Fit}

The program formulated in \ref{subsec:Constraint_Contour_Point_Obtaining} responsible to obtain a new equlibrium point is characterized by its application in two-dimensional Euclidian Space safety \underline{constraints}. Since the safe set \(\bar{\mathcal{B}}\) (complement of \(\mathcal{B}\)) corresponds to the 0-superlevel set of an unkown safety function \(h_{unkown}: \mathbb{R}^n \to \mathbb{R}\), then via logical operations~\ref{sub:unifying_CBF} smoth-approximations (AND~\ref{eq:Min_smoothFunc_tighted}, OR~\ref{eq:Max_smoothFunc_tighted}), combining the different 0-superlevel sets \(\mathcal{C}_i \subset \mathbb{R}^n \quad \forall i \in \mathcal{I} = \{1,...,p\}\) of the the functions \(h_i: \mathbb{R}^n \to \mathbb{R}\):

\begin{equation}
    \begin{array}{l}
        h_{unkown}(\xVec) \approx h(\xVec) = merge(h_i(\xVec)) \quad \forall i \in \mathcal{I} \\
        \bar{\mathcal{B}} \subseteq \mathcal{C} = \{\xVec \in \mathbb{R}^n: h(\xVec) \geq 0\}
    \end{array}
    \label{eq:Polytope_fit}
\end{equation}

\(\bar{\mathcal{B}}\) can also be approximated by an ellipsoide:

\begin{align}
        h_{unkown}(\xVec) \approx h(\xVec) &= \frac{1}{2}((\mathbf{O}\xVec - p)^{\top}\mathbf{A_{xis}}(\mathbf{O}\xVec - p) - 1) \label{eq:Ellipsoidal_fit} \\
        &st. \quad \bar{\mathcal{B}} \subseteq \mathcal{C} \label{eq:Ellipsoidal_fit_Condition}
\end{align}

With \(\mathbf{O} \in \mathbb{R}^{c \times n}\) as constrained states observer (\(c\) equal to the constrained states), \(\mathbf{A_{xis}} \in \mathbb{R}^{c \times c}_{\succ 0}\) and \(p \in \mathbb{R}^{c}\) susceptible to an optimization that tries to minimize the size of the set \(\mathcal{C} \cap \mathcal{B}\) or at least choosing them such that is garanteed \ref{eq:Ellipsoidal_fit_Condition}. Even though the ellipsoidal approximation is less accurate as is not able to capture the shapes of other figures than ellipses, it shows sufficient for some problems, it provides less complexity and given its properties, suited for some strategies planning. \\


\subsection{State Propagation Simulation}
\label{subsub:State_Propagation_Simulation_Algorithm}

\subsubsection{\glsxtrfull{RK} Methods}
\label{subsub:Runge-Kutta_Methods}

Its a technique used to solve \glsxtrfull{ODE}s~\footnote{An \glsxtrshort{ODE} is characterized by having a independent variable \(x \in \mathbb{R}\) (commonly time \(t\)) and dependent variables \(y \in \mathbb{R}^n\) and its derivatives relative to \(x\), such that an \glsxtrshort{ODE} corresponds to an equation equivalent to \(g(x,y, \frac{dy}{dx}, ... \frac{d^{n}y}{dx^n})= 0\)  } initial-value problems~\footnote{A initial-value-problem is an \glsxtrshort{ODE} \(\dot{\mathbf{y}}(t) = f(t, \mathbf{y}(t))\) with \( f: \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n\) with an initial condition \(t_0,y_0\)} (\(\frac{dy}{dx} = f(x,y)\) with \((x_0, y_0)\)), i.e., to obtain an approximation of a non-linear system solution \(y(x)\) at some point in its domain without using the higher order derivatives of it. \\

The \underline{\textbf{Euler}} method shows the most degradation of local truncature error\footnote{A local truncature error corresponds to the error caused by the integrative effect of an iteration of a Runge-Kutta method} as the required (arbitrary valued) step \(\Delta x = x_{n+1} - x_n, \hspace{0.2em} n \in \mathbb{N}_0\) increases but quicker to compute.

\begin{equation}
    y_{n+1} = y_{n} + \Delta x f(x_{n}, y_{n})
    \label{eq:Euler_Method}
\end{equation}

This method has just one stage, while the next \glsxtrfull{RK}s expanded on it such that allow more. Those stages correspond to intermediate point derivatives based on predictive steps. Introducing to the \textbf{Explictit \glsxtrlong{RK}} \label{subsubsub:RK_Explicit}:


\begin{subequations}
    \begin{align}
        &y_{n+1} = y_n + \Delta x \sum^s_{i=1}b_i k_i \label{eq:Update_RKbased} \\
        &k_i = f(x_n + \Delta x c_i, y_n + \Delta x \sum^{i-1}_{j=1} a_{ij}k_j) \label{eq:Derivatives_Explicit-RK}  
    \end{align}
    \label{eq:Explicit-RK}  
\end{subequations}

Where \(s\) is the number of stages, \(\mathbf{a}\) the \glsxtrfull{RK} matrix, with \(a_{ij} \in \mathbb{R}\) and usually tuned such that \(\sum^{i-1}_{j=1} a_{ij} = c_i\), \(\mathbf{b} \in \mathbb{R}^s\) are the weights, so \(\sum^s_{i=1} b_i = 1\), \(\mathbf{c} \in \mathbb{R}^s\) are the nodes and \(c_i \in [0;1]\) . These parameters are usually presented by a Butcher tableau:

\begin{equation}
    \begin{array}{c|cccc}
        0\\
        c_2 & a_{21}\\
        \vdots & \vdots &\ddots \\
        c_s & a_{s;1} & \hdots & a_{s;s-1}\\
        \hline
        & b_1 & \hdots & b_{s-1} & b_s  
    \end{array} = 
    \begin{array}{c|c}
        \mathbf{c} & \mathbf{a} \\
        \hline
        & \mathbf{b}^\top  
    \end{array}
    \label{eq:Explicit_Butcher_Tableu}
\end{equation}


The \underline{\glsxtrshort{RK} 4th order} is defined by \ref{eq:Explicit-RK} and its tableau:

\begin{equation}
    \begin{array}{c|cccc}
        0\\
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & 0 & \frac{1}{2} \\
        1 & 0 & 0 & 1\\
        \hline
        & \frac{1}{6} & \frac{1}{3} & \frac{1}{3} & \frac{1}{6}  
    \end{array}
    \label{eq:RK4_Tableu}
\end{equation}

Although this method solution is slower comparatively to Euler~\ref{eq:Euler_Method}, its accuracy is higher. \par
The \textbf{Adaptive \glsxtrlong{RK}} \label{subsubsub:RK_Adaptive} method, with a adaptive step size \(\Delta x\) according to the estimated local truncature error. If its lower than some treshold, the step size increases and the solutions obtained faster since that point, while if it increases is reduced promoting a accurate solution, but with \(\Delta x \in [\Delta x_{min}; \Delta x_{max}]\).  The error estimations are calculated based on two different methods of one order more than the other. For this new \glsxtrlong{RK} variation the Butcher Tableau is: 

%If the error is higher than some defined relative tolerance then the step size is reduced and the solution given the new step recomputed at the point, while if its lower than some treshold, the step size increases and the solutions obtained faster sice that point.

\begin{equation} 
    \begin{array}{c|c}
        \mathbf{c} & \mathbf{a} \\
        \hline
        & \mathbf{b}^\top  \\
        & \mathbf{b}^{*\top }
    \end{array}
    \label{eq:Explicit_Butcher_Tableu}
\end{equation}


The low and high order solution are based equally on \ref{eq:Update_RKbased}:

\begin{subequations}
    \begin{align}
        y_{n+1} = y_n + \Delta x \sum^{s}_{i=1}b_i k_i \text{  (low-order/output sol.)} \label{eq:Update_Output_adaptiveRK} \\
        z_{n+1} = y_n + \Delta x \sum^{s}_{i=1}b^*_i k_i \text{  (high-order sol.)} \label{eq:Update_HighOrder_adaptiveRK} 
    \end{align}
    \label{eq:Update_Adaptive-RK}  
\end{subequations}

Where the intermediate derivatives \(k_i\) functions keep the same equation \ref{eq:Derivatives_Explicit-RK} and are equal for both (low and hig order methods) reducing some computing time. The local truncature error estimations ig given by:

\begin{equation}
    e_{n+1} = |z_{n+1} - y_{n+1}|
    \label{eq:Local_Truncature_Erro_Estimation_AdaptiveRK}
\end{equation}

This error affects the new step value, the \underline{Dormand Prince} method~\cite{kimura2009dormand} with its particular update step size iteration. This method Butcher Tableau is:

\begin{equation}
    \begin{array}{c|ccccccc}
        0\\
        \frac{1}{5} & \frac{1}{5}\\
        \frac{3}{10} & \frac{3}{40} & \frac{9}{40} \\
        \frac{4}{5} & \frac{44}{45} & -\frac{56}{15} & \frac{32}{9}\\
        \frac{8}{9} & \frac{19372}{6561} & -\frac{25360}{2187} & \frac{64448}{6561} & -\frac{212}{729}\\
        1 & \frac{9017}{3168} & -\frac{355}{33} & \frac{46732}{5247} & \frac{49}{176} & -\frac{5103}{18656}\\
        1 & \frac{35}{384} & 0 & \frac{500}{1113} & \frac{125}{192} & -\frac{2187}{6784} & \frac{11}{84}\\
        \hline
        & \frac{35}{384} & 0 & \frac{500}{1113} & \frac{125}{192} & -\frac{2187}{6784} & \frac{11}{84} & 0  \\
        & \frac{5179}{57600} & 0 & \frac{7571}{16695} & \frac{393}{640} & -\frac{92097}{339200} & \frac{187}{2100} & \frac{1}{40}  
    \end{array}
    \label{eq:Dormand-Prince_Tableu}
\end{equation}

The \(k_7\) derivative is calculated at the predicted point obtained by the low order solution \(y_{n+1}\). The update size step equation as:

\begin{equation}
    \Delta x_{new} = \Delta x \Bigl( \frac{\epsilon\Delta x}{2e_{n+1}}\Bigr)^{\frac{1}{5}}
    \label{eq:Update_Size_Step_Equation}
\end{equation}

With \(\epsilon\) as the error tolerance equal to \(10^{-5}\) in the default version of \href{https://www.mathworks.com/help/matlab/ref/ode45.html}{Matlab ode45}.

\subsubsection{State Propagation Simulation Algorithm}
\label{subsub:State_Propagation_Simulation_Algorithm}

Finding a new equilibrium point \(\bar{\xVec_I}\) requires simulate the state convergence. Aiming to simulate a system evolution based on a decision algorithm, given the ordinary diferential states dynamics equations \(\dot{\xVec} = f(\xVec,\uVec)\):

  \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$f$, $\xVec_0$, $tI$, $\Delta t$, $arg^*$}
    \KwResult{$\xVec$, $\uVec$}
    $i \gets 0$ \;
    \While{$i < tI$ \textbf{AND} Conditions$^*$}{   
        $\uVec_i \gets decision^*(\xVec_i, arg^*)$ \Comment*[r]{Decision Algorithm}
        $\xVec_{i+1} \gets integration^*(f, \xVec_i, \uVec_i, \Delta t)$ \Comment*[r]{\small Euler~\ref{eq:Euler_Method},  \glsxtrshort{RK} 4~\ref{subsubsub:RK_Explicit} or DP~\ref{subsubsub:RK_Adaptive}...}
        $i \gets i +1$ \;
    }
    \caption{State Propagation Simulation (SPS)} \label{alg:State_Propagation_Simulation}
  \end{algorithm}


\newpage %Só para ajeitar

\subsection{Constraint Contour Point Algorithm}
\label{subsec:Constraint_Contour_Point_Algorithm}

\subsubsection{Natural Convergence Direction}
\label{subsubsec:Intersection_Vector}

The new point is obtained accounting with the natural movement direction of those states when they would reach the unsafe zone, which is provided approximately by the propagation algorithm~\ref{alg:State_Propagation_Simulation} if the safety constraint defined by \(h\) (\ref{eq:Polytope_fit} or \ref{eq:Ellipsoidal_fit}) was inactive. 

% \SetKwComment{Comment}{/* }{ */}
%   \begin{algorithm}
%     \KwData{$\xVec$}
%     \KwResult{$dir$}
%     \While{$h(\xVec) > 0$ \textbf{AND} Conditions$^*$}{   
%         $\uVec \gets decision(\xVec)$ \Comment*[r]{Decision Algorithm}
%         $dir \gets \frac{f(\xVec, \uVec)}{||f(\xVec, \uVec)||}$ \Comment*[r]{System Dynamic at (\(\xVec, \uVec\)) Normalized}
%         $\xVec \gets integration(\xVec, \uVec)$ \Comment*[r]{Euler\ref{}, Runge-Kutta \ref{}, \ref{}, \ref{}}
%     }
%     \caption{Natural Convergence Direction Vector} \label{alg:Natural_Convergence_Direction_Vector}
%   \end{algorithm}

%   OR

  \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$f$, $\xVec_0$, $tI$, $\Delta t$, $args^*$}
    \KwResult{$dir$}
    $(\xVec, \uVec) \gets \text{SPS}\hspace{0.1em}(f, \xVec_0, tI, \Delta t, args^*) \hspace{0.25em} \begin{bmatrix*}[c]  conditions^* \gets h(\xVec) > 0 \\ decision^* \\ integration^* \end{bmatrix*}$ \Comment*[r]{Algorithm \ref{alg:State_Propagation_Simulation}} 
    $dir \gets \frac{f(\xVec_{END}, \uVec_{END})}{||f(\xVec_{END}, \uVec_{END})||}$ \Comment*[r]{System Dynamic at (\(\xVec_{END}, \uVec_{END}\)) Normalized}
    \caption{Natural Convergence Direction Vector (\(NCDV\))} \label{alg:Natural_Convergence_Direction_Vector}
  \end{algorithm}

  This vector points to the direction where the system can aproximate more of \(\bar{\xVec}_F\) given some decision algorithm and assuming is always safe.


\subsubsection{Constraint Tangent Point}
\label{subsubsec:Constraint_Tangent_Point}

From the vector \(\mathbf{v} = \bigl[\begin{smallmatrix} v_1\\ v_2 \end{smallmatrix} \bigr]\) obtained in \ref{alg:Natural_Convergence_Direction_Vector} is deduced a tangent point in the respective constraint. This point is the most far from the eminent of colliding unsafe set \(\mathcal{C}\) given a perpendicular direction to the obtained vector.%, therefore the new equilibrium point translates as the necessary variation the system states need in that perpendicular direction to pass the unsafe zone.                
 Assuming it's always used the ellipsoidal fit~\ref{eq:Ellipsoidal_fit}, where \(\mathbf{A_{xis}} = \bigl[\begin{smallmatrix} a_x&a_u \\ a_u&a_y \end{smallmatrix} \bigr] \in \mathbb{R}^{c \times c}_{\succ 0}\), \(\mathbf{O} \in \mathbb{R}^{c \times n}\) and \(\mathbf{p} \in \mathbb{R}^{c}\), the new point could be obtained using a closed-form solution (to see the full deduction \ref{app:CL_Ellipsoide_Tangent_Point}):


\begin{align}
    &d = \frac{a_u\sqrt{2}}{\sqrt{a_xa_y}} \label{eq:Constraint_Tangent_Point_auxElement1} \\
    \notag \\
    &l = \sqrt{\frac{a_x}{2a_y}} \label{eq:Constraint_Tangent_Point_auxElement2} \\
    \notag \\
    &q = \pm\sqrt{\frac{1}{\frac{a_x}{2}(v_1^2 + v_2^2) + a_yv_1^2}}  \label{eq:Constraint_Tangent_Point_auxAuxElement} \\
    \notag \\
    &\mathbf{z}_i = \begin{bmatrix}  v_2 q \\ v_1 q \end{bmatrix} \quad i \in \{1,2,3,4\} \label{eq:Constraint_Tangent_Point_auxVector} 
\end{align}

In order to obtain both solutions from the perpedincular vectors to \(\mathbf{v}\):

\begin{subequations}
    \begin{align}
        &\mathbf{r}_1 = \argminA_{\mathbf{z}_i \in \mathbb{R}^2} = \begin{bmatrix} a_yv_2 & - a_xv_1 \end{bmatrix} \mathbf{z}_{i} \label{eq:Constraint_Tangent_Point_bc_per} \\
        &\mathbf{r}_2 = -\mathbf{r}_1 \label{eq:Constraint_Tangent_Point_bc_perp_inverse}
    \end{align}
    \label{eq:Constraint_Tangent_Point_bc}
\end{subequations}   

%&\mathbf{r}_2 = \argmaxA_{\mathbf{z}_i \in \mathbb{R}^2} = \begin{bmatrix} a_yv_2 & - a_xv_1 \end{bmatrix} \mathbf{z}_{i} \label{eq:Constraint_Tangent_Point_bc_inverse}
    

With \(\mathbf{r} = \bigl[\begin{smallmatrix} \mathbf{r}_1&\mathbf{r}_2\end{smallmatrix} \bigr]\). The respective points are obtained with:

\begin{equation}
    \mathbf{s}_j = \frac{\begin{bmatrix} d \hspace{0.1em}r_{1j} + l \hspace{0.1em}r_{2j} \\ d \hspace{0.1em} r_{2j} - l \hspace{0.1em} r_{1j}\end{bmatrix}}{d^2 + l^2} + \mathbf{p} \quad j\in \{1,2\}
    \label{eq:Constraint_Tangent_Points}
\end{equation}

The closest-point to the current state is chosed as the intermediate equilibrium point:

\begin{equation}
    \mathbf{O}\bar{\xVec_I} = \argminA_{\mathbf{s}_j \in \mathbb{R}^2} ||\mathbf{s}_j - \xVec|| 
    \label{eq:Intermediate_Equilibrium_Point}
\end{equation}



\newpage %Só para ajeitar


\section{Proposed Decision Algorithms}
\label{sec:Proposed_Propagation_Algorithms}

All the proposed algorithms are based on \glsxtrshort{CLF-CBF}~\ref{sec:clf_cbf} decision algorithm, having the objective of stabilizing a system while prioritizing its safety. Using \glsxtrshort{CLF-CBF} allows a relative lower computational effort, in particular its closed form solution~\ref{sub:closed_form}, compared to other techniques, however it lacks adaptive power as its constant parameters consistently dictates an inneficient system behavior for probably the majority of the encoutered scenarios. \\    


\subsection{\glsxtrfull{A-JO}}
\label{subsec:Just_Optimized_Algorithm}

Optimizing parameters can improve the system behaviour for the scenario is deparing with. And an eventual optimization make its behaviour more suited to actuate in an multi-scenario environment. \par
Based on single parameter optimization~\ref{sec:Single_Parameter_Optmization}, the proposed technique is centered on the slope \(\alpha\) of the \glsxtrshort{CBF} constraint function \(\alpha:\mathbb{R} \to \mathbb{R}\) (\glsxtrshort{CLF-CBF} \glsxtrshort{QP}~\ref{eq:CLF-CBF-QP-relaxed}) as an optimization variable:

\begin{equation}
    \alpha(h) = \alpha h
    \label{eq:CBF_Alpha_Formulation_AJO}
\end{equation}

The parameter is optimized using \ref{eq:Gradient_Descent_Normal} but for that is required a cost function \(g_{\xVec_0}:\mathbb{R} \to \mathbb{R}\) which accounts with a whole predction horizon:

\begin{equation}
    \begin{array}{ll}
        g_{\xVec_0}(\alpha) &= g(\alpha, \xVec_0 = cte)  \\   
        &= \sum_{k=0}^{N}\xVec_{k+1}(\xVec_k,\uVec_k)^{\top}\mathbf{Q}\xVec_{k+1}(\xVec_k,\uVec_k) +  \uVec_k(\alpha,\xVec_k)^{\top}\mathbf{R}\uVec_k(\alpha,\xVec_k) \\
        %&= \sum_{k=0}^{N}\xVec_{k+1}(\alpha,\xVec_k)^{\top}\mathbf{Q}\xVec_{k+1}(\alpha,\xVec_k) +  \uVec_k(\alpha,\xVec_k)^{\top}\mathbf{R}\uVec_k(\alpha,\xVec_k) \\
        &= \sum_{k=0}^{N}\xVec_{k+1}(\alpha,\xVec_0)^{\top}\mathbf{Q}\xVec_{k+1}(\alpha,\xVec_0) +  \uVec_k(\alpha,\xVec_0)^{\top}\mathbf{R}\uVec_k(\alpha,\xVec_0) \\
        &= \mathbf{X}(\alpha,\xVec_0)^{\top}\tilde{\mathbf{Q}}\mathbf{X}(\alpha,\xVec_0) +  \mathbf{U}(\alpha,\xVec_0)^{\top}\tilde{\mathbf{R}}\mathbf{U}(\alpha,\xVec_0)
    \end{array}
    \label{eq:Cost_Function_Alpha}
\end{equation}

Where \(N\) is equivalent to the total of samples, \(\xVec_0\) the present state, \(\mathbf{Q} \in \mathbb{R}^n_{\succ 0}\) is the states cost weight matrix, \(\mathbf{R} \in \mathbb{R}^m_{\succ 0}\) is the inputs cost weight matrix.  \par

Because of the conditioned control input values given by \glsxtrshort{CLF-CBF} controller, the gradient \(\nabla g_{\xVec_0}(\alpha)\) exact equation is difficult to have it, hence is required a discretization of it~\ref{subsec:Gradient_Cost_Function_Discretization}. Which leds to the exact function of \(g_{\xVec_0}(\alpha)\) which is not known as \(\uVec_k\) and \(\xVec_{k+1}\) could not be obtained directly. Like it was said, \(\uVec_k\) is obtained based on a closed-form \glsxtrshort{CLF-CBF} controller~\ref{sub:closed_form}, governed by a \glsxtrshort{CLF} \(V:\mathbb{R}^n \to \mathbb{R}\) and a \glsxtrshort{CBF} \(h:\mathbb{R}^n \to \mathbb{R}\). Meanwhile, \(\xVec_{k+1}\) is given by an explicit \glsxtrshort{RK}\ref{subsubsub:RK_Explicit} in particular defined by the fourth order \glsxtrshort{RK} Butcher Tableau\ref{eq:RK4_Tableu}, so given the system dynamics equation \(f:\mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n\) and integration time interval \(\Delta t\):

\begin{equation}
    \begin{bmatrix} \mathbf{X}(\alpha,\xVec_0) & \mathbf{U}(\alpha,\xVec_0) \end{bmatrix} = SPS(f, \xVec_0, N, \Delta t, V, h, \alpha) \hspace{0.2em} \begin{bmatrix*}[l] conditions^* \gets V(\xVec_i) \geq tol\\ decision^* \gets \text{\glsxtrshort{CLF-CBF}~\ref{sub:closed_form} } \\ integration^* \gets \text{\glsxtrshort{RK}4~\ref{eq:RK4_Tableu}} \end{bmatrix*}
    \label{eq:CLF-CBF_RK4_Propagation}
\end{equation}

With \(SPS\) (State Propagation Simulation) as the algorithm \ref{alg:State_Propagation_Simulation}. So, choosing \(nS\) number of samples, \(devS\) referent to the distance between them, \(dev\) the discretization step and with \(g_{\xVec_0}(\alpha)\) given by \ref{eq:CLF-CBF_RK4_Propagation}:

\begin{equation}
    \nabla g_{\xVec_0}(\alpha) = DGF(nS,devS,dev, g_{\xVec_0},\alpha)  
    \label{discrete_gradient_function}
\end{equation}

With \(DGF\) (Discrete Gradient Function) as the algorithm \ref{alg:Discrete_Gradient_Function}  and therefore enabling \ref{sec:Single_Parameter_Optmization} and the optimization of the slope \(\alpha\).\par  
Theoretically the CBF should garantee safety, however due to not be a continuos system (or integration errors), having a high \(\alpha\) can lead to unsafe sets and can also provide a demand on a not possible control input values to keep the system safe, but if that happens is expected to reflect on the path taken, control inputs and therefore penalize in the cost function.

\subsection{Double \glsxtrshort{CLF}}
\label{subsec:Double_CLF}


The \glsxtrshort{CLF-CBF} returns control input values based on purely the current state but with a relative similar behaviour to a single horizon MPC (in particular with euler integration to define next state) with a \glsxtrshort{CBF} constraint.  The global (infinite horizon) suboptimal solutions could lead to states relatively close to the barrier (given some safety tunable defenitions) and unwanted inputs to deviate from it. So, changing the notion of stability by making the system follow another equilibrium point first could help mitigate the problems. This technique focus on the transition between two \glsxtrshort{CLF} aiming to converge for each of the equilibrium points, the current and next. \par


With constraint contour point algorithm~\ref{subsec:Constraint_Contour_Point_Algorithm} is obtained the new equilibrium point \(\bar{\xVec_C} \in \mathbb{R}^n\). Given a system, with dynamics \(f:\mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}^n \), at the sate \(\xVec_0 \in \mathbb{R}^n\), a \glsxtrshort{CLF} \(V\) that targets \(\bar{\xVec_N} \in \mathbb{R}^n\) and \glsxtrshort{CBF} \(h\), the new equilibrium point could be obtained:

\begin{equation}
    \mathbf{v} = NCDV(f, \xVec_0, N, \Delta t, V, h) \hspace{0.2em} \begin{bmatrix*}[l] decision^* \gets \text{\glsxtrshort{CLF-CBF}~\ref{sub:closed_form} } \\ integration^* \gets \text{\glsxtrshort{RK}4~\ref{eq:RK4_Tableu}} \end{bmatrix*}
    \label{eq:New_Equlibrium_Point_DirVec_CLF-CBF_RK4}
\end{equation}

Where \(NCDV\) (Natural Convergence Direction Vector) is algorithm~\ref{alg:Natural_Convergence_Direction_Vector}. Given the direction vector \(\mathbf{v}\), assuming an ellipsoidal fit \ref{eq:Ellipsoidal_fit}, with the constraint tangent point  closed form solution~\ref{subsubsec:Constraint_Tangent_Point} like this obtain \(\bar{\xVec_C}\).\\


\subsubsection{\glsxtrfull{A-CLF-S}}
\label{subsubsec:CLFs_Summed_Algorithm}

From both equilibrium points is defined two \glsxtrshort{CLF}, \(V_C\) following the current equilibrium point \(\bar{\xVec_C}\) and \(V_N\) stabilizing in \(\bar{\xVec_N}\). By doing a wheighted mean between both \glsxtrshort{CLF} is defined a new one (a generalization for multiple \glsxtrshort{CLF} can be found in \ref{app:M_CLF_Transition_Formulation}):

\begin{align}
    V_1(\xVec) = 
    \begin{cases}
        \sigma(\xVec) V_{C}(\xVec) + (1 - \sigma(\xVec))V_{N}(\xVec) \\
        \sigma(\xVec) = \frac{1}{1+e^{-\zeta V_{C}(\xVec)}} 
    \end{cases}
    \label{eq:1stPhase_2-CLF-S}
\end{align}

\begin{align}
    V(\xVec) = 
    \begin{cases}
        V_1(\xVec) \quad \xVec \in \mathcal{S}\\
        V_2(\xVec) = V_{N}(\xVec) \quad \xVec \in \bar{\mathcal{S}}
    \end{cases}
    \label{eq:New_2-CLF-S}
\end{align}

\begin{equation}
    \mathcal{S} = \{\xVec \in \mathbb{R}^n: V_{C}(\xVec) \geq tol \cap \dot{V}_{C}(\xVec, \uVec) \leq 0 \}
    \label{eq:New_2-CLF-S_actuationPhaseSet}
\end{equation}

Where \(\sigma(V_C(\xVec))\) is a sigmoide function responsible to the transition between the \glsxtrshort{CLF} and \(\zeta \in \mathbb{R}_{\geq 0}\) is its slope. \(\mathcal{S}\) refers to the just one time activation set of the respective \glsxtrshort{CLF} \(V_1\). 



The sets of possible control inputs, given the new \glsxtrshort{CLF} \(V_1\) and assuming a a high slope \(\zeta\) (\(\dot{\sigma}(\xVec, \uVec)\approx 0\)), is approximately:

\begin{align}
    K_{V_1}(\xVec) = \Bigl\{\uVec \in \mathbb{R}^m: \dot{V_1}(\xVec, \uVec) &= \dot{V}_{C}(\xVec, \uVec) \sigma(\xVec) + (1-\sigma(\xVec))\dot{V}_{N}(\xVec, \uVec) \notag \\
    &\leq -\gamma( \mu \sigma(\xVec) V_{C}(\xVec) + (1 - \sigma(\xVec))V_{N}(\xVec)  )\Bigr\}
    \label{eq:2-CLF-S_Constraint}
\end{align}

With \(\mu\) as a parameter with default value as \( \frac{V_{N}(\xVec_0)}{V_{C}(\xVec_0)} \), with \(\xVec_0\) equal to system state when is obtained the new equilibrium point \(\bar{\xVec_C}\). The set \(K_{V_2}(\xVec)\) is equivalent to \ref{eq:K-CLF}. \par

Finally, with \glsxtrshort{A-JO}~\ref{subsec:Just_Optimized_Algorithm} adapt the \glsxtrfull{CBF} slope \(\alpha\) to the new \glsxtrfull{CLF} \(V\) and improve possibly even more the state convergence to the final equilibrium point.\\





\subsubsection{\glsxtrfull{A-CLF-I}}
\label{subsubsec:CLFs_Independent_Algorithm}

From both equilibrium points is defined two \glsxtrshort{CLF}, \(V_C\) following the current equilibrium point \(\bar{\xVec_C}\) and \(V_N\) stabilizing in \(\bar{\xVec_N}\)If \(\zeta \to \infty\), according to equations \ref{eq:1stPhase_2-CLF-S} and \ref{eq:New_2-CLF-S}:


\begin{align}
    V(\xVec) = 
    \begin{cases}
        V_C(\xVec) \quad \xVec \in \mathcal{S}\\
        V_N(\xVec) = V_{N}(\xVec) \quad \xVec \in \bar{\mathcal{S}}
    \end{cases}
    \label{eq:New_2-CLF-I}
\end{align}


\begin{equation}
    \mathcal{S} = \{\xVec \in \mathbb{R}^n: V_{C}(\xVec) \geq tol \cap \dot{V}_{C}(\xVec, \uVec) \leq 0 \}
    \label{eq:New_2-CLF-I_actuationPhaseSet}
\end{equation}



And if \(\mu = \frac{V_{N}(\xVec)}{V_{C}(\xVec)}\) then:

\begin{align}
    K_{V_C}(\xVec) = \Bigl\{\uVec \in \mathbb{R}^m:  \dot{V}_{C}(\xVec, \uVec) \leq -\gamma( V_{N}(\xVec)  )\Bigr\}
    \label{eq:2-CLF-I_Constraint}
\end{align}

The set \(K_{V_N}(\xVec)\) is is defined by \ref{eq:K-CLF}. \par
Like in the previous technique~\ref{subsubsec:CLFs_Summed_Algorithm}, it is possible to apply \glsxtrshort{A-JO}~\ref{subsec:Just_Optimized_Algorithm} and improve the trajectory.\\


\subsubsection{\glsxtrfull{A-CLF-U}}
\label{subsubsec:CLFs_Unified_Algorithm}

Estou a pensar em deixar de parte para já, e como os resultados desta tecnica não são grande coisa acho que vou me focar no capitulo 3 para já.
































\endinput































% \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=My Heading]
% This is a \textbf{tcolorbox}.
% \tcblower
% Here, you see the lower part of the box.
% \end{tcolorbox}



% \begin{center}
%   \fbox{\LARGE
%     This manual is outdated and must be revised!}
% \end{center}



% \begin{flushleft}
% \hspace*{0.5cm}“\verb!n015002t.ttf!”, “\verb!n015003t.ttf!”, and “\verb!n015006t.ttf!”
% \end{flushleft}
   

% \ref{it:project_available} above in Section~\ref{sub:with_a_local_latex_installation} (\nameref{sub:with_a_local_latex_installation}).


% subsection with_a_remote_cloud_based_service (end)


% \newcommand{\accessAllowed}{\includegraphics[align=c,width=1.9em]{access_allowed}}
% \newcommand{\accessForbiden}{\includegraphics[align=c,width=1.9em]{dont_touch}}
% \newcommand{\File}{\includegraphics[align=c,width=1.9em]{file}}
% \newcommand{\Folder}{\includegraphics[align=c,width=1.9em]{folder}}


% \bgroup
%     \rowcolors{1}{}{GhostWhite}
%       \begin{xltabular}{\textwidth}{>{\ttfamily}l>{\itshape}lcX}
%         \caption{The folders and files (top level).}
%         \label{tab:folders_and_files}\\
%         \toprule
%         \rowcolor{Gainsboro}%
%         Name & Type & Access & Contents \\
%         \midrule
% template.tex      & \File    & \accessForbiden &
% The main template file. You need to \emph{compile} this file with one of \pdfLaTeX, \XeLaTeX, or \LuaLaTeX\ to obtain the PDF file (”\texttt{template.pdf}”).  I recommend the usage of the ”\texttt{latexmk}” command or, if you use a UN*X-like OS, you may use ”\texttt{make}” (and the ggiven ”\texttt{Makefile}”).
% \\
% Config          & \Folder  & \accessAllowed &
% Configuration files.  Please customize your template by changing the files in this folder!
% \\
%         \bottomrule
%         \end{xltabular}
%     % \end{longtblr}
% \egroup


% \newcommand{\classoption}[4]{\textbf{#1=OPT}\newline\emph{\small#2}&\textbf{#3}\newline{\small#4}\\}
% \newcommand{\defaultopt}[1]{\mbox{$\Rightarrow$~\emph{Default: \texttt{#1}}}\newline}
% \newcommand{\defaultit}[1][default]{($\Leftarrow$~\emph{#1})}


% \bgroup
% \begin{xltabular}{\linewidth}{>{\hsize=.4\hsize\raggedright\arraybackslash}X>{\hsize=.6\hsize}X}
%   \toprule
% %----------------------------------------------------------------------
%   \classoption{doctype}%
%     {phd, phdprop, phdplan, msc, mscplan, bsc, plain}%
%     {The type of the document.}%
% 	{%
%     \begin{tabular}{@{}r@{ $\rightarrow$ }l@{}}
%         phd & PhD thesis \defaultit.\\
%     phdprop & PhD thesis proposal (for FCT-NOVA).\\
%     phdplan & PhD thesis plan.\\
%         msc & MSc thesis.\\
%     mscplan & MSc thesis plan.\\
%         bsc & BSc report.\\
%       plain & Other report.\\
%     \end{tabular}
%     }
% %----------------------------------------------------------------------
%     \midrule
%   \classoption{school}%
%   	{nova/fct
% 	}%
%     {Selection of the university and of the school (and degree variant).}%
%     {\defaultopt{school=nova/fct} }
% %----------------------------------------------------------------------
%     \midrule
%   \classoption{docstatus}%
%     {draft, provisional, final}%
%     {The current status of the document.}%
% 	{}
% %----------------------------------------------------------------------
%     \bottomrule
% \end{xltabular}
% \egroup


% \printbibliography[heading=subbibliography, segment=\therefsegment, title={\bibname\ for chapter~\thechapter}]

% \section{\glsfmtshort{novathesisclass}\ Class Options}
% \label{sec:package_options}

 % \todo[inline]{A a note in a line by itself.}







