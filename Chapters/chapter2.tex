%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%


\chapter{Methodology}
\label{cha:methodology}

\glsresetall %Para em vez de mostrar o acrÃ³nimo voltar a mostrar o nome completo para relembrar




\section{\glsxtrshort{CLF-CBF}}
\label{sec:clf_cbf}  

\glsxtrshort{CLF-CBF} is a control technique that via \glsxtrshort{CLF} and \glsxtrshort{CBF} conditions impose stability and safety to a system respectively. This framework decision is generally obtained through a \glsxtrshort{QP}, whose objective is to minimize the control inputs. The optimization problem has a closed-form solution that allows faster computational speeds. The baseline \glsxtrshort{CLF} and \glsxtrshort{CBF} time derivatives don't contain the control inputs for high-order systems so its conditions aren't able to affect the system and therefore it needs the Backstepping formulation. It considers a solution if it was a first-order system and the conditions had access to the control inputs, that the new \glsxtrshort{CLF} and \glsxtrshort{CBF} and its conditions try to converge to and like the first-order \glsxtrshort{CLF-CBF} have a closed-form solution. Finally, it is encoded logical operations capable to merge the safe states sets (and unsafe sets, e.g. obstacles) defined by multiple \glsxtrshort{CBF} enabling to achieve more complex shapes and more accurate of the unsafe set.\\  

Considering a nonlinear control-affine system:

\begin{equation}
    \dot{\xVec} = f(\xVec) + g(\xVec)\uVec
    \label{eq:Nonlinear-First-Order-System} 
\end{equation}

with state \(\xVec \in \mathbb{R}^n\), control input \(\uVec \in \mathbb{R}^m\), and locally Lipschitz continuous\footnote{Given two metric spaces (\(X, d_X\)) and (\(Y, d_Y\)), where \(d_X\) and \(d_Y\)  represent the \(X\) and \(Y\) metrics respectively, a function \(f:X \to Y\) is Lipschitz continuous if there exist a constant \(L \geq 0\) such that \(\forall (x_1,x_2) \in X\), \hspace{0.4em} \(d_Y(f(x_1),f(x_2))\leq L \hspace{0.2em} d_X(x_1,x_2)\)} functions \(f: \mathbb{R}^n \to \mathbb{R}^n\) and \(g: \mathbb{R}^n \to \mathbb{R}^{n \times m}\). Given a locally Lipschitz continuous \(k: \mathbb{R}^n \to \mathbb{R}^m\), it results in the closed-loop system:

\begin{equation}
    \dot{\xVec} = f(\xVec) + g(\xVec)k(\xVec)
    \label{eq:Nonlinear-First-Order-Closed-Loop-System} 
\end{equation}

For any initial condition \(\xVec_0 \in \mathbb{R}^n\), there is a maximal time interval \(\mathcal{I}(\xVec_0) = [0; t_{max}(\xVec_0)[ \) and unique continuously differentiable solution \(\varphi: \mathcal{I}(\xVec_0) \to \mathbb{R}^n \) such that:

\begin{equation}
    \begin{array}{l}
        \dot{\varphi}(t) = f(\varphi(t))+ g(\varphi(t))k(\varphi(t)) \quad \forall t \in \mathcal{I}(\xVec_0)  \\
        \varphi(0) = \xVec_0
    \end{array}
 \label{eq:Nonlinear-First-Order-Closed-Loop-System-UniqueSol}
\end{equation}


\subsection{Relative Degree One Formulation}
\label{sub:formulation}

\subsubsection{\glsxtrfull{CLF}}
\label{subsub:control_lyapunov_function}

Having the objective of global asymptotically stability of the system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System}, is desired for the system converge to an equilibrium point \( \bar{\mathbf{x}} \). A solution comes by designing a positive definite function \( V: \mathbb{R}^n \rightarrow \mathbb{R}_{\geq 0} \) which converges to zero while the system converges to the respective equilibrium point, and for that, designing a control law that acts in order to achieve specifically the convergence of the function. \\

A Lyapunov function is a continuous scalar function \(V:\mathbb{D}^n \to \mathbb{R}\), designed to be positive definite, i.e, strictly positive with the exception of the equilibrium point \(\bar{\xVec}\) where it takes a null value.  

\begin{equation}
    \begin{array}{l}
    V(\mathbf{\xVec}) > 0, \quad \xVec \in \mathbb{D}^n\setminus \{\mathbf{0}\} \\
    V(\mathbf{\xVec}) = 0
    \end{array}
    \label{eq:LF_condition}
\end{equation}

And it is characterized by its time derivative beeing negative and and equal zero in \(\bar{\xVec}\).  

\begin{equation}
    \begin{array}{l}
        \dot{V}(\xVec) = \nabla V(\xVec)\dot{\xVec} < 0, \quad \xVec \in \mathbb{D}^n\setminus \{\mathbf{0}\} \\
        \dot{V}(\bar{\xVec}) = 0
    \end{array}
    \label{eq:LF_derivative_condition}
\end{equation}

%If the states of the system start relatively close to the origin and remains close forerever (\(V(\xVec) \leq \epsilon, \quad \epsilon \in \mathbb{R}_{\geq 0}\)) then it is Lyapunov stable and the origin is considered an equilibrium point. 
If those conditions, \ref{eq:LF_derivative_condition} are verified it means the Lyapunov function \(V\) converges to zero while the system converges to the equilibrium point, hence the system is asymptotically (lyapunov) stable, which is the notion of stability considered along this thesis.\par
If the Lyapunov function \(V\) domain \(\mathbb{D}^n\) is equivalent to the neighborhood of the equilibrium, where those conditions are just verified, then it is locally asymptotically stable while if it is equal to \(\mathbb{R}^n\) then \(V(\xVec)\) is radially unbounded, \(\lim_{||\mathbf{C}\hspace{0.1em}\xVec - \bar{\xVec}||\to \infty}V(\xVec) = \infty\) with \(\mathbf{C}\) as the observation matrix, and it is considered in the origin a globally asymptotically stable equilibrium. \\

In order to generalize to controllable dinamical systems, it is introduced \glsxtrshort{CLF}, modeled just like the typical the lyapunov functions~\ref{eq:LF_condition} but able through the use of the given control inputs affect its derivative in order to achieve the stability conditions~\ref{eq:LF_derivative_condition}. That beeing said, it is pretended to the \glsxtrshort{CLF} function \( V\mathbf(\mathbf{x}) \) to be continuous and differentiable:

\begin{equation}
\begin{array}{ll}
    \dot{V} (\mathbf{x}, \mathbf{u}) = \frac{\partial V}{\partial \xVec}\dot{\xVec} &= \frac{\partial V}{\partial \xVec}f(\xVec) + \frac{\partial V}{\partial \xVec}g(\xVec)\uVec \\
                                     &=  L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u}
\end{array}
\label{eq:CLF_derivative}
\end{equation}



where \(L_fV\) and \(L_GV\) are the Lie derivatives of \(V\) and \(\mathbf{u}\) the control input vector responsible for manipulating the function, more precisely, to affect its stabilizing convergence rate \(\dot{V}(\xVec, \uVec)\).\par

Therefore, a continuosly differentiable, positive definite function \( V: \mathbb{R}^n \rightarrow \mathbb{R}_{\geq 0}  \) wich \( \lim_{\mathbf{x} \to \bar{\mathbf{x}}}{V(\mathbf{x})} = 0 \) is a \glsxtrshort{CLF} if there exists a \( \gamma \in \mathcal{K}^e_{\infty}  \)\footnote{If a continuous functions \(\alpha:[-b,a] \to \mathbb{R}\) with \((a,b) > 0\), strictly increasing and \(\alpha(0) = 0\) then \(\alpha \in \mathcal{K}^e\) and if it adds \(\lim_{r \to \infty} \alpha(r) = \infty\) then \(\alpha \in \mathcal{K}^e_{\infty}\)}  such that for all \( \mathbf{x} \in \mathbb{R}^n \setminus \{\bar{\mathbf{x}}\} \):

\begin{equation}
 \inf_{\mathbf{u} \in \mathbb{R}^m} [L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u}] \leq -\gamma(V(\mathbf{x}))
 \label{eq:CLF}
\end{equation}\\

From \glsxtrshort{CLF} \(V\) and an adequate tuning of \( \gamma \) parameter results a control law followed by the controller \(K_{CLF}\):

\begin{equation}
 K_{CLF}(\mathbf{x}) = \{ \mathbf{u} \in \mathbb{R}^m: L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \leq -\gamma(V(\mathbf{x})) \}
 \label{eq:K-CLF}
\end{equation}

The stets generated by \(K_{CLF}(\mathbf{x}) \hspace{0.5em} \forall x \in \mathbb{R}^n \) refer to the possible control input values necessary to the \glsxtrshort{CLF} \(V(\mathbf{x})\) continuosly converge to zero while the closed-loop system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System} to the equilibrium point, but more slowly as is close to reach the objective and the \(\gamma(V(\xVec))\) function decreases. \\


\subsubsection{\glsxtrfull{CBF}}
\label{subsub:control_barrier_function}

While the \glsxtrshort{CLF} imposes an "acttracting" behaviour to the system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System}, the \glsxtrshort{CBF} a "repelling" one. Having the objective of guarantee safety to system, is aimed to keep the state inside a safe set \(\mathcal{C}\) (forward invariant\footnote{A set \(\mathcal{S}\) is said forward invariant if the initial system state \( x_0 \in \mathcal{S}\) and so \(x(t) \in \mathcal{S} \hspace{0.25em} \forall t \geq 0\) }) defined accordingly the problem needs (e.g. in the context autonomous vehicles, the outside space occupied by an obstacle).\par 
The notion of safety, i.e., the safe set \(\mathcal{C}\) is defined as the 0-superlevel\footnote{For function \(f: \mathbb{R}^n \to \mathbb{R}\), its superlevel set is given by \(\mathcal{S}(f) = \{(x_1,...,x_n): f(x_1,...,x_n) \geq c\}\), where having \(c = 0\) corresponds to the 0-superlevel set of \(f\)} set of a continuously differentiable function \( h: \mathbb{R}^n \rightarrow \mathbb{R}  \):

\begin{align}
        \mathcal{C} &= \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} )\geq 0\} \label{eq:safe-set}\\
        \partial\mathcal{C} &= \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} ) = 0\} \label{eq:boundary_safe_set}\\
        int(\mathcal{C}) &= \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} ) > 0\} \label{eq:interior_safe_set} \\
        \bar{\mathcal{C}} &= \{ \mathbf{x} \in \mathbb{R}^n : h( \mathbf{x} ) < 0\} \label{eq:unsafe_set}
\end{align}


In order to maintain  \(h( \mathbf{x} )\) positive, there is a need to have a control law but first to be able to interface with the function. Since \(h( \mathbf{x} )\) is differentiable:  

\[\dot{h}(\mathbf{x}, \mathbf{u}) = \frac{\partial h}{\partial x}\dot{x} = L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \]

where \(L_fh\) and \(L_Gh\) are the Lie derivatives of \(h\) and \(\mathbf{u}\) the control input vector responsible to manipulate the function, more precisely, its time derivative.\par

So, a differentiable function \( h: \mathbb{R}^n \rightarrow \mathbb{R}  \) is a \glsxtrshort{CBF} if there exists a \( \alpha \in \mathcal{K}^e_{\infty}  \) such that for all \( \mathbf{x} \in \mathbb{R}^n \setminus \{\bar{\mathbf{x}}\} \):

\begin{equation}
 \sup_{\mathbf{u} \in \mathbb{R}^m} [L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u}] \geq -\alpha(h( \mathbf{x} ))
 \label{eq:CBF}
\end{equation}

Similar to the \glsxtrshort{CLF}, from \glsxtrshort{CBF} \(h\) and an adequate tuning of \( \alpha \) parameter results a control law encapsulated by the controller \(K_{CBF}\):

\begin{equation}
 K_{CBF}(\mathbf{x}) = \{ \mathbf{u} \in \mathbb{R}^m: L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \geq -\alpha(h( \mathbf{x} )) \}
 \label{eq:K-CBF}
\end{equation}


The controller \(K_{CLF}(\mathbf{x}) \hspace{0.5em} \forall x \in \mathbb{R}^n \) renders control input sets that keep the closed-loop system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System} safe with respect to the set \(\mathcal{C}\). \par

By applying the control law to a system, the far from the boundary~\ref{eq:boundary_safe_set} it is (the bigger \(h(\xVec)\) and \(\alpha(h(\xVec))\)) or the bigger the value \(L_fh( \mathbf{x} )\) it takes, the greater the freedom to control the system or approximate to the unsafe set~\ref{eq:unsafe_set}, while the contrary requires a more caution reflecting in a smaller set \(K_{CBF}(\xVec)\). If \(h( \mathbf{x} ) < 0\) it means the system is in the unsafe region~\ref{eq:unsafe_set}, so \(\dot{h}( \mathbf{x}) > 0\), which implies control actions leading the closed-loop system continuously to the safe set \(\mathcal{C}\). Although, safety is a critical factor, having the system in the unsafe set don't necessarily mean collapse as the fit of the region (while defining the \glsxtrshort{CBF}) can be over conservative, already counting with unmodeled perturbations, a discrete behaviour (or integrative periods) as the \glsxtrshort{CBF} conditions just accounts with the given instant,... \\ 

% \newpage %SÃ³ para ajeitar




\subsubsection{Quadratic Program Formulation}
\label{subsub:quadratic_program_formulation}

Now having the objective of not only achieving  safety but also asymptotically  stability through the usage of \glsxtrshort{CLF} and \glsxtrshort{CBF} it renders a set of control inputs able to achieve the respective purposes. Using the conditions imposed by \glsxtrshort{CLF} and \glsxtrshort{CBF} as constraints, is presented an constrained optimization problem via \glsxtrfull{QP}~\footnote{\glsxtrlong{QP} consists of solving a mathematical optimization problem with a convex quadratic objective function and affine constraints}, relative faster than higher degree objective functions. This particular optimization problem aims minimal control actions and it is characterized for penalizing quadratically the control input deviations between each other. \par
Given a locally Lipschitz continuous controller \(k: \mathbb{R}^n \rightarrow \mathbb{R}^m \), which control law is described by the specified \glsxtrshort{QP}:

\begin{equation}
    \begin{array}{l}
        k(\xVec) = \underset{\uVec \in \mathbb{R}^m }{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec) \vspace{1em} \\  
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \leq -\gamma(V(\mathbf{x})) \vspace{0.25em} \\ 
        \quad \quad \hspace{1em} L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \geq -\alpha(h(\mathbf{x}))
    \end{array}
 \label{eq:CLF-CBF-QP}
\end{equation}

Where \(R \in \mathbb{R}^{m \times m}_{0}\) corresponds to the inputs weight matrix. \par

The sets of control inputs restricted to the optimization problem are given by \(K_{CLF-CBF}(\xVec) = K_{CLF}(\xVec) \cap  K_{CBF}(\xVec)\) hopefully non-empty which can happen if the conditions are in conflict.  \par

For a given instant if the minimal speed of convergence to the equilibrium point implies a too fast approximation speed towards the unsafe set then there is a conflict between both active constraints and it is prioritized system safety over stability. So, in order to the closed loop system~\ref{eq:Nonlinear-First-Order-Closed-Loop-System} controller have a solution from the \glsxtrshort{QP}, since safety is critical, using a slack variable \(\delta\)  allows to relax the stability condition imposed by the \glsxtrshort{CLF} and highly contribute to obtaining a solution while keeping system safe:

\begin{equation}
    \begin{array}{l}
        k(\xVec) = \underset{\uVec \in \mathbb{R}^m, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\mathbf{x}) + L_GV(\mathbf{x})\mathbf{u} \leq -\gamma(V(\mathbf{x})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em} L_fh(\mathbf{x}) + L_Gh(\mathbf{x})\mathbf{u} \geq -\alpha(h(\mathbf{x}))
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed}
\end{equation}

With \(p \in \mathbb{R}_{>0}\) as the slack variable weight. If \(p = 0\) then the \glsxtrshort{CLF} is disabled. \par

\newpage %so para ajeitar

Having \(\delta\) as an optimization variable not only allows to balance between the input values exceed usage and the level of relaxation but also not having a fixed \(\delta\) results for a not so fast convergence even when the \glsxtrshort{CBF} is not active as the \glsxtrshort{QP} aims minimal control actions, which is enhanced by the \glsxtrshort{CLF} condition softening, therefore is usually opted a higher \(p\) relative to the inputs weights. \\

%tikz daquele esquema
% \begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
% \begin{itemize}
%     \item If it's not pretended to enforce stability or safety, just remove from the optimization problem the respective constraint responsible to it
% \end{itemize}
% \end{tcolorbox}

%\newpage %SÃ³ para ajeitar


\subsection{Backstepping (Second-Order Formulation)}
\label{sub:backstepping}

\subsubsection{Second-Order Systems}
\label{subsub:higher_order_systems}

Usually control systems are governed by higher-order systems, such as some second-order cyber-physical systems controlling the position by applying some force to manipulate the speed that is felt in the position just instants after.  Higher order systems' output variation is felt by an multiple integrative effect, by an integration of each state affecting the next state dynamic until reaching the so wanted output. Considering a non-linear system in strict-feedback form:

\begin{subequations}
    
    \label{eq:HH-NL-System}
    \begin{align}
        \dot{\xVec} = f_0(\xVec) + g_0(\xVec) \mathbf{\xi} 
        \label{eq:HH-NL-System-1stOrder}\\
        \dot{ \mathbf{\xi} } =  f_1(\xVec, \mathbf{\xi}) + g_1(\xVec, \mathbf{\xi}) \uVec
        \label{eq:HH-NL-System-2ndOrder}
    \end{align}
\end{subequations}

with \(\mathbf{x} \in \mathbb{R}^n \), \(\mathbf{\xi} \in \mathbb{R}^p \), and \(\mathbf{u} \in \mathbb{R}^m \), and functions \(f_0: \mathbb{R}^n \to \mathbb{R}^n \), \(g_0: \mathbb{R}^n \to \mathbb{R}^{n \times p} \), \(f_1: \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^p \) and \(g_1: \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^{ p \times m} \) locally Lipschitz continuous on their respective domains. \par
Given a locally Lipschitz continuous feedback controller \(\mathbf{k}: \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^m  \) the closed-loop system is:

\begin{equation}
    \begin{array}{l}
        \dot{\xVec} = f_0(\xVec) + g_0(\xVec) \mathbf{\xi}\\ 
        \dot{ \mathbf{\xi} } =  f_1(\xVec, \mathbf{\xi}) + g_1(\xVec, \mathbf{\xi}) \mathbf{k}(\xVec, \mathbf{\xi})
    \end{array}
 \label{eq:HH-NL-CL-System}
\end{equation}

for any initial states \((\xVec_0, \mathbf{\xi}_0) \in \mathbb{R}^n \times \mathbb{R}^p\) exists a maximum time interval \(I(\xVec_0, \mathbf{\xi}_0) \subseteq \mathbb{R}_{\geq 0}  \) and a unique solution \( \varphi = (\varphi_{\mathbf{x}}, \varphi_{\mathbf{\xi}}) \) satisfying \ref{eq:Nonlinear-First-Order-Closed-Loop-System-UniqueSol} \(\forall t \in I(\xVec_0, \mathbf{\xi}_0) \). \\


\subsubsection{Purposal Solution Obtaining}
\label{subsub:quadratic_program_formulation}

Having a second order system~\ref{eq:HH-NL-System} the output dynamic don't have direct access to control \(\uVec\) or even is able to control \(\mathbf{\xi}\). In order to affect \(\mathbf{\xi}\) and consequentially control the wanted dynamic \(\dot{\xVec}\), the strategy passes by obtaining the solution if it was a first order system~\ref{eq:Nonlinear-First-Order-System} and if \(\mathbf{\xi}\) could be directly controlled (i.e. as part of the first order system control input). \par

Given a differentiable and locally Lipschitz continuous \(k_0(\xVec):\mathbb{R}^n \to \mathbb{R}^p\) (that encompasses the "controllable \(\mathbf{\xi}\)" ), the closed-loop system:

\begin{equation}
    \dot{\xVec} = f_0(\xVec) + g_0(\xVec) k_0(\xVec)\\ 
 \label{eq:NL-CL-System-0Backstep}
\end{equation}

That being said, the first order solution could be obtained via \glsxtrshort{QP}, so supposing a \glsxtrshort{CLF} \(V_0\),  \glsxtrshort{CBF} \(h_0\),  a differentiable and locally Lipschitz continuous described just now \(k_0(\xVec)\) and \((\gamma_0, \alpha_0) \in \mathcal{K}^e_\infty \times \mathcal{K}^e_\infty\):

\begin{equation}
    \begin{array}{l}
        k_0(\xVec) = \underset{\uVec \in \mathbb{R}^p, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R_0 \uVec + p_0\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV_0(\mathbf{x}) + L_GV_0(\mathbf{x})\mathbf{u} \leq -\gamma_0(V_0(\mathbf{x})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em} L_fh_0(\mathbf{x}) + L_Gh_0(\mathbf{x})\mathbf{u} \geq -\alpha_0(h_0(\mathbf{x}))
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed-0Backstep}
\end{equation}


The solution is equivalent to a value of \(\mathbf{\xi}\) closest to an optimal value taken by it that would impose safety and contribute to stability according to the \glsxtrshort{CBF} and \glsxtrshort{CLF} respectively, not only to the first-order system~\ref{eq:NL-CL-System-0Backstep} but also to the actual second-order system.\par
Although like it was said is not possible to have direct access to \(\mathbf{\xi}\), the backstepping technique is essentially the second-order system trying to converge to the first order solution, which already targets the desired equilibrium point while maintaining  safety. Now considering the second-order system \ref{eq:HH-NL-System}, supposing there is \( V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}): \mathbb{R}^n \times \mathbb{R}^p \rightarrow \mathbb{R}_{\geq 0} \), the new \glsxtrshort{CLF} \(V(\xVec, \mathbf{\xi})\) and \glsxtrshort{CBF} \(h(\xVec, \mathbf{\xi})\) are:

\begin{subequations}
    \label{eq:Backstepp_CF}
    \begin{align}
        V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) = \frac{1}{2}|| \mathbf{\xi} -  k_0(\xVec) ||^2 
        \label{eq:Backstepp_prejudice}\\
        V(\xVec, \mathbf{\xi}) = V_0(\xVec) + \eta V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) 
        \label{eq:Backstepp_V}\\
        h(\xVec, \mathbf{\xi}) = h_0(\xVec) - \eta V_{\mathbf{\xi}}(\xVec, \mathbf{\xi})
        \label{eq:Backstepp_h}
    \end{align}
\end{subequations}


With \(\eta \in \mathbb{R}_{\geq 0}\). By subtracting \(\eta V_{\mathbf{\xi}}(\xVec, \mathbf{\xi})\) to the \Glsxtrshort{CBF} \(h_0(\xVec)\) makes relatively \(h(\xVec, \xi)\) more conservative allowing, even though the delayed effect of the control inputs, to keep the system safe by targeting the obtained solution. Meanwhile, the \Glsxtrshort{CLF} \(V(\xVec, \xi)\) becomes bigger as the value taken by \(\gamma(V((\xVec, \xi)))\), rushing the convergence to \(\mathbf{k}_0(\xVec)\) followed by the desired equilibrium point. \par
Finally, via \Glsxtrshort{QP}, given a differentiable and locally Lipschitz continuous  \(k(\xVec, \mathbf{\xi}):\mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^m\):

\begin{equation}
    \begin{array}{l}
        k(\xVec, \mathbf{\xi}) = \underset{\uVec \in \mathbb{R}^m, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\xVec, \mathbf{\xi}) + L_GV(\xVec, \mathbf{\xi})\mathbf{u} \leq -\gamma(V_0(\mathbf{x})) -\gamma'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em}                 L_fh(\xVec, \mathbf{\xi}) + L_Gh(\xVec, \mathbf{\xi})\mathbf{u} \geq -\alpha(h_0(\mathbf{x})) + \alpha'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi}))
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed-1Backstep}
\end{equation}

It's expected that  \( \gamma' >> \gamma >> 0\) or specially to \(\eta >>0\) contributing to a faster convergence of \(\xi\) and more similar behaviour of the system~\ref{eq:HH-NL-CL-System} relative to the first-order system~\ref{eq:NL-CL-System-0Backstep} and mitigate the chances of the system~\ref{eq:HH-NL-CL-System} not be safe in relation to \(\mathcal{C}_0\). \\

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item If it's not pretended to enforce stability or safety, then it is removed from both optimization problem (\ref{eq:CLF-CBF-QP-relaxed-0Backstep} and \ref{eq:CLF-CBF-QP-relaxed-1Backstep}) the respective constraint responsible to it
    %\item If it's a higher-order (than second) system, then, assuming it's a third-order system, the solution obtained in \ref{eq:CLF-CBF-QP-relaxed-1Backstep} is the "new \(k_0\)" and the third-order solution tries to converge to the second-order one (while this one is converging to the first-order solution)
\end{itemize}
\end{tcolorbox} 



\subsection{Closed Form Solution}
\label{sub:closed_form}

The objective of this thesis is to purpose a set of low computational effort techniques suitable for spacecrafts. Aiming a faster computation of the optimization problems solutions, any of the previous \glsxtrshort{QP}s (\ref{eq:CLF-CBF-QP}, \ref{eq:CLF-CBF-QP-relaxed}, \ref{eq:CLF-CBF-QP-relaxed-0Backstep}, \ref{eq:CLF-CBF-QP-relaxed-1Backstep}) have a closed-form\footnote{An closed-form expression is expressed by a limit number and well defined operations (elementary functions)} solution allowing to calculate directly the respective optimal solution and faster than a numerical solver.\par
Given the \glsxtrshort{QP} optimization problem and a controller \(k(\xVec, \mathcolor{gray}{\mathbf{\xi}}): \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p} \to \mathbb{R}^m\) \textcolor{gray}{(if it is a second-order system)}:

\begin{equation}
    \begin{array}{l}
        k(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = \underset{\uVec \in \mathbb{R}^m, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} \underbrace{L_fV(\xVec \mathcolor{gray}{, \mathbf{\xi}}) + \gamma(\mathcolor{orange}{V_0(\mathbf{x})}) \mathcolor{gray}{+ \gamma'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi}))}}_{F_V(\mathbf{x} \mathcolor{gray}{, \mathbf{\xi}})} + L_GV(\xVec \mathcolor{gray}{, \mathbf{\xi}})\mathbf{u} - \delta \leq  0 \vspace{0.25em}\\
        \quad \quad \hspace{1em}                 \underbrace{L_fh(\xVec \mathcolor{gray}{, \mathbf{\xi}}) + \alpha(\mathcolor{orange}{h_0(\mathbf{x})}) \mathcolor{gray}{- \alpha'(V_{\mathbf{\xi}}(\mathbf{x}, \mathbf{\xi}))}}_{F_h(\mathbf{x} \mathcolor{gray}{, \mathbf{\xi}})} + L_Gh(\xVec \mathcolor{gray}{, \mathbf{\xi}})\mathbf{u} \geq 0 
    \end{array}
 \label{eq:CLF-CBF-QP-relaxed-generic}
\end{equation}

With a \glsxtrshort{CLF} \(V\), \glsxtrshort{CBF} \(h\), \(\mathbf{x}\in\mathbb{R}^n\), \textcolor{orange}{the first-order system \glsxtrshort{CLF}} \(\mathcolor{orange}{V_0} ( = V\) like in \ref{eq:CLF-CBF-QP}, \ref{eq:CLF-CBF-QP-relaxed} and \ref{eq:CLF-CBF-QP-relaxed-0Backstep}) \textcolor{gray}{, and if it is a second-order system with} \(\mathcolor{gray}{\mathbf{\xi}\in\mathbb{R}^p}\) \textcolor{gray}{(equivalent to the \glsxtrshort{QP} seen while doing Backstepping~\ref{eq:CLF-CBF-QP-relaxed-1Backstep})}. \\


Assuming equal control input weights relative to each other (\(R = \mathbf{I}_{m \times m}\)) and just one \glsxtrshort{CLF} and \glsxtrshort{CBF} each, according to the \glsxtrfull{KKT} conditions\footnote{\href{https://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions}{KKT Conditions Wikipedia (Click here to be Redirected)} \label{foot: KKT_Conditions}} (to see the full deduction \ref{app:CL_QP_CLF-CBF}):

\begin{equation}
    \mathbf{k}(\xVec \mathcolor{gray}{, \mathbf{\xi}}) =
    \begin{cases}
        \mathbf{k}_1(\xVec \mathcolor{gray}{, \mathbf{\xi}}), \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_1 \\
        \mathbf{k}_2(\xVec \mathcolor{gray}{, \mathbf{\xi}}), \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_2 \\
        \mathbf{k}_3(\xVec \mathcolor{gray}{, \mathbf{\xi}}), \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_3 \\
        \mathbf{0}_{m \times 1 } \quad  \hspace{0.25em}, \quad (\xVec \mathcolor{gray}{, \mathbf{\xi}}) \in \mathcal{S}_4
    \end{cases}
    \label{eq:closed-form_controller}
\end{equation}


Where each controller \(\mathbf{k}_i\), for \( i = 1, ..., 4 \), corresponds to the closed-form equation depending which constraints (\glsxtrshort{CLF} or \glsxtrshort{CBF}) are active what can be inferred by \(\mathcal{S}_i\), for \( i = 1, ..., 4 \). Via \glsxtrshort{KKT} conditions the respective  controllers and domains are obtained with the help of \glsxtrshort{KKT} multipliers.\\

Auxiliary Variables:

\begin{align*}
    & L \sOrderArgB = L_GV \sOrderArgB \hspace{0.2em} L_Gh\sOrderArgB^{\top} \\
    & \Delta = L \sOrderArgB^2 (p^{-1} + ||L_Gh\sOrderArgB||^2)  \hspace{0.2em} || L_Gh\sOrderArgB||^2
\end{align*}


The constraint activation subdomains \(\mathcal{S}_i\) are respective  based on the dual feasibilities  condition~\footref{foot: KKT_Conditions}, which states that an constraint is active if its respective optimal \glsxtrshort{KKT} multiplier is positive \(\lambda_i^* > 0\), while the inactivation of those constraints are stated by the null value taken by the optimal \(\lambda_i^*\)  (\(\lambda_i^* = 0\)). However, since it is not known which constraints are active, it isn't possible to have \(\lambda_i^*\) and vice versa. The solution comes from the dual problem solution considering the \glsxtrshort{CLF} and \glsxtrshort{CBF} conditions active, such that if \(\lambda_i <0\) the respective constraint is inactive. 


\glsxtrshort{KKT} multipliers if \glsxtrshort{CLF} and \glsxtrshort{CBF} are active (dual problem solution):

\begin{align}
    & \lambda_1(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = \Delta^{-1} (L \sOrderArgB \hspace{0.2em} F_h \sOrderArgB \hspace{0.2em} - ||L_Gh\sOrderArgB||^2  \hspace{0.2em}F_V \sOrderArgB)
    \notag\\
    & \lambda_2(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = \Delta^{-1} (L \sOrderArgB F_V \sOrderArgB \hspace{0.2em} - (p^{-1} + ||L_Gh\sOrderArgB||^2)  \hspace{0.2em}F_h \sOrderArgB) 
    \label{eq:KKT-Multipliers_formulas}
\end{align}


\begin{align}
    \intertext{\hspace{4em} If CLF is active: }
    \mathcal{S}_1 \sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 \geq 0 \cap \lambda_2 < 0 \}
    \label{eq:Constraints_Activation_Subdomain_1} \\
    \intertext{\hspace{4em} If CBF is active:}
    \mathcal{S}_2 \sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 < 0 \cap \lambda_2 \geq 0 \}
    \label{eq:Constraints_Activation_Subdomain_2} \\
    \intertext{\hspace{4em} If CLF and CBF is active:}
    \mathcal{S}_3 \sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 \geq 0 \cap \lambda_2 \geq 0 \}
    \label{eq:Constraints_Activation_Subdomain_3} \\
    \intertext{\hspace{4em} If Any is active:}
    \mathcal{S}_4\sOrderArgB = \{ \sOrderArgB \in \mathbb{R}^n \mathcolor{gray}{\times \mathbb{R}^p}: \lambda_1 < 0 \cap \lambda_2 < 0 \}
    \label{eq:Constraints_Activation_Subdomain_4} 
\end{align}


For each activation subdomain, corresponds a closed-form control law \(\mathbf{k}_i\):

\begin{align}
    &\mathbf{k}_1(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = -(p^{-1} + ||L_GV \sOrderArgB||^2)^{-1} \hspace{0.2em} F_V\sOrderArgB \hspace{0.2em} L_GV\sOrderArgB^{\top} \label{eq:closed-form_controller_k1_formula} \\ 
    &\mathbf{k}_2(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = -||L_Gh \sOrderArgB||^{-2} \hspace{0.2em} F_h \sOrderArgB \hspace{0.2em} L_Gh \sOrderArgB^{\top}  \label{eq:closed-form_controller_k2_formula}\\ 
    &\mathbf{k}_3(\xVec \mathcolor{gray}{, \mathbf{\xi}}) = -\lambda_1\sOrderArgB L_GV \sOrderArgB^{\top} + \lambda_2\sOrderArgB L_Gh \sOrderArgB^{\top} \label{eq:closed-form_controller_k3_formula}
\end{align}

In the \(\mathbf{k}_3(\xVec \mathcolor{gray}{, \mathbf{\xi}})\) control law~\ref{eq:closed-form_controller_k3_formula} \(\lambda_1, \lambda_2 \in \mathbb{R}_{\geq 0}\) are optimal dual solutions and verified strong duality. 

\subsection{Unifying \glsxtrshort{CBF}}
\label{sub:unifying_CBF}

The previous controllers synthesized control inputs based on just one \glsxtrshort{CBF}, i.e., those controllers guarantee safety w.r.t. a single safe set \(\mathcal{C}\), but given different scenarios, it could exist more complex safety specifications and the need to impose more than one safety contraints simultaneously such as \glsxtrshort{CBF} \(h_i(\xVec)\), for \(i \in  \mathcal{I}=\{1,...N\}\):

\begin{align}
    & \mathcal{C}_i = \{ \xVec \in \mathbb{R}_n: h_i(\xVec) \geq 0\}
    \label{eq:Multiple_Safe_Sets} \\ 
    \notag\\
    & \mathcal{C} = \bigcap_{i \in \mathcal{I}} \mathcal{C}_i 
    \label{eq:Safe_Set_of_multiple}
\end{align}

Having more than one constraint could leave to conflict problems, an inexistent set of acceptable control inputs (\(\mathbf{k}\sOrderArgB = \emptyset\)) when the system is in the resultant unsafe set, incapable to converge to the resultant safe set \(\mathcal{C} \neq \emptyset\). In order to avoid conflicts, \cite{molnar2023composing} showed a way to merge all \glsxtrshort{CBF} into a single one that is able to capture all those safety notions and allow for even more complex safety specifications. \\


\subsubsection{First-Order System}
\label{subsub:uniCBF_firstOrder_system}

The more complex safety specifications are defined via boolean logical operations between 0-superlevel sets of the diffrent \glsxtrshort{CBF}. Those operations are tipically seen in constrained optimization problems, like in \glsxtrshort{QP} of the form:


\begin{equation}
    \begin{array}{l}
        k(\xVec) = \underset{\uVec \in \mathbb{R}^p, \hspace{0.2em} \delta \in \mathbb{R}}{\mathrm{argmin}} \quad \hspace{-0.5em} \frac{1}{2} (\uVec^\top R \uVec + p\delta^2 ) \vspace{1em} \\ 
        \quad \hspace{0.5em}  st. \hspace{0.5em} L_fV(\xVec) + L_GV(\xVec)\mathbf{u} \leq -\gamma(V(\mathbf{x})) + \delta \vspace{0.25em}\\
        \quad \quad \hspace{1em}                 L_fh_i(\xVec) + L_Gh_i(\xVec)\mathbf{u} \geq -\alpha_i(h_i(\mathbf{x}))  \quad \forall i \in \mathcal{I}    
    \end{array}
 \label{eq:CLF-MultipleCBF-QP-relaxed-generic}
\end{equation}

Where the resultant safe set~\ref{eq:Safe_Set_of_multiple} of the system refers to the intersection of all safe sets~\ref{eq:Multiple_Safe_Sets}. More than be propitious to be not solvable, the formulation itself of the optimization problem~\ref{eq:CLF-MultipleCBF-QP-relaxed-generic} is limitated by no other logical operation than AND. \\

Among the the logical operations that could be applied to \glsxtrshort{CBF}s, there are:

\begin{description}

    \item[Identity ( Class-\(\mathcal{K}^e\)):] 
    \begin{subequations}
        \begin{align}
            \mathcal{C}_i &= \{(\xVec, \alpha_i) \in \mathbb{R}^n \times \mathcal{K}^e: \alpha_i(h_i(\xVec) \geq 0)\} = 
            \label{eq:Indentity_set} \\
                          &= \{\xVec \in \mathbb{R}^n: h_i(\xVec) \geq 0\}
            \label{eq:Gen_Safe_set} 
        \end{align}
    \end{subequations}

    \item[Complement Set (Negation):] 
    
    \begin{equation}
        \bar{\mathcal{C}_i} = \{\xVec \in \mathbb{R}^n: -h_i(\xVec) \geq 0\} 
        \label{eq:Negation_set} 
    \end{equation}

    \item[Intersection of Sets (AND):] 

        \begin{equation}
            \begin{array}{rl}      
            \underset{i \in \mathcal{I}}{\bigcap} \mathcal{C}_i &= \{\xVec \in \mathbb{R}^n: h_i(\xVec) \geq 0 \hspace{0.5em} \forall i \in \mathcal{I}\}  \\
                                                      &= \{\xVec \in \mathbb{R}^n: \underset{i \in \mathcal{I}}{\min}(h_i(\xVec)) \geq 0 \} 
            \end{array}
            \label{eq:Intersection_set}
        \end{equation}

    \item[Union of Sets (OR):] 
    
        \begin{equation}
            \begin{array}{rl}
                \underset{i \in \mathcal{I}}{\bigcup} \mathcal{C}_i &= \{\xVec \in \mathbb{R}^n: \exists i \in \mathcal{I} \hspace{0.5em} st. \hspace{0.5em} h_i(\xVec) \geq 0 \}  \\
                                                          &= \{\xVec \in \mathbb{R}^n: \underset{i \in \mathcal{I}}{\max}(h_i(\xVec)) \geq 0 \} 
            \end{array}
            \label{eq:Union_set}
        \end{equation}
\end{description}

The logical operations identity, complement, intersection and union are represented by respectively, class-\(\mathcal{K}^e\) functions, negation, min and max operations. The class-\(\mathcal{K}^e\) (\(\alpha_i(h_i(\xVec))\)) and negation (\(-h_i(\xVec)\)) functions are continuosly differentiable and valid as \glsxtrshort{CBF}, contrary to the AND and OR functions, \(\min_{ i \in \mathcal{I}} (h_i(\xVec))\) and \(\max_{ i \in \mathcal{I}} (h_i(\xVec))\) that aren't continuosly differentiable and therefore not \glsxtrshort{CBF} candidates.  The \(\max\) and \(\min\) functions capacitate combining safe sets defined by multiple \glsxtrshort{CBF} (accordingly to the operation) and to obtain a new safety specification as is expected to see while merging multiple \glsxtrshort{CBF} into a single one. So, in order to viablize the AND and OR operation and achieve the merge of \glsxtrshort{CBF}s, the \(min\) and \(max\) functions can be replaced via smooth-aproximation, keeping their properties with just a relative small degradation of accuracy, by continuosly differentiable \underline{log-sum-exp functions}:


\begin{description}

    \item[Intersection of Sets \(\setminus\)AND \(\setminus\) Minimum]

    \begin{align}
        h(\xVec) &= \frac{-1}{k}ln(\sum_{i \in \mathcal{I}} e^{-k \hspace{0.2em} h_i(\xVec)}) 
        \label{eq:Min_smoothFunc} \\
                 &\approx \underset{i \in \mathcal{I}}{\min} (h_i(\xVec))
        \notag  
    \end{align}

    Being  \(k \in \mathbb{R}_{>0}\) a smoothing parameter. The safe set fit isn't perfect unless \(k \to \infty\) ( which could lead computationally to an error) as can be seen more clearly in \cite{molnar2023composing} figure 1. If \(k<0\) then is equivalent to the smooth-aproximation of \(\max_{i \in \mathcal{I}}(h_i(\xVec))\). 

    \begin{subequations}
        \begin{align}
            &\dot{h}(\xVec , \uVec) = \underbrace{\sum_{i \in \mathcal{I}} [\lambda_i \hspace{0.2em} L_fh_i(\xVec)]}_{Lfh(\xVec)} + (\underbrace{\sum_{i \in \mathcal{I}} [\lambda_i \hspace{0.2em} L_Gh_i(\xVec)]}_{LGh(\xVec)})\uVec 
            \label{eq:Min_smooth_derivative} \\ \notag \\
            &\lambda_i(\xVec) = e^{-k(h_i(\xVec)-h(\xVec))}
            \label{eq:Min_lambda_smoothFunc}
        \end{align}
    \end{subequations}

    With \(\sum_{i \in \mathcal{I}} \lambda_i(\xVec) = 1\) \par
    Encountering an under-aproximation of \(h\) relative to the \(\min\) function, given the safe set \(\mathcal{C}\) defined based on \ref{eq:safe-set} it verifies:

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\min} (h_i(\xVec)) \geq h(\xVec) \quad \forall \xVec \in \mathbb{R}^n 
            \label{eq:Intersection_CBF_Comparation} \\
            &\mathcal{C} \subseteq \bigcap_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Intersection_SafeSet_Comparation}
        \end{align}
    \end{subequations}

    But, \(\hspace{0.4em} \lim_{k \to \infty} h(\xVec) = {\min}_{i \in \mathcal{I}} (h_i(\xVec)) \hspace{0.4em}\) and \( \hspace{0.4em} \lim_{k \to \infty} \mathcal{C} = \bigcap_{i \in \mathcal{I}} \mathcal{C}_i\) \par

    On par with that, the new intersected \glsxtrshort{CBF} \(h(\xVec)\) \ref{eq:Min_smoothFunc} could restrict too much the closed-loop system in comparison to theoretical \glsxtrshort{CBF} \( \lim_{k \to \infty} h(\xVec)\). In order to tight the unsafe set \(\bar{\mathcal{C}}\) and potentially the safe set contain \( \bigcap_{i \in \mathcal{I}} \mathcal{C}_i\): 
    
    \begin{equation}
        h(\xVec) = \frac{-1}{k}ln(\sum_{i \in \mathcal{I}} e^{-k \hspace{0.2em} h_i(\xVec)}) + \frac{b}{k}
        \label{eq:Min_smoothFunc_tighted} \\  
    \end{equation}



    Being \(b\) a parameter tuned according with the problem needs, knowing that if \(b = ln (N)\):

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\min} (h_i(\xVec)) \leq h(\xVec) % \leq \underset{i \in\mathcal{I}}{\max} + \frac{b}{k} \quad \forall \xVec \in \mathbb{R}^n
            \label{eq:Intersection_CBF_Comparation_tighted} \\
            &\mathcal{C} \supseteq \bigcap_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Intersection_SafeSet_Comparation_tighted}
        \end{align}
    \end{subequations}

    Making the closed-loop system less constrained but not as safe, which can be seen more clearly in \cite{molnar2023composing} figure 1.

    \item[Union of Sets \(\setminus\) OR \(\setminus\) Maximum]

    \begin{align}
        h(\xVec) &= \frac{1}{k}ln(\sum_{i \in \mathcal{I}} e^{k \hspace{0.2em} h_i(\xVec)}) 
        \label{eq:Max_smoothFunc} \\
                 &\approx \underset{i \in \mathcal{I}}{\max} (h_i(\xVec))
        \notag  
    \end{align}

    Being \(k \in \mathbb{R}_{>0}\) a smoothing parameter and serving the same purposes as \ref{eq:Min_smoothFunc}. The derivative of \(h(\xVec)\) is equal to \ref{eq:Min_smooth_derivative} but:

    \begin{equation}
        \lambda_i(\xVec) = e^{k(h_i(\xVec)-h(\xVec))}
        \label{eq:Max_lambda_smoothFunc}
    \end{equation}

    With also \(\sum_{i \in \mathcal{I}} \lambda_i(\xVec) = 1\). \par
    Due to an over-approximation of \(h\) relative to the \(\max\) function, given the safe set \(\mathcal{C}\) defined based on \ref{eq:safe-set} it verifies:

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\max} (h_i(\xVec)) \leq h(\xVec) \quad \forall \xVec \in \mathbb{R}^n 
            \label{eq:Union_CBF_Comparation} \\
            &\mathcal{C} \supseteq \bigcup_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Union_SafeSet_Comparation}
        \end{align}
    \end{subequations}

    But, \(\hspace{0.4em} \lim_{k \to \infty} h(\xVec) = {\max}_{i \in \mathcal{I}} (h_i(\xVec)) \hspace{0.4em}\) and \( \hspace{0.4em} \lim_{k \to \infty} \mathcal{C} = \bigcup_{i \in \mathcal{I}} \mathcal{C}_i\) \par

    Now showing a different point of view, safety is a critical factor, so in order to make the safe set \(\mathcal{C}\) more conservative, potentially be contained within \( \bigcup_{i \in \mathcal{I}} \mathcal{C}_i\): 
    
    \begin{equation}
        h(\xVec) = \frac{1}{k}ln(\sum_{i \in \mathcal{I}} e^{k \hspace{0.2em} h_i(\xVec)}) - \frac{b}{k}
        \label{eq:Max_smoothFunc_tighted} \\  
    \end{equation}

    Being  \(b\) a tunable parameter. Taking \(b = ln (N)\):

    \begin{subequations}
        \begin{align}
            &\underset{i \in \mathcal{I}}{\max} (h_i(\xVec)) \geq h(\xVec) % \leq \underset{i \in\mathcal{I}}{\max} + \frac{b}{k} \quad \forall \xVec \in \mathbb{R}^n
            \label{eq:Union_CBF_Comparation_tighted} \\
            &\mathcal{C} \subseteq \bigcup_{i \in \mathcal{I}} \mathcal{C}_i
            \label{eq:Union_SafeSet_Comparation_tighted}
        \end{align}
    \end{subequations}

    Therefore the higher \(b\), the safer the system~\ref{eq:NL-CL-System-0Backstep} but possibly too constrained by the new \glsxtrshort{CBF} \(h(\xVec)\) \ref{eq:Max_smoothFunc_tighted} (figure 1 from \cite{molnar2023composing}). 
\end{description}



\subsubsection{Second-Order System}
\label{subsub:uniCBF_secondOrder_system}

Following the explanations given in the previous subsubsection~\ref{subsub:uniCBF_firstOrder_system}, now applying for a second-order system~\ref{eq:HH-NL-System}, more precisely, for a Backstepping~\ref{sub:backstepping} situation. \\

Given a \glsxtrshort{CLF} \(V_0(\xVec): \) and multiple \glsxtrshort{CBF} \(h0_i(\xVec) \quad \forall i \in \mathcal{I}\) combined into a \glsxtrshort{CBF} \(h_0(\xVec)\) via \ref{eq:Min_smoothFunc_tighted} (intersection) or \ref{eq:Max_smoothFunc_tighted} (union) (they can be combined using both operations), through the \glsxtrshort{QP}~\ref{eq:CLF-CBF-QP-relaxed-0Backstep} (or its closed-form solution~\ref{eq:closed-form_controller}) is obtained a differentiable and locally Lipschitz continuous theoretical controller \(k_0(\xVec): \mathbb{R}^n \to \mathbb{R}\). \\

In order to the second-order system~\ref{eq:HH-NL-CL-System} dynamic converge to \(k_0(\xVec)\) imposed dynamic, given multiple \glsxtrshort{CBF} \(h_i(\xVec) \quad \forall i \in \mathcal{I} \):

\begin{subequations}
    \label{eq:Backstepp_CF_multipleCBF}
    \begin{align}
        &V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) = \frac{1}{2}|| \mathbf{\xi} -  k_0(\xVec) ||^2 
        \label{eq:Backstepp_prejudice_multipleCBF}\\
        &V(\xVec, \mathbf{\xi}) = V_0(\xVec) + V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) 
        \label{eq:Backstepp_V_multipleCBF}\\
        &h_i(\xVec, \mathbf{\xi}) = h0_i(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) \quad \forall i \in \mathcal{I}
        \label{eq:Backstepp_h_multipleCBF}
    \end{align}
\end{subequations}

The resultant \glsxtrshort{CBF} \(h_i(\xVec, \mathbf{\xi}) \quad \forall i \in \mathcal{I}\), one more time can be merged via \ref{eq:Min_smoothFunc_tighted} or \ref{eq:Max_smoothFunc_tighted} (according it is an AND or an OR operation, or even both) into a single \glsxtrshort{CBF} \(h(\xVec, \mathbf{\xi})\). However there is another and more direct way in this case to have the new single \glsxtrshort{CBF} \(h(\xVec, \mathbf{\xi})\), so, considering the second-order system~\ref{eq:HH-NL-System} and \(lo \in \{\max, \min\}\):

\begin{align}
        \underset{ i \in \mathcal{I}}{lo}(h0_i(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi})) &= \underset{ i \in \mathcal{I}}{lo}(h0_i(\xVec)) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) \approx 
                                                                                                \notag\\
                                                                                               &\approx h_0(\xVec) - V_{\mathbf{\xi}}(\xVec, \mathbf{\xi}) = 
                                                                                               \notag\\
                                                                                               &=h(\xVec, \mathbf{\xi})
                                                                                               \label{eq:Backstepp_h_multipleCBF2}
\end{align}

\begin{equation}
        \dot{h}(\xVec, \mathbf{\xi}) = \underbrace{L_fh_0(\xVec) + L_Gh_0(\xVec)\mathbf{\xi} - L_fV_{\mathbf{\xi}}(\xVec, \mathbf{\xi})}_{L_fh(\xVec, \mathbf{\xi})} + (\underbrace{- L_GV_{\mathbf{\xi}}(\xVec, \mathbf{\xi})}_{L_Gh(\xVec, \mathbf{\xi})}) \uVec
        \label{eq:Backstepp_h_multipleCBF_derivatives2}
\end{equation}


Having \glsxtrshort{CBF} \(h\), the second-order solution, a differentiable and locally Lipschitz continuous controller \(k(\xVec, \mathbf{\xi}): \mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}\) is obtained using the \glsxtrshort{QP}~\ref{eq:CLF-CBF-QP-relaxed-1Backstep} (or its closed-form solution~\ref{eq:closed-form_controller}). \\

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item There are other possible variations of a second-order system that invalidates \ref{eq:Backstepp_h_multipleCBF_derivatives2} like the unicycle model that will be shown next chapter.
\end{itemize}
\end{tcolorbox} 



\newpage %SÃ³ para ajeitar




\section{Single Parameter Optmization}
\label{sec:Single_Parameter_Optmization}


%The optimization problems such as \ref{eq:CLF-CBF-QP}, \ref{eq:CLF-CBF-QP-relaxed}, \ref{eq:CLF-CBF-QP-relaxed-0Backstep}, \ref{eq:CLF-CBF-QP-relaxed-1Backstep}, can be processed by making use of solvers able to optimize multiple variables using diffrent methods like the ones presented here, or if it is feasible using the closed-form solution of the optimization problem like \ref{sub:closed_form}. However, the methods shown will just be compreended to one optimization variable, owing to the specifications is pretended to achieve with the  proposed decision algorithms~\ref{sec:Proposed_Propagation_Algorithms}. \par 
The optimization of the respective parameter is based on minimizing a cost function \(g:\mathbb{R} \to \mathbb{R}\), which in the context of this work expected to be quasi-convex~\footnote{ A quasi-convex function \(f:\mathcal{S} \to \mathbb{R}\), beeing \(\mathcal{S}\) a convex set, verifies \(\forall x,y \in \mathcal{S}\) and \(\lambda \in [0;1]\), \(f(\lambda x + (1-\lambda)y) \leq max\{f(x), f(y)\}\)   } along the optimization variable \(x \in \mathbb{R}\), and to do that there are algorithms such as the highly used gradient descent~\ref{subsec:Gradient_Descent}  and binary search~\ref{subsec:Binary_Search} method.



\subsection{Gradient of the Cost Function - Discretization}
\label{subsec:Gradient_Cost_Function_Discretization}

In both algorithms is made use of \(g(x)\) gradient in order to essentially indicate the direction the optimization variable \(x\) must take in order to minimize the cost. Beyond that, if \(g(x)\) is a convex function, to quantify the proximity to a optimal point and possibly vary the variable proportionally to its value taken in that point, as \(\nabla g(x)\) converges to zero while it gets close to a minimum. \par
If the \(\nabla g(x)\) exact equation is not accessible to obtain then:

\begin{equation}
    \nabla g(x) = \lim_{dev \to 0} \frac{g(x + dev) - g(x)}{dev}
    \label{eq:Gradient_formula}
\end{equation}

Is approximated by:

\begin{equation}
    \nabla g(x, dev) = \frac{g(x + dev) - g(x)}{dev}
    \label{eq:Gradient_Discretization}
\end{equation}

With \(dev \in \mathbb{R}\) sufficiently close to \(0\). \par

If \(g(x)\) has relevant noise could compromise the value obtained of \(\nabla g(x)\), it could be needed increase the number of samples (\(nS \in \mathbb{R}\)) in the neighborhood of \(x\) separated by \(devN\) between them, and do a weighted mean, so digitally:
\SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$nS$, $devN$, $dev$, $g$, $x$}
    \KwResult{$mean(grad_i)$}
    $i\gets 0$\;
    \While{$i \leq nS -1$}{
        $devN_i \gets  \frac{1}{2} \times (i + (i \hspace{0.2em} mod \hspace{0.2em} 2) ) \times devN $ \;
        $grad_i \gets \nabla g(x + (-1)^{i}  \times devN_i, (-1)^i \times dev)$ \Comment*[r]{Equation \ref{eq:Gradient_Discretization}}
        $i\gets i + 1$\;    
        }
    \caption{Discrete Gradient Function (DGF)} \label{alg:Discrete_Gradient_Function}
  \end{algorithm}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item It is possible to digitally obtain \(\nabla g : \mathbb{R}^n \to \mathbb{R}^n\) by otaining each row \(r\) separately with \(dev_r \in \{ dev_1, ..., dev_n \} \). Being \(dev_r \) a column vector, just equal to the divisor \(dev\) in the respective row element (\(dev = ||dev_r||\)).
\end{itemize}
\end{tcolorbox} 


%\newpage %SÃ³ para ajeitar

\subsection{Gradient Descent}
\label{subsec:Gradient_Descent} 

This method is characterized by an iterative update of the optimization variable \(x\), based on the current variable value and function \(g(x)\) gradient responsible to indicate the evolution direction of \(x\) in order to minimize the cost and adjust proportionally the step intensity:

\begin{equation}
    x_{k+1} = x_k + \underbrace{(- a \nabla g(x_k))}_{step_{k}}
    \label{eq:Gradient_Descent_Normal}
\end{equation}

where \(a\) is an arbitrary constant for the step intensity adjustment . Assuming \(g(x)\) is a quasi-convex function, it is expected the closure to local minimum the smaller the gradient (or step) module. \par

However, this method has another variations including adding "\underline{momentum}" which has in account the previous iteration step applied:

\begin{equation}
    x_{k+1} = x_k + b \underbrace{(x_k - x_{k-1})}_{step_{k-1}} - a \nabla g(x_k)
    \label{eq:Gradient_Descent_Momentum}
\end{equation}

where \(b\) is an argument of this algorithm like \(a\) responsible to tune the previous step intensity. \par
According to \cite{hao2025analysis} there are also another two methods. Referring  to one of them, this expands on the idea of momentum called \underline{Nesterov's Accelerated Gradient}, using a step based on the previous one prediction: 

\begin{equation}
    \begin{array}{rl}
      y_k &=  x_k + b (x_k - x_{k-1})  \\
      x_{k+1} &= y_k + (\underbrace{- a \nabla g(y_k)}_{predictive  \hspace{0.2em}step})
    \end{array}
    \label{eq:Gradient_Descent_Nesterov}
\end{equation}

where \(a\) now tunes the predictive step. \par

Finally the \underline{Triple-Momentum} method which presents an output equation and the parameters applied on the previous step are distinct in all usages of it:

\begin{equation}
    \begin{array}{rl}
      z_k &= x_k + d (x_k - x_{k-1}) \\
      y_k &=  x_k + c (x_k - x_{k-1})  \\
      x_{k+1} &= x_k + b(x_k - x_{k-1}) - a \nabla g(y_k)
    \end{array}
    \label{eq:Gradient_Triple_Momentum}
\end{equation}

where \(a, b, c, d\) are the respective steps intensity tuning parameter. \\




\begin{tcolorbox}[colback=blue!5!white,colframe=blue!35!white,title=Notes:]
\begin{itemize}
    \item The method supports multiple optimization variables.
\end{itemize}
\end{tcolorbox} 







\newpage %SÃ³ para ajeitar


\subsection{Binary Search}
\label{subsec:Binary_Search}

This method is composed by two phases, being  the first one responsible to obtain two extreme points \((xI,xS) \in \mathbb{R} \times \mathbb{R}\) such that the Bolzano theorem~\footnote{Bolzano's theorem says that given the continuous function \(f:[a;b] \subset \mathbb{R} \to \mathbb{R}\) and \(f(a)f(b) < 0\), then \(\exists x \in  ]a; b[\) such that \(f(x) = 0\)} is verified for the continuous gradient function (\(  x^*= \{ x \in ]xI; xS[ \hspace{0.1em}  : \hspace{0.25em} \exists x \hspace{0.25em}\nabla g(x) = 0 \} \)). In order to achieve that, is used a variation of the gradient descent method~\ref{eq:Gradient_Descent_Normal}:

\begin{equation}
    x_{k+1} = x_k + \underbrace{(- a \nabla g(x_{k}))}_{step}
    \label{eq:Binary_Search_Phase1_gradDesc}
\end{equation}

That being  said, the phase 1 algorithm: 

\SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$xI$, $a$}
    \KwResult{$sort(xI,xS)$}
    $xS \gets xI - a \nabla g(x_I)$    \Comment*[r]{Equation \ref{eq:Binary_Search_Phase1_gradDesc}}
    \While{$\nabla g(x_I) \times \nabla g(xS) > 0$}{
        $xI\gets xS$\;
        $xS \gets xI - a \nabla g(x_I)$\;
    }
    \caption{Binary Search Phase 1} \label{alg:Binary_Search_Phase_1}
  \end{algorithm}


The second phase consists in iteratively taking the extreme points and doing a weighted mean obtaining a new delimiter that replaces one of the previous points while preserving the application of the Bolzano theorem for next iterations, narrowing the interval and approximating the points to a local minimum. Moreover, the weight from the mean is updated, according to the interval evolution, given by the function:

\begin{equation}
    sig_{\pm}(x) = \frac{0.125}{1+e^{\pm5(x-0.5)}} -0.0125
    \label{eq:binsea_weight_update_function}
\end{equation}

Assuming a quasi-convex function, if there is a tendency to recurrently approximate to an interval limit, updating the weight it is possible to narrow the interval even more and converge faster to a local minimum.

\SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$xI$, $xS$, $tI$, $w$}
    \KwResult{$xMid$}
    \While{$i < tI$ \textbf{AND} Conditions$^*$}{   
        $xMid \gets w \times xI + (1-w) \times xS $ \Comment*[r]{Weighted Mean}
        \eIf{$\nabla g(xMid) > 0$}{
            $xS\gets xMid$\;
            $w \gets w - sig_{-}(w)$  \Comment*[r]{Sigmoide function \ref{eq:binsea_weight_update_function}}
        }{
            $xI\gets xMid$\;
            $w \gets w + sig_{+}(w)$\;
        }
    }
    \caption{Binary Search Phase 2} \label{alg:Binary_Search_Phase_2}
  \end{algorithm}



\newpage %SÃ³ para ajeitar


\section{Constraint Contour Point}
\label{sec:Constraint_Contour_Point}

Having the objective of global asymptotically  stability of a system, the system converges a desired equilibrium point \(\bar{\xVec}_F\) but kept outside a bounded unsafe set \(\mathcal{B} \subset \mathbb{R}^n\). Depending on certain decision algorithms, such as \ref{eq:CLF-CBF-QP-relaxed} which uses \glsxtrshort{CLF-CBF}, by pursuing another equilibrium point \(\bar{\xVec}_I\) first, the path taken by the system's states \(\xVec \in \mathbb{R}^n\) until \(\bar{\xVec}_F\) could be oriented in order to be a better fit for problem demands, specially to deal more efficiently with the implications of an unsafe set \(\mathcal{B}\), e.g., following a safer and shorter path.\\

\subsection{Constraint Fit}
\label{subsec:Constraint_Fit}

The program formulated in \ref{subsubsec:Constraint_Tangent_Point} responsible to obtain a new equilibrium point is characterized by the use of \glsxtrshort{CBF}. Since the safe set \(\bar{\mathcal{B}}\) (complement of \(\mathcal{B}\)) corresponds to the 0-superlevel set of an unkown safety function \(h_{unkown}: \mathbb{R}^n \to \mathbb{R}\), then:

By doing a \underline{polytopic fit}, via logical operations~\ref{sub:unifying_CBF} smooth-approximations (AND~\ref{eq:Min_smoothFunc_tighted}, OR~\ref{eq:Max_smoothFunc_tighted}), is combined the different 0-superlevel sets \(\mathcal{C}_i \subset \mathbb{R}^n \quad \forall i \in \mathcal{I} = \{1,...,p\}\) of the functions \(h_i: \mathbb{R}^n \to \mathbb{R}\):

\begin{equation}
    \begin{array}{l}
        h_{unkown}(\xVec) \approx h(\xVec) = merge(h_i(\xVec)) \quad \forall i \in \mathcal{I} \\
        \bar{\mathcal{B}} \subseteq \mathcal{C} = \{\xVec \in \mathbb{R}^n: h(\xVec) \geq 0\}
    \end{array}
    \label{eq:Polytope_fit}
\end{equation}

And minimized the size of the set \(\bar{\mathcal{C} }\cap \bar{\mathcal{B}}\). \par
\(\bar{\mathcal{B}}\) can also be approximated by an \underline{ellipsoide fit}:

\begin{align}
        h_{unkown}(\xVec) \approx h(\xVec) &= \frac{1}{2}((\mathbf{O}\xVec - p)^{\top}\mathbf{A_{xis}}(\mathbf{O}\xVec - p) - 1) \label{eq:Ellipsoidal_fit} \\
        &st. \quad \bar{\mathcal{B}} \subseteq \mathcal{C} \label{eq:Ellipsoidal_fit_Condition}
\end{align}

With \(\mathbf{O} \in \mathbb{R}^{c \times n}\) as constrained states observer, \(\mathbf{A_{xis}} \in \mathbb{R}^{c \times c}_{\succ 0}\) and \(p \in \mathbb{R}^{c}\) susceptible to an optimization that tries to minimize the size of the set \(\bar{\mathcal{C} }\cap \bar{\mathcal{B}}\) or at least choosing them such that is guaranteed \ref{eq:Ellipsoidal_fit_Condition} and accepting a fit potentially more conservative. Even though the ellipsoidal approximation is less accurate as is not able to capture the shapes of other figures than ellipses, it shows sufficient for some problems, it provides less complexity and given its properties, suited for some strategies planning. \\


\subsection{State Propagation Simulation}
\label{subsub:State_Propagation_Simulation_Algorithm}

\subsubsection{\glsxtrfull{RK} Methods}
\label{subsub:Runge-Kutta_Methods}

Its a technique used to solve \glsxtrfull{ODE}s~\footnote{An \glsxtrshort{ODE} is characterized by having a independent variable \(x \in \mathbb{R}\) (commonly time \(t\)) and dependent variables \(y \in \mathbb{R}^n\) and its derivatives relative to \(x\), such that an \glsxtrshort{ODE} corresponds to an equation equivalent to \(g(x,y, \frac{dy}{dx}, ... \frac{d^{n}y}{dx^n})= 0\)  } initial-value problems~\footnote{A initial-value-problem is an \glsxtrshort{ODE} \(\dot{\mathbf{y}}(t) = f(t, \mathbf{y}(t))\) with \( f: \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n\) with an initial condition \(t_0,y_0\)} (\(\frac{dy}{dx} = f(x,y)\) with \((x_0, y_0)\)), i.e., to obtain an approximation of a non-linear system solution \(y(x)\) at some point in its domain without using the higher order derivatives of it. \\

The \underline{\textbf{Euler}} method is the quickest one to compute. It shows the most degradation of local truncature error\footnote{A local truncature error corresponds to the error caused by the integrative effect of an iteration of a Runge-Kutta method} as the required (arbitrary valued) step \(\Delta x = x_{n+1} - x_n \hspace{0.2em} (n \in \mathbb{N}_0)\) increases.

\begin{equation}
    y_{n+1} = y_{n} + \Delta x f(x_{n}, y_{n})
    \label{eq:Euler_Method}
\end{equation}

This method has just one stage, while the next \glsxtrfull{RK}is expanded on it such that it allows more. Those stages correspond to intermediate point derivatives based on predictive steps. Introducing to the \textbf{Explictit \glsxtrlong{RK}} \label{subsubsub:RK_Explicit}:

\begin{subequations}
    \begin{align}
        &y_{n+1} = y_n + \Delta x \sum^s_{i=1}b_i k_i \label{eq:Update_RKbased} \\
        &k_i = f(x_n + \Delta x c_i, y_n + \Delta x \sum^{i-1}_{j=1} a_{ij}k_j) \label{eq:Derivatives_Explicit-RK}  
    \end{align}
    \label{eq:Explicit-RK}  
\end{subequations}

Where \(s\) is the number of stages, \(\mathbf{a}\) the \glsxtrfull{RK} matrix, with \(a_{ij} \in \mathbb{R}\) and usually tuned such that \(\sum^{i-1}_{j=1} a_{ij} = c_i\), \(\mathbf{b} \in \mathbb{R}^s\) are the weights, so \(\sum^s_{i=1} b_i = 1\), \(\mathbf{c} \in \mathbb{R}^s\) are the nodes and \(c_i \in [0;1]\) . These parameters are usually presented by a Butcher tableau:

\begin{equation}
    \begin{array}{c|cccc}
        0\\
        c_2 & a_{21}\\
        \vdots & \vdots &\ddots \\
        c_s & a_{s;1} & \hdots & a_{s;s-1}\\
        \hline
        & b_1 & \hdots & b_{s-1} & b_s  
    \end{array} = 
    \begin{array}{c|c}
        \mathbf{c} & \mathbf{a} \\
        \hline
        & \mathbf{b}^\top  
    \end{array}
    \label{eq:Explicit_Butcher_Tableu}
\end{equation}


The \underline{\glsxtrshort{RK} 4th order} is defined by \ref{eq:Explicit-RK} and its tableau:

\begin{equation}
    \begin{array}{c|cccc}
        0\\
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & 0 & \frac{1}{2} \\
        1 & 0 & 0 & 1\\
        \hline
        & \frac{1}{6} & \frac{1}{3} & \frac{1}{3} & \frac{1}{6}  
    \end{array}
    \label{eq:RK4_Tableu}
\end{equation}

Although this method solution is slower compared to Euler~\ref{eq:Euler_Method}, its accuracy is higher. \par
The \textbf{Adaptive \glsxtrlong{RK}} \label{subsubsub:RK_Adaptive} method, with a adaptive step size \(\Delta x\) according to the estimated local truncature error. If its lower than some threshold, the step size increases and the solutions obtained faster since that point, while if it increases is reduced promoting a accurate solution, but with \(\Delta x \in [\Delta x_{min}; \Delta x_{max}]\).  The error estimations are calculated based on two different methods of one order more than the other. For this new \glsxtrlong{RK} variation the Butcher Tableau is:
%If the error is higher than some defined relative tolerance then the step size is reduced and the solution given the new step recomputed at the point, while if its lower than some treshold, the step size increases and the solutions obtained faster sice that point.

\begin{equation} 
    \begin{array}{c|c}
        \mathbf{c} & \mathbf{a} \\
        \hline
        & \mathbf{b}^\top  \\
        & \mathbf{b}^{*\top }
    \end{array}
    \label{eq:Explicit_Butcher_Tableu}
\end{equation}


The low and high order solution are based equally on \ref{eq:Update_RKbased}:

\begin{subequations}
    \begin{align}
        y_{n+1} = y_n + \Delta x \sum^{s}_{i=1}b_i k_i \text{  (low-order/output sol.)} \label{eq:Update_Output_adaptiveRK} \\
        z_{n+1} = y_n + \Delta x \sum^{s}_{i=1}b^*_i k_i \text{  (high-order sol.)} \label{eq:Update_HighOrder_adaptiveRK} 
    \end{align}
    \label{eq:Update_Adaptive-RK}  
\end{subequations}

Where the intermediate derivatives \(k_i\) functions keep the same equation \ref{eq:Derivatives_Explicit-RK} and are equal for both (low and high order methods) reducing some computing time. The local truncature error estimations ig given by:

\begin{equation}
    e_{n+1} = |z_{n+1} - y_{n+1}|
    \label{eq:Local_Truncature_Erro_Estimation_AdaptiveRK}
\end{equation}

This error affects the new step value, the \underline{Dormand Prince} method~\cite{kimura2009dormand} with its particular update step size iteration. This method Butcher Tableau is:

\begin{equation}
    \begin{array}{c|ccccccc}
        0\\
        \frac{1}{5} & \frac{1}{5}\\
        \frac{3}{10} & \frac{3}{40} & \frac{9}{40} \\
        \frac{4}{5} & \frac{44}{45} & -\frac{56}{15} & \frac{32}{9}\\
        \frac{8}{9} & \frac{19372}{6561} & -\frac{25360}{2187} & \frac{64448}{6561} & -\frac{212}{729}\\
        1 & \frac{9017}{3168} & -\frac{355}{33} & \frac{46732}{5247} & \frac{49}{176} & -\frac{5103}{18656}\\
        1 & \frac{35}{384} & 0 & \frac{500}{1113} & \frac{125}{192} & -\frac{2187}{6784} & \frac{11}{84}\\
        \hline
        & \frac{35}{384} & 0 & \frac{500}{1113} & \frac{125}{192} & -\frac{2187}{6784} & \frac{11}{84} & 0  \\
        & \frac{5179}{57600} & 0 & \frac{7571}{16695} & \frac{393}{640} & -\frac{92097}{339200} & \frac{187}{2100} & \frac{1}{40}  
    \end{array}
    \label{eq:Dormand-Prince_Tableu}
\end{equation}

The \(k_7\) derivative is calculated at the predicted point obtained by the low order solution \(y_{n+1}\). The update size step equation as:

\begin{equation}
    \Delta x_{new} = \Delta x \Bigl( \frac{\epsilon\Delta x}{2e_{n+1}}\Bigr)^{\frac{1}{5}}
    \label{eq:Update_Size_Step_Equation}
\end{equation}

With \(\epsilon\) as the error tolerance equal to \(10^{-5}\) in the default version of \href{https://www.mathworks.com/help/matlab/ref/ode45.html}{Matlab ode45}.

\subsubsection{State Propagation Simulation Algorithm}
\label{subsub:State_Propagation_Simulation_Algorithm}

Finding a new equilibrium point \(\bar{\xVec_I}\) requires simulating state convergence. Aiming to simulate a system evolution using a decision algorithm, given the ordinary differential states dynamics equations \(\dot{\xVec} = f(\xVec,\uVec)\) propagated via a runge-kutta method~\ref{subsub:Runge-Kutta_Methods}:

  \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$f$, $\xVec_0$, $tI$, $\Delta t$, $arg^*$}
    \KwResult{$\xVec$, $\uVec$}
    $i \gets 0$ \;
    \While{$i < tI$ \textbf{AND} Conditions$^*$}{   
        $\uVec_i \gets decision^*(\xVec_i, arg^*)$ \Comment*[r]{Decision Algorithm}
        $\xVec_{i+1} \gets integration^*(f, \xVec_i, \uVec_i, \Delta t)$ \Comment*[r]{\small Euler~\ref{eq:Euler_Method},  \glsxtrshort{RK} 4~\ref{subsubsub:RK_Explicit} or DP~\ref{subsubsub:RK_Adaptive}...}
        $i \gets i +1$ \;
    }
    \caption{State Propagation Simulation (SPS)} \label{alg:State_Propagation_Simulation}
  \end{algorithm}


\newpage %SÃ³ para ajeitar

\subsection{Constraint Contour Point Algorithm}
\label{subsec:Constraint_Contour_Point_Algorithm}

\subsubsection{Natural Convergence Direction}
\label{subsubsec:Intersection_Vector}

The new point is obtained accounting with the natural movement direction of those states when they would reach the unsafe zone, which is provided approximately by the propagation algorithm~\ref{alg:State_Propagation_Simulation} if the \glsxtrshort{CBF} was inactive. 

% \SetKwComment{Comment}{/* }{ */}
%   \begin{algorithm}
%     \KwData{$\xVec$}
%     \KwResult{$dir$}
%     \While{$h(\xVec) > 0$ \textbf{AND} Conditions$^*$}{   
%         $\uVec \gets decision(\xVec)$ \Comment*[r]{Decision Algorithm}
%         $dir \gets \frac{f(\xVec, \uVec)}{||f(\xVec, \uVec)||}$ \Comment*[r]{System Dynamic at (\(\xVec, \uVec\)) Normalized}
%         $\xVec \gets integration(\xVec, \uVec)$ \Comment*[r]{Euler\ref{}, Runge-Kutta \ref{}, \ref{}, \ref{}}
%     }
%     \caption{Natural Convergence Direction Vector} \label{alg:Natural_Convergence_Direction_Vector}
%   \end{algorithm}

%   OR

  \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$f$, $\xVec_0$, $tI$, $\Delta t$, $args^*$}
    \KwResult{$dir$, $\xVec_{END}$}
    $(\xVec, \uVec) \gets \text{SPS}\hspace{0.1em}(f, \xVec_0, tI, \Delta t, args^*) \hspace{0.25em} \begin{bmatrix*}[c]  conditions^* \gets h(\xVec) > 0 \\ decision^* \\ integration^* \end{bmatrix*}$ \Comment*[r]{Algorithm \ref{alg:State_Propagation_Simulation}} 
    $dir \gets \frac{f(\xVec_{END}, \uVec_{END})}{||f(\xVec_{END}, \uVec_{END})||}$ \Comment*[r]{System Dynamic at (\(\xVec_{END}, \uVec_{END}\)) Normalized}
    \caption{Natural Convergence Direction Vector (\(NCDV\))} \label{alg:Natural_Convergence_Direction_Vector}
  \end{algorithm}

\(\xVec_{END}\) is approximately the intersection point with the boundary. The vector \(dir\) points the system evolution direction at \(\xVec_{END}\), where the system can aproximate more of \(\bar{\xVec}_F\) given some decision algorithm and assuming always safety. 

\subsubsection{Constraint Tangent Point}
\label{subsubsec:Constraint_Tangent_Point}

Both algorithms are responsible to obtain the farest point (or points \ref{sssec:Constraint_Tangent_Point_Any}), restricted to the two-dimensional Euclidean Space, given the perpendicular direction of \(dir\), obtained using algorithm \ref{alg:Natural_Convergence_Direction_Vector}.\par
The distance along \(dir_{\perp}\) between the starting state \(\xVec_0\) and \(\bar{\xVec_I}\) (\(dp_{min} = v_{\perp}^{\top}(\xVec_0 - \bar{\xVec_I})\)) corresponds to the minimal distance along \(dir_{\perp}\) necessary to contour the unsafe set towards the desired equilibrium point. And the distance along \(dir\) between \(\xVec_0\) and \(\bar{\xVec_I}\) (\(d = v^{\top}(\xVec_0 - \bar{\xVec_I})\))  proportional to the urgency to move along \(dir_{\perp}\), i.e, \(\frac{dp_{min}}{d}\) is equivalent to the rate of change of \(dp\) along \(dir\), imposed by the new equilibrium point \(\bar{\xVec_I}\). %the slope of the rect containing \(\xVec_0\) and \(\xVec_I\), given the change of basis \([v, \hspace{0.1em}v_{\perp}]\), is equivalent to the imposed turning rate by the new equilibrium point \(\xVec_I\).\\
 

\underline{Ellipsoidal}
\label{sssec:Constraint_Tangent_Point_Ellipsoidal} \\

From the vector \(\mathbf{v} = \bigl[\begin{smallmatrix} v_1\\ v_2 \end{smallmatrix} \bigr]\) (\(\mathbf{v} = \mathbf{O} \hspace{0.2em} dir\), with \(\mathbf{O} \in \mathbb{R}^{2 \times n}\) as the observer matrix of the two constrained states) is deduced a tangent point in the respective constraint. This point is the most far from the eminent of colliding unsafe set \(\bar{\mathcal{C}}\) given a perpendicular direction to the obtained vector.%, therefore the new equilibrium point translates as the necessary variation the system states need in that perpendicular direction to pass the unsafe zone.                
Assuming it's used a ellipsoidal fit~\ref{eq:Ellipsoidal_fit} for the construction of a \glsxtrshort{CBF} \(h_0\), where \(\mathbf{A_{xis}} = \bigl[\begin{smallmatrix} a_x&a_u \\ a_u&a_y \end{smallmatrix} \bigr] \in \mathbb{R}^{2 \times 2}_{\succ 0}\) is the ellipse axis defining matrix and \(\mathbf{p} \in \mathbb{R}^{2}\) the center of it, the new point could be obtained using a closed-form solution (to see the full deduction \ref{app:CL_Ellipsoide_Tangent_Point}):


\begin{align}
    &d = \frac{a_u\sqrt{2}}{\sqrt{a_xa_y}} \label{eq:Constraint_Tangent_Point_auxElement1} \\
    \notag \\
    &l = \sqrt{\frac{a_x}{2a_y}} \label{eq:Constraint_Tangent_Point_auxElement2} \\
    \notag \\
    &q = \pm\sqrt{\frac{1}{\frac{a_x}{2}(v_1^2 + v_2^2) + a_yv_1^2}}  \label{eq:Constraint_Tangent_Point_auxAuxElement} \\
    \notag \\
    &\mathbf{z}_i = \begin{bmatrix}  v_2 q \\ v_1 q \end{bmatrix} \quad i \in \{1,2,3,4\} \label{eq:Constraint_Tangent_Point_auxVector} 
\end{align}

In order to obtain both solutions from the perpedincular vectors \(\mathbf{v}_{\perp}\):

\begin{subequations}
    \begin{align}
        &\mathbf{r}_1 = \argminA_{\mathbf{z}_i \in \mathbb{R}^2} = \begin{bmatrix} a_yv_2 & - a_xv_1 \end{bmatrix} \mathbf{z}_{i} \label{eq:Constraint_Tangent_Point_bc_per} \\
        &\mathbf{r}_2 = -\mathbf{r}_1 \label{eq:Constraint_Tangent_Point_bc_perp_inverse}
    \end{align}
    \label{eq:Constraint_Tangent_Point_bc}
\end{subequations}   

%&\mathbf{r}_2 = \argmaxA_{\mathbf{z}_i \in \mathbb{R}^2} = \begin{bmatrix} a_yv_2 & - a_xv_1 \end{bmatrix} \mathbf{z}_{i} \label{eq:Constraint_Tangent_Point_bc_inverse}
    

With \(\mathbf{r} = \bigl[\begin{smallmatrix} \mathbf{r}_1&\mathbf{r}_2\end{smallmatrix} \bigr]\). The respective points are obtained with:

\begin{equation}
    \mathbf{s}_j = \frac{\begin{bmatrix} d \hspace{0.1em}r_{1j} + l \hspace{0.1em}r_{2j} \\ d \hspace{0.1em} r_{2j} - l \hspace{0.1em} r_{1j}\end{bmatrix}}{d^2 + l^2} + \mathbf{p} \quad j\in \{1,2\}
    \label{eq:Constraint_Tangent_Points}
\end{equation}

The closest-point to the current state is chosen as the intermediate equilibrium point:

\begin{equation}
    \mathbf{O}\bar{\xVec_I} = \argminA_{\mathbf{s}_j \in \mathbb{R}^2} ||\mathbf{s}_j - \xVec|| 
    \label{eq:Intermediate_Equilibrium_Point}
\end{equation}


\underline{Any}
\label{sssec:Constraint_Tangent_Point_Any} \\


The previous obtaining strategy is exclusive to ellipsoidal figures, while the proposed  technique aims to be able to find the wanted point for any two dimensional shape. \par 
It is characterized to a discrete behaviour as the search for the point is through steps along \(\mathbf{v} = \bigl[\begin{smallmatrix} \mathbf{v}_1 \\ \mathbf{v}_2 \end{smallmatrix}\bigr]\) and its perpendicular direction \(\mathbf{vp} = \bigl[\begin{smallmatrix} -\mathbf{v}_2 \\ \mathbf{v}_1 \end{smallmatrix}\bigr]\).  \par
Given \(\mathbf{p} = \mathbf{O}\xVec_{END}\) and \(\mathbf{v}\) returned by \(NCDV\)~\ref{alg:Natural_Convergence_Direction_Vector}, the first order \glsxtrshort{CBF} \(h0\), arbitrary  steps sizes, \(step\) and \(stepP\) along \(\mathbf{v}\) and \(\mathbf{vp}\) respectively and a maximum of iterations while searching through the respective directions. 

\newpage %so para ajeitar

  \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$p$, $v$, $h_0$, $step$, $stepP$, $tI$}
    \KwResult{$(p,q)$}

    $vp = v_{\perp}$

    $q \gets FI(p, v, h_0, step)$ \Comment*[r]{Algorithm \ref{alg:Find_Intersection}} 

    $(p_l, q_l) \gets (p, q)$ \Comment*[r]{Initializing the left pair of points}
     
    $(p_r, q_r) \gets (p, q)$ \Comment*[r]{Initializing the right pair of points} 

    \While{1}{   
        $ (p, q) \gets FNLIs(p_l, q_l, v, vp, h_0, step, stepP, tI)$ \Comment*[r]{Algorithm \ref{alg:Find_Next_Layer_Intersections}} 

        \eIf{$any(isempty(p)$ \textbf{OR} $isempty(q))$}{ 
                $(p, q) \gets (p_l, q_l)$\;
                $break$ \Comment*[r]{Reach the top on the left side first} 
            }{
                $(p_l, q_l) \gets (p, q)$\;
            }       


        $ (p, q) \gets FNLIs(p_r, q_r, v, \mathbf{-}vp, h_0, step, stepP, tI)$ \Comment*[r]{Algorithm \ref{alg:Find_Next_Layer_Intersections}} 
        
        \eIf{$any(isempty(p)$ \textbf{OR} $isempty(q))$}{ 
                $(p, q) \gets (p_r, q_r)$\;
                $break$ \Comment*[r]{Reach the top on the right side first} 
            }{
                $(p_r, q_r) \gets (p, q)$\;
            }
    }
    
    \caption{Find Contour Point (FCP)} \label{alg:Find_Contour_Point}
  \end{algorithm}

This algorithm is generally applied to polytopes, which could have multiple farest points, e.g., an edge. So those points could be approximately two vertices of the polytope of the same edge or considered just one vertice if they are too close. \par
Depending how small the steps are and how big is the total number of searching iteration steps along \(v\) and \(vp\) (and their inverses \(-v\) and \(-vp\)), the more robust the algorithm  is.\par
The algorithm starts by obtaining \(q\) at the intersection of the boundary with the half-line \(p + v\times b, \quad b \in \mathbb{R}_{>0}\). Following with a iterative  search through the layers of each side (pointed by \(\mathbf{vp}\) and its symmetric) separated by a distance equal to \(stepP\) until reach the peak of one of the sides indicated by the inexistence of intersections returned by \(FNLIs\).\\


  \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$p0$, $q0$, $v$, $vp$ $h_0$, $step$, $stepP$, $tI$}
    \KwResult{$(p_{new}, q_{new})$}
    
    $[p0, q0] \gets [p0, q0] + vp \times stepP$ \Comment*[r]{Jump of size $stepP$ to the next layer}
    
    
    $(p_{new}, r) \gets FLI(p0, q0, v, h_0, step, [tI, tI])$
        
    \If{$not(isempty(p_{new}))$}{
        \If{$r \geq 1$}{
            $p0 \gets p_{new}$\;
            \If{$r == 2$}{ 
                $q0 \gets p0 + v \times step$
            }
        }
        $(q_{new}, -) \gets FLI(q0, p0, -v, h_0, step, [tI, 0])$
    }
      
    \caption{Find Next Layer Intersections (FNLIs)} \label{alg:Find_Next_Layer_Intersections}
  \end{algorithm}

  Through \(FNLIs\) obtains two intersection points between the layer and boundary defined according with the \glsxtrshort{CBF} \(h0\). If the first point isn't found then is assumed inexistence of intersections as it is expected a resultant pair of points from the intersection.\\

    \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$p0$, $q0$, $v$, $vp$ $h_0$, $step$, $stepP$, $tI(1:2)$}
    \KwResult{$(p_{new}, reach)$}
    $reach \gets 0$ \Comment*[r]{Indicates $p_{new}$ relative to $q0$}
    $p \gets p0$ \Comment*[r]{Starting the search} 
    \While{$i \leq tI(1)$}{      

        \If{$h_0(xp)\leq 0$ ($\mathbf{O}xp = p$)}{
            $p_{new} \gets FI(p, -v, h_0, step)$\Comment*[r]{Obligatory use of Algorithm \ref{alg:Find_Intersection}.}
            $break$ \Comment*[r]{Here $step$ can be reduced if it's wanted more accuracy.}
        }
        
        $p \gets p - v \times step$ \Comment*[r]{Search for the interior of the unsafe set along \(-v\)}
    }

    \If{$isempty(p_{new})$}{
        $reach \gets 1$ \Comment*[r]{Searching in the direction of \(q0\)}
        $p \gets p0$ \Comment*[r]{If didn't intersect, it comebacks to the initial point}
        \While{$i \leq tI(2) + \frac{||p0-q0||}{step}$}{      


            \If{$h_0(xp)\leq 0$ ($\mathbf{O}xp = p$)}{
                $p_{new} \gets FI(p, -v, h_0, step)$\Comment*[r]{Optional use of Algorithm \ref{alg:Find_Intersection}.} 
                $break$ \Comment*[r]{Here $step$ can be reduced if it's wanted more accuracy.}
            }

            $p \gets p + v \times step$ \Comment*[r]{Search along \(v\) of the unsafe set interior}

        }
        \If{$i > \frac{||p0-q0||}{step}$}{
                $reach \gets 2$ \Comment*[r]{It reached \(q0\)}
            }
    }
      
    \caption{Find Layer Intersection (FLI)} \label{alg:Find_Layer_Intersection}
  \end{algorithm}

  This algorithm is responsible for obtaining one of the layer intersections for a chosen direction \(v\). It starts by searching along the layer, propagated by steps of size \(step\) and with \(-v\) direction. If it isn't capable to find any point it follows the symmetric direction \(v\).\\

    \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$p$, $v$, $h_0$, $step$}
    \KwResult{$p$}
    \While{$h_0(p)\leq 0$ ($\mathbf{O}xp = p$)}{   
        $p \gets p + v \times step$
    }
    $p \gets p - v \times step$\;
    \caption{Find Intersection (FI)} \label{alg:Find_Intersection}
  \end{algorithm}

 The function returns the intersection point between a half line (\(p + v \times b, \quad b\in \mathbb{R}_{>0}\)), contained in the layer, and the boundary. The function is used for cases where \(p\) starts  already inside the boundary so it is expected an approximated intersection point. The intersection point of the given function is more accurate the smaller the \(step\). \\

\newpage %SÃ³ para ajeitar

\section{Proposed Decision Algorithms}
\label{sec:Proposed_Decision_Algorithms}

All the proposed algorithms are based on \glsxtrshort{CLF-CBF}~\ref{sec:clf_cbf} decision algorithm, having the objective of stabilizing a system while prioritizing its safety. Using \glsxtrshort{CLF-CBF} allows a relative lower computational effort, in particular its closed form solution~\ref{sub:closed_form}, compared to other techniques, however it lacks adaptive power as its constant parameters consistently dictates an inefficient system behavior for probably the majority of the encountered  scenarios. \\    


\subsection{\glsxtrfull{A-JO}}
\label{subsec:Just_Optimized_Algorithm}

Optimizing parameters can improve the system behaviour for the scenario it is deparing with. And an eventual optimization makes its behaviour more suited to actuate in an multi-scenario environment. \par
Based on single parameter optimization~\ref{sec:Single_Parameter_Optmization}, the proposed technique is centered on the slope \(\alpha_d\) of the \glsxtrshort{CBF} constraint function \(\alpha:\mathbb{R} \to \mathbb{R}\) (\glsxtrshort{CLF-CBF} \glsxtrshort{QP}~\ref{eq:CLF-CBF-QP-relaxed}) as an optimization variable:

\begin{equation}
    \alpha(h, \alpha_d) = \alpha_d h
    \label{eq:CBF_Alpha_Formulation_AJO}
\end{equation}

The parameter is optimized using the binary search algorithm \ref{subsec:Binary_Search} but for that is required a cost function \(g_{\xVec_0}:\mathbb{R} \to \mathbb{R}\) which accounts with a whole prediction horizon:

\begin{equation}
    \begin{array}{ll}
        g_{\xVec_0}(\alpha_d) &= g(\alpha_d, \xVec_0 = cte)  \\   
        &= \sum_{k=0}^{N}\xVec_{k+1}(\xVec_k,\uVec_k)^{\top}\mathbf{Q}\xVec_{k+1}(\xVec_k,\uVec_k) +  \uVec_k(\alpha_d,\xVec_k)^{\top}\mathbf{R}\uVec_k(\alpha_d,\xVec_k) \\
        %&= \sum_{k=0}^{N}\xVec_{k+1}(\alpha_d,\xVec_k)^{\top}\mathbf{Q}\xVec_{k+1}(\alpha_d,\xVec_k) +  \uVec_k(\alpha_d,\xVec_k)^{\top}\mathbf{R}\uVec_k(\alpha_d,\xVec_k) \\
        &= \sum_{k=0}^{N}\xVec_{k+1}(\alpha_d,\xVec_0)^{\top}\mathbf{Q}\xVec_{k+1}(\alpha_d,\xVec_0) +  \uVec_k(\alpha_d,\xVec_0)^{\top}\mathbf{R}\uVec_k(\alpha_d,\xVec_0) \\
        &= \mathbf{X}(\alpha_d,\xVec_0)^{\top}\tilde{\mathbf{Q}}\mathbf{X}(\alpha_d,\xVec_0) +  \mathbf{U}(\alpha_d,\xVec_0)^{\top}\tilde{\mathbf{R}}\mathbf{U}(\alpha_d,\xVec_0)
    \end{array}
    \label{eq:Cost_Function_Alpha}
\end{equation}

Where \(N\) is equivalent to the total of samples, \(\xVec_0\) the present state, \(\mathbf{Q} \in \mathbb{R}^n_{\succ 0}\) is the states cost weight matrix, \(\mathbf{R} \in \mathbb{R}^m_{\succ 0}\) is the inputs cost weight matrix.  \par

Because of the n states and inputs continuous differential solution (\(\xVec(t) and \uVec(t)\)), potentialized by the conditioned control input values given by CLF-CBF closed-form controller~\ref{eq:closed-form_controller}, the gradient \(\nabla g_{\xVec_0}(\alpha_d)\) exact equation is difficult to have it, hence is required a discretization of it~\ref{subsec:Gradient_Cost_Function_Discretization}. Which leds to the exact function of \(g_{\xVec_0}(\alpha_d)\) which is not known as \(\uVec_k\) and \(\xVec_{k+1}\) could not be obtained directly.\par 
Digitally, through the \(SPS\) algorithm \ref{alg:Discrete_Gradient_Function} is computed the approximated states \(\mathbf{X}(\alpha_d,\xVec_0)\) and inputs \(\mathbf{U}(\alpha_d,\xVec_0)\) along the horizon. Like it was said, \(\uVec_k\) is obtained based on a closed-form \glsxtrshort{CLF-CBF} controller~\ref{sub:closed_form}, governed by a \glsxtrshort{CLF} \(V\) and a \glsxtrshort{CBF} \(h\). Meanwhile, given the control action \(\uVec_k\), \(\xVec_{k+1}\) is propagated by an explicit \glsxtrshort{RK}\ref{subsubsub:RK_Explicit} in particular defined by the fourth order \glsxtrshort{RK} Butcher Tableau\ref{eq:RK4_Tableu}, more accurate than the Euler method~\ref{eq:Euler_Method} and faster than the \glsxtrshort{DP}~\ref{eq:Dormand-Prince_Tableu}. So given the system dynamics equation \(f:\mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n\) and integration time interval \(\Delta t\):

\begin{equation}
    \begin{bmatrix} \mathbf{X}(\alpha_d,\xVec_0) & \mathbf{U}(\alpha_d,\xVec_0) \end{bmatrix} = SPS(f, \xVec_0, N, \Delta t, V, h, \alpha_d) \hspace{0.2em} \begin{bmatrix*}[l] conditions^* \gets V(\xVec_i) \geq tol\\ decision^* \gets \text{\glsxtrshort{CLF-CBF}~\ref{sub:closed_form} } \\ integration^* \gets \text{\glsxtrshort{RK}4~\ref{eq:RK4_Tableu}} \end{bmatrix*}
    \label{eq:CLF-CBF_RK4_Propagation}
\end{equation}

So, choosing \(nS\) number of samples, \(devS\) referent to the distance between them, \(dev\) the discretization step and with \(g_{\xVec_0}(\alpha_d)\) given by \ref{eq:CLF-CBF_RK4_Propagation}:

\begin{equation}
    \nabla g_{\xVec_0}(\alpha_d) = DGF(nS,devS,dev, g_{\xVec_0},\alpha_d)  
    \label{eq:discrete_gradient_function}
\end{equation}

With \(DGF\) (Discrete Gradient Function) as the algorithm \ref{alg:Discrete_Gradient_Function}  and therefore enabling the optimization of the slope \(\alpha_d\) via \ref{sec:Single_Parameter_Optmization}.\par
Among the presented single parameter optimization techniques there are:

\begin{itemize}
    \item Gradient Descent \ref{eq:Gradient_Descent_Normal}
    \item Gradient Descent with Momentum \ref{eq:Gradient_Descent_Momentum}
    \item Nesterov's Accelerated Gradient Descent \ref{eq:Gradient_Descent_Nesterov}
    \item Triple Momentum Gradient Descent \ref{eq:Gradient_Triple_Momentum}
    \item Default: Binary Search Algorithm \ref{alg:Binary_Search_Phase_1}-\ref{alg:Binary_Search_Phase_2}
\end{itemize}

The Binary Search is preferred due to be suited to quasi-convex specters, which was verified for the given problems.\par 
Theoretically the CBF should guarantee safety, however due to not be a continuous system (or integration errors), having a high \(\alpha_d\) can lead to unsafe sets and can also provide a demand on a not possible control input values to keep the system safe, but if that happens is expected to reflect on the path taken, control inputs and therefore penalize in the cost function as in the gradient, although it could be reflected as noise during the optimization.\\

Afterwards, having the optimized \(\alpha_d\), it is used the typical \glsxtrshort{CLF-CBF} framework, more precisely the closed-form controller \ref{eq:closed-form_controller} as decision algorithm to speed up.



\subsection{Other Cost Function}
\label{subsubsec:Other Cost Function}

The proposed algorithm follows an optimization based on a cost function given a whole horizon of states, yet changing it could give different results \ref{sec:experiments}. 

\begin{equation}
    \begin{array}{ll}
        g_{\xVec_0}(\alpha_d) &= g(\alpha_d, \xVec_0 = cte)  \\   
        &= \xVec_{N+1}(\alpha_d,\xVec_0)^{\top}\mathbf{Q}\xVec_{N+1}(\alpha_d,\xVec_0) + \sum_{k=0}^{N}  \uVec_k(\alpha_d,\xVec_0)^{\top}\mathbf{R}\uVec_k(\alpha_d,\xVec_0) \\
        &= \xVec_{N+1}(\alpha_d,\xVec_0)^{\top}\mathbf{Q}\xVec_{N+1}(\alpha_d,\xVec_0) +  \mathbf{U}(\alpha_d,\xVec_0)^{\top}\tilde{\mathbf{R}}\mathbf{U}(\alpha_d,\xVec_0)
    \end{array}
    \label{eq:Cost_Function_Alpha_last_state}
\end{equation}

This cost function just takes into account the last state from the horizon, i.e., paths where the last system state is as close as possible to the desired state at the end of the horizon, or even reached it return lower costs, and between those paths the less input consuming and generally the shortest paths are the optimal ones. Compared to \ref{eq:Cost_Function_Alpha} it demands a higher \(\mathbf{Q}\), so the cost value returned could be relatively high according to how far from the stability objective the system is.


\begin{equation}
    \begin{array}{ll}
        g_{\xVec_0}(\alpha_d) &= g(\alpha_d, \xVec_0 = cte)  \\   
        &= \sum_{k=0}^{N} \xVec_{k+1}(\alpha_d,\xVec_0)^{\top}\mathbf{Q}_i\xVec_{k+1}(\alpha_d,\xVec_0) +  \uVec_k(\alpha_d,\xVec_0)^{\top}\mathbf{R}\uVec_k(\alpha_d,\xVec_0)
    \end{array}
    \label{eq:Cost_Function_Alpha_evol_state}
\end{equation}

Where \(det(\mathbf{Q}_i) \leq  det(\mathbf{Q}_{i+1})\), in order to keep a similar behaviour to \ref{eq:Cost_Function_Alpha_last_state} without the optimized path simply targeting the last instant.

\subsection{\glsxtrfull{A-CLF-S}}
\label{subsec:CLFs_Summed_Algorithm}


The \glsxtrshort{CLF-CBF} returns control input values based on purely the current state, resulting in global (infinite horizon) suboptimal solutions that could lead to states relatively close to the barrier (given some safety tunable definitions) and unwanted inputs to deviate from it. So, changing the notion of stability by making the system follow another equilibrium point first could help mitigate the problems. This technique focuses  on the transition between two \glsxtrshort{CLF} aiming to converge for each of the equilibrium points, the current and next. \par


With constraint contour point algorithm~\ref{subsec:Constraint_Contour_Point_Algorithm} is obtained the new equilibrium point \(\bar{\xVec_C} \in \mathbb{R}^n\). Given a system, with dynamics \(f:\mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}^n \), at the state \(\xVec_0 \in \mathbb{R}^n\), a \glsxtrshort{CLF} \(V\) that targets \(\bar{\xVec_N} \in \mathbb{R}^n\) and \glsxtrshort{CBF} \(h\), the new equilibrium point could be obtained:


\begin{equation}
    \begin{array}{l}
        [dir, \hspace{0.1em} \xVec_{int}] = NCDV(f, \xVec_0, N, \Delta t, V, h) \hspace{0.2em} \begin{bmatrix*}[l] decision^* \gets \text{\glsxtrshort{CLF-CBF}~\ref{sub:closed_form} } \\ integration^* \gets \text{\glsxtrshort{RK}4~\ref{eq:RK4_Tableu}} \end{bmatrix*}  \\
        \mathbf{v} = \mathbf{O}\hspace{0.1em}^.\hspace{0.1em}dir   
    \end{array}
    \label{eq:New_Equlibrium_Point_DirVec_CLF-CBF_RK4}
\end{equation}



Where \(NCDV\) (Natural Convergence Direction Vector) is algorithm~\ref{alg:Natural_Convergence_Direction_Vector}, and \(\mathbf{O} \in \mathbb{R}^{2 \times n}\) the constrained states observer matrix. Regarding the choice of \glsxtrshort{RK}4 over other methods is because as it was said before, since it is more accurate than the Euler method~\ref{eq:Euler_Method} and faster than the \glsxtrshort{DP}~\ref{eq:Dormand-Prince_Tableu}. \par
Given the direction vector \(\mathbf{v}\), assuming an ellipsoidal fit \ref{eq:Ellipsoidal_fit}, with the constraint tangent point  closed form solution~\ref{subsubsec:Constraint_Tangent_Point} like this obtain \(\bar{\xVec_C}\). If the system is deparing with a polytope \glsxtrshort{CBF} \(h_0\), then using algorithm \ref{alg:Find_Contour_Point} with \(\mathbf{v}\) and \(\xVec_{int}\) as arguments is obtained the new equilibrium point(s).\\


\subsubsection{Baseline Framework}
\label{subsubsec:CLFs_Summed_Algorithm_Baseline_Framework}

From both equilibrium points is defined two \glsxtrshort{CLF}, \(V_{C_0}\) following the current equilibrium point \(\bar{\xVec_C}\) and \(V_{N_0}\) stabilizing in \(\bar{\xVec_N}\). By doing a weighted  mean between both \glsxtrshort{CLF} is defined a new one (a generalization for multiple \glsxtrshort{CLF} can be found in \ref{app:M_CLF_Transition_Formulation}):

\begin{align}
    &\sigma(\xVec) = \frac{1}{1+e^{-\zeta V_{C_0}(\xVec)}} \label{eq:sig_CLF_Vc0} 
\end{align}

\begin{align}
    &V_{0_1}(\xVec) =    \sigma(\xVec) V_{C_0}(\xVec) + (1 - \sigma(\xVec))V_{N_0}(\xVec) \label{eq:1stPhase_2-CLF-S}   
\end{align}

\begin{align}
    V_0(\xVec) = 
    \begin{cases}
        V_{0_1}(\xVec) \quad \xVec \in \mathcal{S}\\
        V_{0_2}(\xVec) = V_{N_0}(\xVec) \quad \xVec \in \bar{\mathcal{S}}
    \end{cases}
    \label{eq:New_2-CLF-S}
\end{align}

\begin{equation}
    \mathcal{S} = \{\xVec \in \mathbb{R}^n: V_{C_0}(\xVec) \geq tol \cap \dot{V}_{C_0}(\xVec, \uVec) \leq 0 \cap \delta \leq tol_2 \}
    \label{eq:New_2-CLF-S_actuationPhaseSet}
\end{equation}

Where \(\sigma(V_{C_0}(\xVec))\) is a sigmoide function responsible to the transition between the \glsxtrshort{CLF} and \(\zeta \in \mathbb{R}_{\geq 0}\) is its slope. \(\mathcal{S}\) refers to the just one time activation set of the respective \glsxtrshort{CLF} \(V_{0_1}\), with \(\delta\) referent to the \glsxtrshort{CLF-CBF} \glsxtrshort{QP} optimized relaxation variable. 

The sets of possible control inputs, given the new \glsxtrshort{CLF} \(V_{0_1}\) and assuming a high slope \(\zeta\) (\(\dot{\sigma}(\xVec, \uVec_0)\approx 0\)), is approximately:

\begin{align}
    K_{V_{0_1}}(\xVec) = \Bigl\{\uVec_0 \in \mathbb{R}^m: \dot{V_{0_1}}(\xVec, \uVec_0) &= \sigma(\xVec) \dot{V}_{C_0}(\xVec, \uVec_0)  + (1-\sigma(\xVec))\dot{V}_{N_0}(\xVec, \uVec_0) \notag \\
    &\leq -\gamma( \mu \sigma(\xVec) V_{C_0}(\xVec) + (1 - \sigma(\xVec))V_{N_0}(\xVec)  )\Bigr\}
    \label{eq:2-CLF-S_Constraint}
\end{align}

With \(\mu\) as a parameter with default value as \( \frac{V_{N_0}(\xVec)}{V_{C_0}(\xVec)} \). The set \(K_{V_{0_2}}(\xVec)\) is equivalent to \ref{eq:K-CLF}. \\

%\textbf{\underline{Second-Order Systems}}
\subsubsection{Second-Order Systems}
\label{subsubsubsec:CLFs_Summed_Algorithm_2nd_order}

Given the backstepping formulation~\ref{sub:backstepping}, it is assumed the previous obtained \glsxtrshort{CLF}~\ref{eq:New_2-CLF-S} as the first-order formulation and \ref{eq:2-CLF-S_Constraint} as the condition.\par 
Now, considering a second-order system \ref{eq:HH-NL-System}, the \glsxtrshort{CLF} that are trying to converge to \(\bar{\xVec}_C\) and  \(\bar{\xVec}_N\) respectively are:


\begin{align}
    &V_C(\xVec, \xi) = V_{C_0}(\xVec) + V_{\xi}(\xVec, \xi) \label{eq:hh_current_clf}\\
    &V_N(\xVec, \xi) = V_{N_0}(\xVec) + V_{\xi}(\xVec, \xi) \label{eq:hh_next_clf}
\end{align}

Given \(V_C\) and \(V_N\) it is possible to obtain the new \glsxtrshort{CLF} \(V\) equally as \ref{eq:1stPhase_2-CLF-S}-\ref{eq:New_2-CLF-S} but keeping the first-order sigmoide function \(\sigma(\xVec)\) from \ref{eq:sig_CLF_Vc0}. That beeing said, it is possible to reduce the \glsxtrshort{CLF} \(V\) equation to:

\begin{equation}
    V(\xVec, \xi) = V_0(\xVec) + V_{\xi}(\xVec, \xi)
    \label{eq:HH_New_2-CLF-S}
\end{equation}\\

The sets of possible control inputs, given the new \glsxtrshort{CLF} \(V_{1}\) and assuming a high slope \(\zeta\) (\(\dot{\sigma}(\xVec, \xi)\approx 0\)), is approximately:


\begin{align}
    K_{V_{1}}(\xVec, \xi) = \Bigl\{\uVec \in \mathbb{R}^m: \dot{V_{1}}(\xVec, \xi, \uVec) &= \dot{V}_{0_1}(\xVec, \xi \mathcolor{gray}{, \uVec}) + \dot{V}_{\xi}(\xVec, \xi, \uVec) \notag \\
    &\leq -\gamma( \mu \sigma(\xVec) V_{C_0}(\xVec) + (1 - \sigma(\xVec))V_{N_0}(\xVec) + V_{\xi}(\xVec, \xi))\Bigr\}
    \label{eq:HH_2-CLF-S_Constraint}
\end{align}

Keeping \(\mu\) as a parameter with default value as \( \frac{V_{N_0}(\xVec)}{V_{C_0}(\xVec)} \) and the set \(K_{V_{2}}(\xVec)\) equivalent to \ref{eq:K-CLF}. \textcolor{gray}{If it is a mixed-relative-degree-two system like the unicycle model used in \ref{eq:unicycle_model}.} \\

Regarding the stability objective changing condition, it maintains equal to \ref{eq:New_2-CLF-S_actuationPhaseSet} using the first-order \glsxtrshort{CLF} and control input \(\mathbf{k}_0\):

\begin{equation}
    \mathcal{S} = \{\xVec \in \mathbb{R}^n: V_{C_0}(\xVec) \geq tol \cap \dot{V}_{C_0}(\xVec, \mathbf{k}_0) \leq 0 \cap \delta \leq tol_2 \}
    \label{eq:HH_New_2-CLF-S_actuationPhaseSet}
\end{equation}\\


Finally, with \glsxtrshort{A-JO}~\ref{subsec:Just_Optimized_Algorithm}, using the cost function \ref{eq:Cost_Function_Alpha_last_state} adapt the \glsxtrshort{CBF} slope \(\alpha_d\) to the new low and high order \glsxtrshort{CLF}s and improve possibly even more the state convergence to the final equilibrium point.\\

%\textbf{\underline{Multiple Waypoints}}
\subsubsection{Multiple Waypoints (Polytope Version)}
\label{subsubsubsec:CLFs_Summed_Algorithm_multiple_waypoints}

If the unsafe set is restricted by a polytopic fit~\ref{eq:Polytope_fit} and the contour point obtaining algorithm is \ref{alg:Find_Contour_Point} then it is returned two equilibrium points. Those points are approximately two vertices of the polytope of a same edge or just one vertice if they are too close.  It keeps having a current point a \glsxtrshort{CLF} is trying to converge but the next one don't necessarily is the desired final equilibrium point \(\bar{\xVec}_F\) but the other computed point.\par
For \(N \in \mathbb{N}\) waypoints, \(i \in \mathcal{I}=\{1,...,N\}\) the first order \glsxtrshort{CLF} is defined as:

\begin{align}
    V_{0_i}(\xVec) = 
    \begin{cases}
        \sigma_i(\xVec) V_{C_{0_i}}(\xVec) + (1 - \sigma_i(\xVec))V_{N_{0_i}}(\xVec) \\
        \sigma_i(\xVec) = \frac{1}{1+e^{-\zeta V_{C_{0_i}}(\xVec)}} 
    \end{cases}
    \label{eq:1stPhase_2-CLF-S_multiple_waypoints}
\end{align}

\begin{align}
    V_0(\xVec) = 
    \begin{cases}
        V_{0_i}(\xVec) \quad \xVec \in \mathcal{S}_i \qquad i \in \mathcal{I}\setminus\{N\}\\
        V_{0_N}(\xVec) = V_{F_0}(\xVec) \quad \xVec \in \bigcup_{i = 1}^{N-1} \bar{\mathcal{S}}_i
    \end{cases}
    \label{eq:New_2-CLF-S_multiple_waypoints}
\end{align}


With \(V_{C_{0_i}}\) and \(V_{C_{0_i}}\) the base \glsxtrshort{CLF} defining the stability of the current (the computed points(in order of wanted convergence)) and corresponding next (the intermediate and final) convergence points. \par
The sets of possible control inputs, given the new \glsxtrshort{CLF} \(V_{0_i}\) and assuming a high slope \(\zeta\) (\(\dot{\sigma_i}(\xVec, \uVec_0)\approx 0\)),  are approximately:

\begin{align}
    K_{V_{0_i}}(\xVec) = \Bigl\{\uVec_0 \in \mathbb{R}^m: \dot{V}_{0_i}(\xVec, \uVec_0) &= \sigma_i(\xVec) \dot{V}_{C_{0_i}}(\xVec, \uVec_0)  + (1-\sigma_i(\xVec))\dot{V}_{N_{0_i}}(\xVec, \uVec_0) \notag \\
    &\leq -\gamma( \mu \sigma_i(\xVec) V_{C_{0_i}}(\xVec) + (1 - \sigma_i(\xVec))V_{N_{0_i}}(\xVec))\Bigr\}
    \label{eq:2-CLF-S_Constraint_multiple_waypoints}
\end{align}

Now with the default value of \(\mu =\frac{V_{F_{0}}(\xVec)- V_{N_{0_i}}(\xVec)(1 -\sigma_i(\xVec))}{\sigma_i(\xVec) V_{C_{0_i}}(\xVec)} \) .\\


Now considering the second-order \glsxtrshort{CLF}:

\begin{equation}
    V(\xVec, \xi) = V_0(\xVec) + V_{\xi}(\xVec, \xi)
    \label{eq:HH_New_2-CLF-S_multiple_waypoints}
\end{equation}


Given the new \glsxtrshort{CLF} \(V_{i}\) and assuming a high slope \(\zeta\) (\(\dot{\sigma_i}(\xVec, \xi)\approx 0\)):

\begin{align}
    K_{V_{i}}(\xVec, \xi) = \Bigl\{\uVec \in \mathbb{R}^m: \dot{V}_{i}(\xVec, \xi,  \uVec) &= \dot{V}_{0_i}(\xVec, \xi \mathcolor{gray}{, \uVec}) + \dot{V}_{\xi}(\xVec, \xi, \uVec) \notag \\
    &\leq -\gamma( \mu \sigma_i(\xVec) V_{C_{0_i}}(\xVec) + (1 - \sigma_i(\xVec))V_{N_{0_i}}(\xVec) + V_{\xi}(\xVec, \xi))\Bigr\}
    \label{eq:HH_2-CLF-S_Constraint_multiple_waypoints}
\end{align}

Beeing the default value of \(\mu =\frac{V_{F_{0}}(\xVec)- V_{N_{0_i}}(\xVec)(1 -\sigma_i(\xVec))}{\sigma_i(\xVec) V_{C_{0_i}}(\xVec)} \) .\\


The transitive (\(i = i+1\)) condition is based on the first-order \glsxtrshort{CLF} \(V_{C_i}\) as it was already seen~\ref{eq:HH_New_2-CLF-S_actuationPhaseSet} given by:

\begin{equation}
    \mathcal{S}_i = \{\xVec \in \mathbb{R}^n: V_{C_{0_i}}(\xVec) \geq tol \cap \dot{V}_{C_{0_i}}(\xVec, \mathbf{k}_0) \leq 0 \cap \delta \leq tol_2 \}
    \label{eq:HH_New_2-CLF-S_actuationPhaseSet}
\end{equation}\\



    \SetKwComment{Comment}{/* }{ */}
  \begin{algorithm}
    \KwData{$f$, $\xVec_0$, $N$, $\Delta t$, $V$, $h$, $\mathbf{O}$, $waypoints$, $nIte = 1$}
    \KwResult{$V$, $waypoints$, $\alpha_d$}
    $i \gets 0$\;
    \While{$i < nIte$, $i++$}{ 
        $[dir, \hspace{0.1em} \xVec_{int}] = NCDV(f, \xVec_0, N, \Delta t, V_{1st-waypoint}, h) \hspace{0.2em} \begin{bmatrix*}[l] decision^* \gets \text{\glsxtrshort{CLF-CBF}~\ref{sub:closed_form} } \\ integration^* \gets \text{\glsxtrshort{RK}4~\ref{eq:RK4_Tableu}} \end{bmatrix*}$\;
        $ [\mathbf{v}, \hspace{0.1em} \mathbf{p}] \gets \mathbf{O}\hspace{0.1em}^.\hspace{0.1em}[dir, \hspace{0.1em} \xVec_{int}]$ \;
        $waypoints \gets [CCP^*(\mathbf{v},\mathbf{p}, h_0), \hspace{0.1em} waypoints]$ \Comment*[r]{Ellipse~\ref{sssec:Constraint_Tangent_Point_Ellipsoidal}, Any~\ref{sssec:Constraint_Tangent_Point_Any}}
    }
    $V \gets $ Multiple $waypoints\hspace{0.2em} \begin{Bmatrix*}[l] 1^{st}\text{ order \glsxtrshort{CLF}~\ref{eq:New_2-CLF-S_multiple_waypoints}, condition~\ref{eq:2-CLF-S_Constraint_multiple_waypoints}, transition~\ref{eq:HH_New_2-CLF-S_actuationPhaseSet}} \\ \text{High order \glsxtrshort{CLF}~\ref{eq:HH_New_2-CLF-S_multiple_waypoints}, condition~\ref{eq:HH_2-CLF-S_Constraint_multiple_waypoints}}\end{Bmatrix*}$\;
    $\alpha_d \gets$ \glsxtrshort{A-JO} $\begin{bmatrix*}[l] g_{\xVec_0}(\alpha_d)^* \gets \text{Last State~\ref{eq:Cost_Function_Alpha_last_state}}\end{bmatrix*}$\;
    \caption{CLF-Summed (\glsxtrshort{A-CLF-S})} \label{alg:A-CLF-S}
  \end{algorithm}

 Given the new first and high order \glsxtrshort{CLF}, its conditions and transitions, and the new \(\alpha_d\), the system control action are decided based on the \glsxtrshort{CLF-CBF} closed-form controller~\ref{eq:closed-form_controller}. \\

When sending existing points, finding others helps to draw a path to existing waypoints, and that includes when \(nIte >1\).\par
The general \glsxtrshort{CLF} \(V_0\) formulation is described as the 2-norm between the current output (\(\mathbf{y} = \mathbf{C}\xVec, \hspace{0.2em}\mathbf{C}\) is the observer matrix) and the respective waypoint (\(||\mathbf{y}-\mathbf{ref}||\)).


\subsubsection{\glsxtrfull{A-CLF-I}}
\label{subsubsec:CLFs_Independent_Algorithm}

From both equilibrium points is defined two \glsxtrshort{CLF}, \(V_C\) following the current equilibrium point \(\bar{\xVec_C}\) and \(V_N\) stabilizing in \(\bar{\xVec_N}\). If \(\zeta \to \infty\), according to equations \ref{eq:1stPhase_2-CLF-S} and \ref{eq:New_2-CLF-S}:


\begin{align}
    V(\xVec) = 
    \begin{cases}
        V_C(\xVec) \quad \xVec \in \mathcal{S}\\
        V_N(\xVec) = V_{N}(\xVec) \quad \xVec \in \bar{\mathcal{S}}
    \end{cases}
    \label{eq:New_2-CLF-I}
\end{align}


\begin{equation}
    \mathcal{S} = \{\xVec \in \mathbb{R}^n: V_{C}(\xVec) \geq tol \cap \dot{V}_{C}(\xVec, \uVec) \leq 0 \cap \delta \leq tol_2\}
    \label{eq:New_2-CLF-I_actuationPhaseSet}
\end{equation}



And if \(\mu = \frac{V_{N}(\xVec)}{V_{C}(\xVec)}\) then:

\begin{align}
    K_{V_C}(\xVec) = \Bigl\{\uVec \in \mathbb{R}^m:  \dot{V}_{C}(\xVec, \uVec) \leq -\gamma( V_{N}(\xVec)  )\Bigr\}
    \label{eq:2-CLF-I_Constraint}
\end{align}

The set \(K_{V_N}(\xVec)\) is is defined by \ref{eq:K-CLF}. \par


\newpage %so para ajeitar

\subsubsection{Last Control Phase \(\mathbf{k}_N\)}
\label{subsubsec:Last_Control_Phase}

Using \glsxtrfull{A-CLF-S} and other \glsxtrfull{CLF-CBF} based controllers, as the system gets closer to the equilibrium point, its speed of convergence slows down, possibly not reaching the point in an acceptable time as can be seen in the results~\ref{sec:experiments}. \par
The \glsxtrshort{A-CLF-S} last phase control is governed by a typical \glsxtrshort{CLF-CBF}, which its \glsxtrshort{CLF} stability objective is just the final equilibrium point, which can be replaced by other control techniques, preferably when there is no possible collision on sight.

Aiming low cost resources, there are optimal control techniques such as:

\begin{itemize}
    \item \underline{\glsxtrfull{LQR}}: Unable to account with constraints. However, by simulating the nominal \glsxtrshort{CLF-CBF} trajectory, if there is no collision detected and separation is small, it can be a fast solution.
    \item \underline{\glsxtrfull{MPC}}: The solution obtaining can be speed up using as warm start solution the nominal \glsxtrshort{CLF-CBF} solution obtained via \(SPS\) algorithm~\ref{subsub:State_Propagation_Simulation_Algorithm}.
    \item \underline{Other}: It can lie any control strategy targeting a specific problem.
\end{itemize}






























\endinput































% \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=My Heading]
% This is a \textbf{tcolorbox}.
% \tcblower
% Here, you see the lower part of the box.
% \end{tcolorbox}



% \begin{center}
%   \fbox{\LARGE
%     This manual is outdated and must be revised!}
% \end{center}



% \begin{flushleft}
% \hspace*{0.5cm}â\verb!n015002t.ttf!â, â\verb!n015003t.ttf!â, and â\verb!n015006t.ttf!â
% \end{flushleft}
   

% \ref{it:project_available} above in Section~\ref{sub:with_a_local_latex_installation} (\nameref{sub:with_a_local_latex_installation}).


% subsection with_a_remote_cloud_based_service (end)


% \newcommand{\accessAllowed}{\includegraphics[align=c,width=1.9em]{access_allowed}}
% \newcommand{\accessForbiden}{\includegraphics[align=c,width=1.9em]{dont_touch}}
% \newcommand{\File}{\includegraphics[align=c,width=1.9em]{file}}
% \newcommand{\Folder}{\includegraphics[align=c,width=1.9em]{folder}}


% \bgroup
%     \rowcolors{1}{}{GhostWhite}
%       \begin{xltabular}{\textwidth}{>{\ttfamily}l>{\itshape}lcX}
%         \caption{The folders and files (top level).}
%         \label{tab:folders_and_files}\\
%         \toprule
%         \rowcolor{Gainsboro}%
%         Name & Type & Access & Contents \\
%         \midrule
% template.tex      & \File    & \accessForbiden &
% The main template file. You need to \emph{compile} this file with one of \pdfLaTeX, \XeLaTeX, or \LuaLaTeX\ to obtain the PDF file (â\texttt{template.pdf}â).  I recommend the usage of the â\texttt{latexmk}â command or, if you use a UN*X-like OS, you may use â\texttt{make}â (and the ggiven â\texttt{Makefile}â).
% \\
% Config          & \Folder  & \accessAllowed &
% Configuration files.  Please customize your template by changing the files in this folder!
% \\
%         \bottomrule
%         \end{xltabular}
%     % \end{longtblr}
% \egroup


% \newcommand{\classoption}[4]{\textbf{#1=OPT}\newline\emph{\small#2}&\textbf{#3}\newline{\small#4}\\}
% \newcommand{\defaultopt}[1]{\mbox{$\Rightarrow$~\emph{Default: \texttt{#1}}}\newline}
% \newcommand{\defaultit}[1][default]{($\Leftarrow$~\emph{#1})}


% \bgroup
% \begin{xltabular}{\linewidth}{>{\hsize=.4\hsize\raggedright\arraybackslash}X>{\hsize=.6\hsize}X}
%   \toprule
% %----------------------------------------------------------------------
%   \classoption{doctype}%
%     {phd, phdprop, phdplan, msc, mscplan, bsc, plain}%
%     {The type of the document.}%
% 	{%
%     \begin{tabular}{@{}r@{ $\rightarrow$ }l@{}}
%         phd & PhD thesis \defaultit.\\
%     phdprop & PhD thesis proposal (for FCT-NOVA).\\
%     phdplan & PhD thesis plan.\\
%         msc & MSc thesis.\\
%     mscplan & MSc thesis plan.\\
%         bsc & BSc report.\\
%       plain & Other report.\\
%     \end{tabular}
%     }
% %----------------------------------------------------------------------
%     \midrule
%   \classoption{school}%
%   	{nova/fct
% 	}%
%     {Selection of the university and of the school (and degree variant).}%
%     {\defaultopt{school=nova/fct} }
% %----------------------------------------------------------------------
%     \midrule
%   \classoption{docstatus}%
%     {draft, provisional, final}%
%     {The current status of the document.}%
% 	{}
% %----------------------------------------------------------------------
%     \bottomrule
% \end{xltabular}
% \egroup


% \printbibliography[heading=subbibliography, segment=\therefsegment, title={\bibname\ for chapter~\thechapter}]

% \section{\glsfmtshort{novathesisclass}\ Class Options}
% \label{sec:package_options}

 % \todo[inline]{A a note in a line by itself.}







